---
layout: post
title: Sample size determination using historical data and simulation
date: 2019-12-31
math: true
keywords:
  - R
  - bootstrap
  - hypothesis testing
  - sample size determination
  - simulation
---

In order to test a hypothesis, one has to design and execute an adequate
experiment. Typically, it is neither feasible nor desirable to involve the whole
population. Instead, a relatively small subset of the population is studied, and
given the outcome for this small sample, relevant conclusions are drawn with
respect to the population. An important question to answer is then, What is the
minimal sample size needed for the experiment to succeed? At this point, one
might reach for a thick textbook in statistics, match the situation at hand with
a statistical test, agree to the terms and conditions of the test, and proceed
to estimating the sample size using the corresponding formulae either manually
or via software packages. In what follows, we shall answer the question posed
above without any statistical formalism and any mathematical formulae. The only
prerequisites are the availability of historical data describing the status quo
and the ability to write a few lines of code in a programming language of
choice.

# Problem

For concreteness, consider the following scenario. We run an online business and
hypothesize that a specific change in promotion campaigns, such as making them
personalized, will have a positive effect on a specific performance metric, such
as the average deposit. In order to investigate if it is actually the case, we
decide to perform an A/B test. There are the following two competing hypotheses.

* The null hypothesis postulates that the change has no effect on the metric.

* The alternative hypothesis postulates that the change has a positive effect on
  the metric.

There will be two groups: a control group and a treatment group. The former will
be exposed to the current promotion policy, while the latter to the new one.
There are also certain requirements imposed on the test. First, we have a level
of statistical significance $$\alpha$$ and a level of practical significance
$$\delta$$ in mind. The former puts a limit on false positives, and the latter
indicates the smallest effect that we still care about; anything smaller is as
good as zero for any practical purpose. In addition, we require the test to have
a prescribed statistical power $$(1 - \beta)$$, which constrains false
negatives.

For our purposes, the test is considered well designed if it is capable of
detecting a difference as small as $$\delta$$ so that the false positive and
false negative rates are controlled to levels $$\alpha$$ and $$\beta$$,
respectively. Typically, parameters $$\alpha$$ and $$\delta$$ are held constant,
and the desired statistical power $$(1 - \beta)$$ is attained by varying the
number of participants in each group, which we denote by $$n$$. Note that we do
not want any of the parameters to be smaller than the prescribed values, as it
would be wasteful.

So what should the sample size be for the test to be well designed?

# Solution

Depending on the distribution of the data and on the chosen metric, one might or
might not be able to find a suitable test among the standard ones, while
ensuring that the test’s assumptions can safely be considered satisfied. More
importantly, a textbook solution might not be the most intuitive one, which, in
particular, might lead to misuse of the test. It is the understanding that
matters.

Here we take a more pragmatic and rather general approach that circumvents the
above concerns. It requires only historical data and basic programming skills.
Despite its simplicity, the method below goes straight to the core of what the
famed statistical tests are doing behind all the math. The approach belongs to
the class of so-called bootstrap techniques and is as follows.

Suppose we have historical data on customers’ behavior under the current
promotion policy, which is commonplace in practice. An important realization is
that this data set represents what we expect to observe in the control group. It
is also what is expected of the treatment group provided that the null
hypothesis is true, that is, when the proposed change has no effect. This
realization enables one to simulate what would happen if each group was limited
to an arbitrary number of participants. Then, by varying this size parameter, it
is possible to find the smallest value that makes the test well designed, that
is, make the test satisfy the requirements on $$\alpha$$, $$\beta$$, and
$$\delta$$, as discussed in the previous section.

This is all. The rest is an elaboration of the above idea.

The simulation entails the following. To begin with, note that what we are
interested in testing is the difference between the performance metric applied
to the treatment group and the same metric applied to the control group, which
is referred to as the test statistic:

```
Test statistic = Metric(Treatment sample) - Metric(Control sample).
```

`Treatment sample` and `Control sample` stand for sets of observations, and
`Metric(Sample)` stands for computing the performance metric given such a
sample. For instance, each observation could be the total deposit of a customer,
and the metric could be the average value:

```
Metric(Sample) = Sum of observations / Number of observations.
```

Note, however, that it is an example; the metric can be arbitrary, and this is a
huge advantage of this approach to sample size determination based on data and
simulation.

Large positive values of the test statistic speak in favor of the treatment
(that is, the new promotion policy in our example), while those that are close
to zero suggest that the treatment is futile.

A sample of $$n$$ observations corresponding to the status quo (that is, the
current policy in our example) can be easily obtained by drawing $$n$$ data
points with replacement from the historical data:

```
Sample = Choose random with replacement(Data, N).
```

This expression is used for `Control sample` under both the null and alternative
hypotheses. As alluded to earlier, this is also how `Treatment sample` is
obtained under the null. Regarding the alternative hypothesis being true, one
has to express the hypothesized outcome as a distribution for the case of the
minimal detectable difference, $$\delta$$. The simplest and reasonable solution
is to sample the data again, apply the metric, and then adjust the result to
reflect the alternative hypothesis:

```
Metric(Choose random with replacement(Data, N)) + Delta.
```

Here, again, one is free to change the logic under the alternative according to
situation at hand. For instance, instead of an additive effect, one could
simulate a multiplicative one.

The above is a way to simulate a single instance of the experiment under either
the null or alternative hypothesis; the result is a single value for the test
statistic. The next step is to estimate how the test statistic would vary if the
experiment was repeated many times in the two scenarios. This simply means that
the procedure should be repeated multiple times:

```
Repeat many times:
  Sample 1 = Choose random with replacement(Data, N)
  Sample 2 = Choose random with replacement(Data, N)
  Metric 1 = Metric(Sample 1)
  Metric 2 = Metric(Sample 2)
  Test statistic under null = Metric 1 - Metric 2

  Sample 3 = Choose random with replacement(Data, N)
  Sample 4 = Choose random with replacement(Data, N)
  Metric 3 = Metric(Sample 3) + Delta
  Metric 4 = Metric(Sample 4)
  Test statistic under alternative = Metric 3 - Metric 4
```

This yields a collection of values for the test statistic under the null
hypothsis and a collection of values for the test statistic under the
alternative hypothesis. Each one contains realizations from the so-called
sampling distribution in the corresponding scenario. These two distributions are
what we are after, as they allow one to compute the statistical power. First,
given $$\alpha$$, the sampling distribution under the null is used in order to
find a value beyond which the probability mass is equal to $$\alpha$$:

```
Critical value = Quantile({ Test statistic under null }, 1 - alpha).
```

`Quantile` computes the quantile specified by the second argument given a set of
observations. This quantity is called the critical value of the test. Second,
the sampling distribution under the alternative is used in order to compute the
false negative rate, $$\beta$$:

```
Beta = Mean({ Test statistic under alternative < Critical value }).
```

It corresponds to the probability mass of the alternative distribution to the
left of the critical value.

# Implementation

She sells seashells by the seashore.

```{r, cache = TRUE, echo = TRUE}
library(tidyverse)

set.seed(42)

# Artificial data for illustration
observation_count <- 20000
data <- tibble(value = rlnorm(observation_count))

# Performance metric
metric <- mean
# Statistical significance
alpha <- 0.05
# Statistical power
power <- 0.8
# Practical significance
delta <- 0.1 * metric(data$value)

simulate <- function(sample_size, replication_count) {
  # Function for drawing a single sample of size sample_size
  run_one <- function() sample(data$value, sample_size, replace = TRUE)
  # Function for drawing replication_count samples of size sample_size
  run_many <- function() replicate(replication_count, { metric(run_one()) })

  # Simulation under the null hypothesis
  control_null <- run_many()
  treatment_null <- run_many()
  delta_null <- treatment_null - control_null

  # Simulation under the alternative hypothesis
  control_alternative <- run_many()
  treatment_alternative <- run_many() + delta
  delta_alternative <- treatment_alternative - control_alternative

  # Computation of the critical value
  critical_value <- quantile(delta_null, 1 - alpha)
  # Computation of the statistical power
  power <- 1 - mean(delta_alternative < critical_value)

  list(delta_null = delta_null,
       delta_alternative = delta_alternative,
       critical_value = critical_value,
       power = power)
}

# Number of replications
replication_count <- 1000
# Interval of possible values for the sample size
search_interval <- c(1, 10000)
# Root finding to attain the desired power by varying the sample size
target <- function(n) power - simulate(as.integer(n), replication_count)$power
sample_size <- as.integer(uniroot(target, interval = search_interval)$root)
```

Let us plot the final simulation results:

```{r sampling-distribution}
library(tidyverse)

theme_set(theme_minimal(base_size = 14))

result <- simulate(sample_size, replication_count)
tibble(Null = result$delta_null, Alternative = result$delta_alternative) %>%
  gather(group, value) %>%
  ggplot(aes(value, fill = group)) +
  geom_density(alpha = 0.7, color = NA) +
  geom_vline(xintercept = result$critical_value, linetype = 'dashed') +
  labs(x = 'Difference between the treatment and control groups',
       y = 'Sampling distribution') +
  scale_x_continuous(breaks = seq(-1, 1, by = 0.1)) +
  scale_y_continuous(breaks = seq(0, 10)) +
  scale_fill_manual(values=c('#ef8a62', '#67a9cf')) +
  theme(legend.title = element_blank(), legend.position = c(0.9, 0.9))
```

Here one can see the sampling distribution of the test statistic for two cases:
one is when the null is true (in blue), and one is when the alternative is true
(in red). The dashed line denotes the critical value. When the test statistic
falls to the right of the line, we reject the null hypothesis; otherwise, we
fail to reject it. The blue and red areas to the right of the critical value
correspond to the desired significance level $$\alpha$$ and statistical power,
respectively.

# Conclusion

She sells seashells by the seashore.

# Acknowledgments

This article is inspired by “[There is only one test!][Allen Downey]” by Allen
Downey and “[Statistics without the agonizing pain][John Rauser]” by John
Rauser. I also would like to thank [Aaron Rendahl] for the help with the
implementation.

[Aaron Rendahl]: http://users.stat.umn.edu/~rend0020/
[Allen Downey]: http://allendowney.blogspot.com/2011/05/there-is-only-one-test.html
[John Rauser]: https://www.youtube.com/watch?v=5Dnw46eC-0o
