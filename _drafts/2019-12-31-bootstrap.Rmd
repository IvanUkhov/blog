---
layout: post
title: Sample size determination using historical data and simulation
date: 2019-12-31
math: true
---

In order to test a hypothesis, one has to design and execute an adequate
experiment. Typically, it is neither feasible nor desirable to involve the whole
population. Instead, a relatively small subset of the population is studied, and
given the outcome for this small sample, relevant conclusions are drawn with
respect to the population. An important question to answer is then, What is the
minimal sample size needed for the experiment to succeed? At this point, one
might reach for a thick textbook in statistics, match the situation at hand with
a statistical test, agree to the terms and conditions of the test, and proceed
to estimating the sample size using the corresponding formulae either manually
or via software packages. In what follows, we shall answer the question posed
above without any statistical formalism and any mathematical formulae. The only
prerequisites are the availability of historical data describing the status quo
and the ability to write a few lines of code in a programming language of
choice.

# Problem

For concreteness, suppose we run a business and hypothesize that a certain
change in promotion campaigns will have a positive effect on a certain
performance metric, such as the average deposit. This means that there are the
following two competing hypotheses.

* The null hypothesis postulates that the change has no effect on the metric.

* The alternative hypothesis postulates that the change has a positive effect on
  the metric.

In order to resolve the quandary in a statistically sound way, we decide to
perform an A/B test. There will be two groups of customers: a control group and
a treatment group. The former will be exposed to the current promotion policy,
while the latter to the new one. There are certain requirements imposed on the
test. First, we have a level of statistical significance $$\alpha$$ and a level
of practical significance $$\delta$$ in mind. The former puts a limit on false
positives, and the latter indicates the smallest effect that we still care
about; anything smaller is as good as zero for any practical purpose. In
addition, we require the test to have a prescribed statistical power $$(1 -
\beta)$$, which enforces a constraint on false negatives.

The test is said to be well designed if it is able to detect a difference as
small as $$\delta$$ while controlling the false positive and false negative
rates to levels $$\alpha$$ and $$\beta$$, respectively.

Now, what should the size of each group be for the test to be well designed?

# Solution

Depending on the distribution of the data and on the chosen metric, one might or
might not be able to find a suitable test among the standard ones, while
ensuring that the test’s assumptions can safely be considered satisfied. More
importantly, an analytical solution might not be the most intuitive one, which,
in particular, might lead to misuse of the test. It is the understanding that
matters.

Here we take a more pragmatic and rather general approach that circumvents the
above concerns. We only need historical data and basic programming skills.
Despite its simplicity, the method below goes straight to the core of what the
famed statistical tests are doing behind all the math. The approach belongs to
the class of so-called bootstrap techniques and is as follows.

Suppose we have historical data on customers’ behavior under the current
promotion policy, which is commonplace in practice. An important realization is
that this data set represents what we expect to observe in the control group. It
is also what is expected of the treatment group provided that the null
hypothesis is true, that is, when the proposed change has no effect. This
realization can be leveraged in order to simulate what would happen if each
group was limited to a certain number of participants. We can then vary this
size parameter until we find the smallest value that makes the test well
designed, that is, satisfying the requirements on $$\alpha$$, $$\beta$$, and
$$\delta$$, as discussed in the previous section.

To begin with, note that what we are interested in testing is the difference
between the performance metric applied to the treatment group and the same
metric applied to the control group. This quantity will be referred to as the
test statistic in what follows. Large positive values of the test statistic
speak in favor of the new policy, while those that are close to zero suggest
that the treatment is futile.

# Implementation

She sells seashells by the seashore.

```{r, cache = TRUE, echo = TRUE}
library(tidyverse)

set.seed(42)

# Artificial data for illustration
observation_count <- 20000
data <- tibble(value = rlnorm(observation_count))

# Performance metric
metric <- mean
# Statistical significance
alpha <- 0.05
# Statistical power
power <- 0.8
# Practical significance
delta <- 0.1 * metric(data$value)

simulate <- function(sample_size, replication_count) {
  # Function for drawing a single sample of size sample_size
  run_one <- function() sample(data$value, sample_size, replace = TRUE)
  # Function for drawing replication_count samples of size sample_size
  run_many <- function() replicate(replication_count, { metric(run_one()) })

  # Simulation under the null hypothesis
  control_null <- run_many()
  treatment_null <- run_many()
  delta_null <- treatment_null - control_null

  # Simulation under the alternative hypothesis
  control_alternative <- run_many()
  treatment_alternative <- run_many() + delta
  delta_alternative <- treatment_alternative - control_alternative

  # Computation of the critical value
  critical_value <- quantile(delta_null, 1 - alpha)
  # Computation of the statistical power
  power <- 1 - mean(delta_alternative < critical_value)

  list(delta_null = delta_null,
       delta_alternative = delta_alternative,
       critical_value = critical_value,
       power = power)
}

# Number of replications
replication_count <- 1000
# Interval of possible values for the sample size
search_interval <- c(1, 10000)
# Root finding to attain the desired power by varying the sample size
target <- function(n) power - simulate(as.integer(n), replication_count)$power
sample_size <- as.integer(uniroot(target, interval = search_interval)$root)
```

Let us plot the final simulation results:

```{r sampling-distribution}
library(tidyverse)

theme_set(theme_minimal(base_size = 14))

result <- simulate(sample_size, replication_count)
tibble(Null = result$delta_null, Alternative = result$delta_alternative) %>%
  gather(group, value) %>%
  ggplot(aes(value, fill = group)) +
  geom_density(alpha = 0.7, color = NA) +
  geom_vline(xintercept = result$critical_value, linetype = 'dashed') +
  labs(x = 'Difference between the treatment and control groups',
       y = 'Sampling distribution') +
  scale_x_continuous(breaks = seq(-1, 1, by = 0.1)) +
  scale_y_continuous(breaks = seq(0, 10)) +
  scale_fill_manual(values=c('#ef8a62', '#67a9cf')) +
  theme(legend.title = element_blank(), legend.position = c(0.9, 0.9))
```

Here one can see the sampling distribution of the test statistic for two cases:
one is when the null is true (in blue), and one is when the alternative is true
(in red). The dashed line denotes the critical value. When the test statistic
falls to the right of the line, we reject the null hypothesis; otherwise, we
fail to reject it. The blue and red areas to the right of the critical value
correspond to the desired significance level $$\alpha$$ and statistical power,
respectively.

# Conclusion

She sells seashells by the seashore.

# Acknowledgments

This article is inspired by “[There is only one test!][Allen Downey]” by Allen
Downey and “[Statistics without the agonizing pain][John Rauser]” by John
Rauser. I also would like to thank [Aaron Rendahl] for the help with the
implementation.

[Aaron Rendahl]: http://users.stat.umn.edu/~rend0020/
[Allen Downey]: http://allendowney.blogspot.com/2011/05/there-is-only-one-test.html
[John Rauser]: https://www.youtube.com/watch?v=5Dnw46eC-0o
