---
layout: post
title: Heteroscedastic Gaussian process regression
date: 2020-06-22
math: true
keywords:
  - Bayesian statistics
  - Gaussian process
  - R
  - Stan
  - data science
  - heteroscedasticity
  - regression
---

```{r, echo = FALSE, message = FALSE}
library(rstan)
library(tidybayes)
library(tidyverse)

source('../_scripts/2020-06-22-gaussian-process/common.R')

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
theme_set(theme_minimal(base_size = 14))

set.seed(42)
```

Gaussian process regression is a nonparametric Bayesian technique for modeling
relationships between variables of interest. The vast flexibility and rigor
mathematical foundation of this approach make it the default choice in many
problems involving small to medium-sized data sets.

In what follows, we illustrate how Gaussian process regression can be used in
practice. To make the case even more compelling, we shall consider a setting
where linear regression would be inadequate. The focus will be not on getting
the job done as fast as possible but on the learning process.

# Data

Consider the following example taken from [_Semiparametric Regression_][SR] by
Ruppert _et al._:

```{r data, include = FALSE}
data(lidar, package = 'SemiPar')
data <- lidar %>%
  transmute(x = (range - min(range)) / diff(range(range)),
            y = logratio)
data %>%
  ggplot(aes(x, y)) +
  geom_point(size = 1) +
  labs(x = 'Distance',
       y = 'Response')
```

![](/assets/images/2020-06-22-gaussian-process/data-1.svg)

The figure shows 221 observations collected in a [light detection and
ranging][LIDAR] experiment. Each observation can be interpreted as the sum of
the true underlying response at the corresponding distance and random noise. It
can be clearly seen that the variance of the noise varies with the distance: the
spread is substantially larger toward the left-hand side. This phenomenon is
known as heteroscedasticity. Homoscedasticity (the absence of
heteroscedasticity) is one of the key assumptions of linear regression. Applying
linear regression to the above problem would yield suboptimal results. The
estimates of the regression coefficients would still be unbiased; however, the
standard errors of the coefficients would be incorrect and hence misleading. A
different modeling technique is needed in this case.

The above data set will be our running example. For formally and slightly more
generally, we assume that there is a data set of $$m$$ observations:

$$
\left\{
  (\mathbf{x}_i, y_i): \,
  \mathbf{x}_i \in \mathbb{R}^d; \,
  y_i \in \mathbb{R}; \,
  i = 1, \dots, m
\right\}
$$

where the independent variable, $$\mathbf{x}$$, is $$d$$-dimensional, and the
dependent variable, $$y$$, is scalar. In the running example, $$d$$ is 1, and
$$m$$ is 221. It is time for modeling.

# Model

To begin with, consider the following model with additive noise:

$$
y_i = f(\mathbf{x}_i) + \epsilon_i, \text{ for } i = 1, \dots, m. \tag{1}
$$

In the above, $$f: \mathbb{R}^d \to \mathbb{R}$$ represents the true but unknown
underlying function, and $$\epsilon_i$$ represents the perturbation of the
$$i$$th observation by random noise. In the classical linear-regression setting,
the unknown function is modeled as a linear combination of (arbitrary
transformations of) the $$d$$ covariates. Instead of assuming any particular
functional form, we put a Gaussian process prior on the function:

$$
f(\mathbf{x}) \sim \text{Gaussian process}\left( 0, k(\mathbf{x}, \mathbf{x}') \right).
$$

The above notation means that, before observing any data, the function is a draw
from a Gaussian process with zero mean and a covariance function $$k$$. The
covariance function dectates the degree of correlation between two arbitrary
locations $$\mathbf{x}$$ and $$\mathbf{x}'$$ in $$\mathbb{R}^d$$. For instance,
a frequent choice for $$k$$ is the squared-exponential covariance function:

$$
k(\mathbf{x}, \mathbf{x}')
= \sigma_\text{process}^2 \exp\left( -\frac{\|\mathbf{x} - \mathbf{x}'\|_2^2}{2 \, \ell_\text{process}^2} \right)
$$

where $$\|\cdot\|_2$$ stands for the Euclidean norm, $$\sigma_\text{process}^2$$
is the variance (to see this, substitute $$\mathbf{x}$$ for $$\mathbf{x}'$$),
and $$\ell_\text{process}$$ is known as the length scale. While the variance
parameter is intuitive, the length-scale one requires an illustration. The
parameter controls the speed with which the correlation fades with the distance.
The following figure shows 10 random draws for $$\ell_\text{process} = 0.1$$:

```{r prior-short, include = FALSE}
prior_plot(ell_process = 0.1)
```

![](/assets/images/2020-06-22-gaussian-process/prior-short-1.svg)

With $$\ell_\text{process} = 0.5$$, the behavior changes to the following:

```{r prior-long, include = FALSE}
prior_plot(ell_process = 0.5)
```

![](/assets/images/2020-06-22-gaussian-process/prior-long-1.svg)

It can be seen that it takes a greater distance for a function with a larger
length scale (_top_) to change to the same extent compared to a function with a
smaller length scale (_bottom_).

Let us now return to Equation (1) and discuss the error terms, $$\epsilon_i$$.
In linear regression, they are modeled as independent identically distributed
Gaussian random variables:

$$
\epsilon_i \sim \text{Gaussian}\left( 0, \sigma_\text{noise}^2 \right),
\text{ for } i = 1, \dots, m.
$$

This is also the approach one can take with Gaussian process regression;
however, one does not have to. There are reasons to believe the problem at hand
is heteroscedastic, and it should be reflected in the model. To this end, the
magnitude of the noise is allowed to vary with the covariates:

$$
\epsilon_i | \mathbf{x}_i \sim \text{Gaussian}\left(0, \sigma^2_{\text{noise}, i}\right),
\text{ for } i = 1, \dots, m.
$$

The error terms are still independent (given the covariates) but not identically
distributed. At this point, one has to make a choice about the dependence of
$$\sigma_{\text{noise}, i}$$ on $$\mathbf{x}_i$$. Keeping in mind that
$$\sigma_{\text{noise}, i}$$ is nonnegative, a reasonable choice is a
generalized linear model:

$$
\ln \sigma^2_{\text{noise}, i} = \alpha_\text{noise} + \boldsymbol{\beta}^\intercal_\text{noise} \, \mathbf{x}_i,
\text{ for } i = 1, \dots, m,
$$

where $$\alpha$$ is the intercept of the regression line, and
$$\boldsymbol{\beta} \in \mathbb{R}^d$$ contains the slopes.

Thus far, a model for the unknown function $$f$$ and a model for the noise have
been prescribed. In total, there are $$d + 3$$ parameters:
$$\sigma_\text{process}$$, $$\ell_\text{process}$$, $$\alpha_\text{noise}$$, and
$$\beta_{\text{noise}, i}$$ for $$i = 1, \dots, d$$. The first two are
positive, and the rest are arbitrary. The final piece is prior distributions for
these parameters.

The priors are as follows:

$$
\begin{align}
\sigma_\text{process} & \sim \text{half-Gaussian}\left( 0, 1 \right); \\
\ell_\text{process} & \sim \text{half-Gaussian}\left( 0, 1 \right); \\
\alpha_\text{noise} & \sim \text{Gaussian}\left( 0, 1 \right); \text{ and} \\
\beta_{\text{noise}, i} & \sim \text{Gaussian}\left( 0, 1 \right),
\text{ for } i = 1, \dots, d.\\
\end{align}
$$

Putting everything together, the final model is as follows:

$$
\begin{align}
y_i
& = f(\mathbf{x}_i) + \epsilon_i,
\text{ for } i = 1, \dots, m; \\

f(\mathbf{x})
& \sim \text{Gaussian process}\left( 0, k(\mathbf{x}, \mathbf{x}') \right); \\

k(\mathbf{x}, \mathbf{x}')
& = \sigma_\text{process}^2 \exp\left( -\frac{\|\mathbf{x} - \mathbf{x}'\|_2^2}{2 \, \ell_\text{process}^2} \right); \\

\epsilon_i | \mathbf{x}_i
& \sim \text{Gaussian}\left( 0, \sigma^2_{\text{noise}, i} \right),
\text{ for } i = 1, \dots, m; \\

\ln \sigma^2_{\text{noise}, i}
& = \alpha_\text{noise} + \boldsymbol{\beta}_\text{noise}^\intercal \, \mathbf{x}_i,
\text{ for } i = 1, \dots, m; \\

\sigma_\text{process}
& \sim \text{half-Gaussian}\left( 0, 1 \right); \\

\ell_\text{process}
& \sim \text{half-Gaussian}\left( 0, 1 \right); \\

\alpha_\text{noise}
& \sim \text{Gaussian}\left( 0, 1 \right); \text{ and} \\

\beta_{\text{noise}, i}
& \sim \text{Gaussian}\left( 0, 1 \right),
\text{ for } i = 1, \dots, d.\\
\end{align}
$$

# Acknowledgments

I would like to thank Mattias Villani.

# References

* Carl Rasmussen _et al._, [_Gaussian Processes for Machine
  Learning_][GPML], the MIT Press, 2006.

* David Ruppert _et al._, [_Semiparametric Regression_][SR], Cambridge
  University Press, 2003.

* Mattias Villani, “[Advanced Bayesian learning][ABL],” Stockholm
  University, 2020.

[ABL]: https://github.com/mattiasvillani/AdvBayesLearnCourse
[GPML]: http://www.gaussianprocess.org/gpml
[LIDAR]: https://en.wikipedia.org/wiki/Lidar
[SR]: http://www.stat.tamu.edu/~carroll/semiregbook
