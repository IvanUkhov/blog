---
layout: post
title: Heteroscedastic Gaussian process regression
date: 2020-06-22
math: true
keywords:
  - Bayesian statistics
  - Gaussian process
  - R
  - Stan
  - data science
  - heteroscedasticity
  - regression
---

```{r, echo = FALSE, message = FALSE}
library(rstan)
library(tidybayes)
library(tidyverse)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
theme_set(theme_minimal(base_size = 14))

set.seed(42)
```

Gaussian process regression is a nonparametric Bayesian technique for modeling
relationships between variables of interest. The vast flexibility and rigor
mathematical foundation of this approach make it the default choice in many
problems involving small to medium-sized data sets.

In what follows, we illustrate how Gaussian process regression can be used in
practice. To make the case even more compelling, we shall consider a setting
where linear regression would be inadequate. The focus will be not on getting
the job done as fast as possible but on the learning process.

Consider the following example taken from [_Semiparametric Regression_][SR] by
Ruppert _et al._:

```{r data, include = FALSE}
data(lidar, package = 'SemiPar')
data <- lidar %>%
  transmute(x = (range - min(range)) / diff(range(range)),
            y = logratio)
lidar %>%
  ggplot(aes(range, logratio)) +
  geom_point(size = 1) +
  labs(x = 'Distance',
       y = 'Response')
```

![](/assets/images/2020-06-22-gaussian-process/data-1.svg)

The figure shows 221 observations collected in a [light detection and
ranging][LIDAR] experiment. Each observation can be interpreted as the sum of
the true underlying response at the corresponding distance and random noise. It
can be clearly seen that the variance of the noise varies with the distance: the
spread is substantially larger toward the left-hand side. This phenomenon is
known as heteroscedasticity. Homoscedasticity (the absence of
heteroscedasticity) is one of the key assumptions of linear regression. Applying
linear regression to the above problem would yield suboptimal results.
More specifically, the estimates of the regression coefficients would still be
unbiased; however, the standard errors of the coefficients would be incorrect
and hence misleading.

# Model

Suppose there is a data set of $$m$$ observations:

$$
\left\{
  (\mathbf{x}_i, y_i): \,
  \mathbf{x}_i \in \mathbb{R}^d; \,
  y_i \in \mathbb{R}; \,
  i = 1, \dots, m
\right\}.
$$

The independent variable, $$\mathbf{x}$$, is $$d$$-dimensional, and the
dependent one, $$y$$, is scalar. In the example given earlier, $$d$$ is one. To
begin with, consider the following model with additive noise:

$$
y_i = f(\mathbf{x}_i) + \epsilon_i, \text{ for } i = 1, \dots, m.
$$

In the above, $$f: \mathbb{R}^d \to \mathbb{R}$$ represents the true underlying
function, and $$\epsilon_i$$ represents the perturbation of the $$i$$th
observation by random noise.

$$
\begin{align}
& f(\mathbf{x}) \sim \text{Gaussian process}\left(\mathbf{0}, k(\mathbf{x}, \mathbf{x}')\right); \\
& \epsilon_i \sim \text{Gaussian}\left(0, \sigma^2_\text{noise}\right),
\text{ for } i = 1, \dots, m.
\end{align}
$$

# Acknowledgments

I would like to thank Mattias Villani.

# References

* Carl Rasmussen _et al._, [_Gaussian Processes for Machine
  Learning_][GPML], the MIT Press, 2006.

* David Ruppert _et al._, [_Semiparametric Regression_][SR], Cambridge
  University Press, 2003.

* Mattias Villani, “[Advanced Bayesian learning][ABL],” Stockholm
  University, 2020.

[ABL]: https://github.com/mattiasvillani/AdvBayesLearnCourse
[GPML]: http://www.gaussianprocess.org/gpml
[LIDAR]: https://en.wikipedia.org/wiki/Lidar
[SR]: http://www.stat.tamu.edu/~carroll/semiregbook
