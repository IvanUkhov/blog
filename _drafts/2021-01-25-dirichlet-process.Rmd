---
layout: post
title: Breaking sticks, or estimation of probability distributions using the Dirichlet process
date: 2021-01-25T08:00:00+02:00
math: true
keywords:
  - Bayesian statistics
  - Dirichlet process
  - Gibbs sampling
  - R
  - data science
---

```{r, echo = FALSE, message = FALSE}
library(MASS)
library(latex2exp)
library(tidyverse)

work_directory <- '../_scripts/2021-01-25-dirichlet-process'
source(file.path(work_directory, 'common.R'))

cache_directory <- '../_caches'
dir.create(cache_directory, showWarnings = FALSE)

theme_set(theme_minimal(base_size = 14))
```

Recall the last time you wanted to understand the distribution of given data.
One alternative was to plot a histogram. However, it resulted in frustration due
to the choice of the number of bins to use, which led to drastically different
outcomes. Another alternative was kernel density estimation. Despite having a
similar choice to make, it has the advantage of producing smooth estimates,
which are more realistic for continuous quantities with regularities. However,
kernel density estimation was unsatisfactory too: it did not aid in
understanding the underlying structure of the data and, moreover, provided no
means of quantifying the uncertainty associated with the results. In this
article, we discuss a Bayesian approach to the estimation of data-generating
distributions that addresses the aforementioned concerns.

The approach we shall discuss is based on the family of Dirichlet processes. How
specifically such processes are constructed will be described in the next
section; here, we focus on the big picture.

A Dirichlet process is a stochastic process, that is, an indexed sequence of
random variables. Each realization of this process is a discrete probability
distribution, which makes the process a distribution over distributions,
similarly to a Dirichlet distribution. The process has only one parameter: a
measure $$\nu: \mathcal{B} \to [0, \infty]$$ in a suitable finite measure space
$$(\mathcal{X}, \mathcal{B}, \nu)$$ where $$\mathcal{X}$$ is a set, and
$$\mathcal{B}$$ is a $$\sigma$$-algebra on $$\mathcal{X}$$. We shall adopt the
following notation:

$$
P \sim \text{Dirichlet Process}(\nu)
$$

where $$P$$ is a _random_ probability distribution that is distributed according
to the Dirichlet process. Note that measure $$\nu$$ does not have to be a
probability measure; that is, $$\nu(\mathcal{X}) = 1$$ is not required. In order
to obtain a probability measure, one can divide $$\nu$$ by the total volume
$$\lambda = \nu(\mathcal{X})$$:

$$
P_0(\cdot) = \frac{1}{\lambda} \nu(\cdot).
$$

Since this normalization is always possible, it is common and convenient to
replace $$\nu$$ with $$\lambda P_0$$ and consider the process to be
parametrized by two quantities instead of one:

$$
P \sim \text{Dirichlet Process}(\lambda P_0).
$$

Parameter $$\lambda$$ is sometimes referred to as the concentration parameter of
the process.

There are two major alternatives of using the Dirichlet process for estimating
distributions: as a direct prior for the data at hand and as a mixing prior. We
begin with the former.

# Direct prior

Given a data set of $$n$$ observations $$\{ x_i \}_{i = 1}^n$$, a Dirichlet
process can be used as a prior:

$$
\begin{align}
x_i | P_x & \sim P_x, \text{ for } i = 1, \dots, n; \text{ and} \\
P_x & \sim \text{Dirichlet Process}(\lambda P_0). \tag{1}
\end{align}
$$

It is important to realize that the $$x_i$$’s are assumed to be distributed
_not_ according to the Dirichlet process but according to a distribution drawn
from the Dirichlet process. Parameter $$\lambda$$ allows one to control the
strength of the prior: the larger it is, the more shrinkage toward the prior is
induced.

## Inference

Due to the conjugacy property of the Dirichlet process in the above setting, the
posterior is also a Dirichlet process and has the follow simple form:

$$
P_x | \{ x_i \}_{i = 1}^n
\sim \text{Dirichlet Process}\left( \lambda P_0 + \sum_{i = 1}^n \delta_{x_i} \right). \tag{2}
$$

That is, the total volume and normalized measure are updated as follows:

$$
\begin{align}
\lambda & := \lambda + n \quad \text{and} \\
P_0 & := \frac{\lambda}{\lambda + n} P_0 + \frac{1}{\lambda + n} \sum_{i = 1}^n \delta_{x_i}.
\end{align}
$$

Here, $$\delta_x(\cdot)$$ is the Dirac measure, meaning that $$\delta_x(X) = 1$$
if $$x \in X$$ for any $$X \subseteq \mathcal{X}$$, and otherwise, it is zero.
It can be seen in Equation (2) that the base measure has simply been augmented
with unit masses placed at the $$n$$ observed data points.

The main question now is, How to draw samples from a Dirichlet process given
$$\lambda$$ and $$P_0$$?

As noted earlier, a draw from a Dirichlet process is a discrete probability
distribution $$P_x$$. The probability measure of this distribution admits the
follow representation:

$$
P_x(\cdot) = \sum_{i = 1}^\infty p_i \delta_{x_i}(\cdot) \tag{3}
$$

where $$\{ p_i \}$$ is a set of probabilities that sum up to one, and $$\{ x_i
\}$$ is a set of points in $$\mathcal{X}$$. Such a draw can be obtained using
the so-called stick-breaking construction, which prescribes $$\{ p_i \}$$ and
$$\{ x_i \}$$. To begin with, for practical computations, the infinite summation
is truncated to retain the only first $$m$$ elements:

$$
P_x(\cdot) = \sum_{i = 1}^m p_i \delta_{x_i}(\cdot).
$$

Atoms $$\{ x_i \}_{i = 1}^m$$ are drawn independently from the normalized base
measure $$P_0$$. The calculation of probabilities $$\{ p_i \}$$ is more
elaborate, and this is where the construction and this article get their name,
“stick breaking.” Imagine a stick of unit length, representing the total
probability. The procedure is to keep breaking the stick into two parts where,
for each iteration, the left part yields $$p_i$$, and the right one, the
remainder, is carried over to the next iteration. How much to break off is
decided on by drawing $$m$$ independent realizations from a carefully chosen
beta distribution:

$$
q_i \sim \text{Beta}(1, \lambda), \text{ for } i = 1, \dots, m. \tag{4}
$$

All of them lie in the unit interval and are the proportions to break off of the
remainder. When $$\lambda = 1$$, these proportions (of the reminder) are
uniformly distributed. When $$\lambda < 1$$, the probability mass is shifted to
the right, which means that there are likely to be a small number of large
pieces, covering virtually the entire stick. When $$\lambda > 1$$, the
probability mass is shifted to the left, which means that there are likely to be
a large number of small pieces, struggling to reach the end of the stick.

Formally, the desired probabilities are given by the following expression:

$$
p_i = q_i \prod_{j = 1}^{i - 1} (1 - q_j), \text{ for } i = 1, \dots, m,
$$

which, as noted earlier, are the left parts of the remainder of the stick during
each iteration. For instance, $$p_1 = q_1$$, $$p_2 = q_2 (1 - q_1)$$, and so on.
Due to the truncation, the probabilities $$\{ p_i \}_{i = 1}^m$$ do not sum up
to one, and it is common to set $$q_m := 1$$ so that $$p_m$$ takes up the
remaining probability mass.

To recapitulate, a single draw from a Dirichlet process is obtained in two
steps: prescribe atoms $$\{ x_i \}$$ via draws from the normalized base measure
and prescribe the corresponding probabilities $$\{ p_i \}$$ via the
stick-breaking construction. The two give a complete description of a discrete
probability distribution. Recall that this distribution is still a single draw.
By repeating this process many times, one obtains the distribution of this
distribution, which can be used to, for instance, quantify uncertainty in the
estimation.

## Illustration

```{r data, include = FALSE}
data <- tibble(x = galaxies / 1000)
```

It is time to demonstrate how the Dirichlet process behaves as a direct prior.
To this end, we shall use a [data set][galaxies] containing velocities of “82
galaxies from 6 well-separated conic sections of an unfilled survey of the
Corona Borealis region.” It was studied in [Roeder (1990)], which gives us a
reference point.

> For the curious reader, the source code of this R [notebook] along with
> auxiliary [scripts] that are used for performing all the calculations
> presented below can be found on GitHub.

The empirical cumulative distribution function of the velocity is as follows:

```{r data-cdf, include = FALSE}
ggplot() +
  geom_observation(data) +
  labs(x = TeX('Velocity ($10^6$ m/s)'),
       y = 'Cumulative probability') +
  scale_color() +
  theme(legend.position = 'none')
```

![](/assets/images/2021-01-25-dirichlet-process/data-cdf-1.svg)

Already here, it is apparent that the distribution is multimodal: there are two
distinct regions, one to the left and one to the right, where the curve is flat,
meaning there are no observations there. The proverbial histogram gives a
confirmation:

```{r data-histogram, include = FALSE}
ggplot() +
  geom_observation(data, type = 'histogram') +
  labs(x = TeX('Velocity ($10^6$ m/s)'),
       y = 'Number of galaxies') +
  scale_color() +
  theme(legend.position = 'none')
```

![](/assets/images/2021-01-25-dirichlet-process/data-histogram-1.svg)

It can be seen that there is a handful of galaxies moving relatively slowly and
relatively fast compared to the majority located somewhere in the middle around
twenty thousand kilometers per second. For completeness, kernel density
estimation results in the following plot:

```{r data-kde, include = FALSE}
ggplot(data, aes(x)) +
  geom_density(color = 'gray70', size = 1) +
  labs(x = TeX('Velocity ($10^6$ m/s)'),
       y = 'Probability density')
```

![](/assets/images/2021-01-25-dirichlet-process/data-kde-1.svg)

How many clusters of galaxies are there? What are their average velocities? How
uncertain are these estimates? Our goal is to answer these questions by virtue
of the Dirichlet process.

Now that the intention is to apply the presented theory in practice, we have to
make all choices we have conveniently glanced over. Specifically, $$P_0$$ has to
be chosen, and we shall use the following:

$$
P_0(\cdot) = \text{Gaussian}(\, \cdot \, | \mu_0, \sigma_0^2). \tag{5}
$$

In the above, $$\text{Gaussian}(\cdot)$$ refers to the probability measure of a
Gaussian distribution with parameters $$\mu_0$$ and $$\sigma_0$$. In addition to
these two, there is one more: $$\lambda$$. We shall set $$\mu_0$$ and
$$\sigma_0$$ to 20 and 5, respectively—which correspond roughly to the mean and
standard deviation of the data—and present results for different $$\lambda$$’s
to investigate how the prior volume affects shrinkage toward the prior.

First, we do not condition on the data to get a better understanding of the
prior itself, which corresponds to Equation (1). The following figure shows a
single draw from four Dirichlet processes with different $$\lambda$$’s (the gray
curves show the cumulative distribution function of the data as a reference):

```{r direct-prior, fig.asp = 1.2, include = FALSE}
set.seed(42)

tibble(lambda = c(1, 10, 100, 1000)) %>%
  mutate(data = map(lambda, ~ sample_DP(l = 10000,
                                        nu = nu_prior(lambda = .x,
                                                      mu0 = 20,
                                                      sigma0 = 5)))) %>%
  unnest(data) %>%
  mutate(lambda = str_c('$\\lambda$ = ', lambda)) %>%
  group_by(lambda) %>%
  plot_distribution(data) +
  facet_wrap(vars(lambda),
             ncol = 1,
             labeller = as_labeller(TeX, default = label_parsed))
```

![](/assets/images/2021-01-25-dirichlet-process/direct-prior-1.svg)

It can be seen that the larger the prior volume, the smoother the curve. This is
because larger $$\lambda$$’s “break” the stick into more pieces, allowing the
normalized base measure to be extensively sampled, which, in the limit,
converges to this very measure; see Equation (5).

Now, conditioning on the observed data—that is, sampling as shown in Equation
(2)—we obtain the following draws from the posterior Dirichlet distributions
with different $$\lambda$$’s:

```{r direct-posterior, fig.asp = 1.2, include = FALSE}
set.seed(42)

tibble(lambda = c(1, 10, 100, 1000)) %>%
  mutate(data = map(lambda, ~ sample_DP(l = 10000,
                                        nu = nu_posterior(data$x,
                                                          lambda = .x,
                                                          mu0 = 20,
                                                          sigma0 = 5)))) %>%
  unnest(data) %>%
  mutate(lambda = str_c('$\\lambda$ = ', lambda)) %>%
  group_by(lambda) %>%
  plot_distribution(data) +
  facet_wrap(vars(lambda),
             ncol = 1,
             labeller = as_labeller(TeX, default = label_parsed))
```

![](/assets/images/2021-01-25-dirichlet-process/direct-posterior-1.svg)

When the prior volume is small, virtually no data points come from $$P_0$$;
instead, they are mostly uniform draws from the observed data set, leading to a
curve that is nearly indistinguishable from the one of the data (the top curve).
As $$\lambda$$ gets larger, the prior gets stronger, and the estimate gets
shrunk toward it, up to a point where the observations appear to be entirely
ignored (the bottom curve).

The above model has a serious limitation: it assumes a discrete probability
distribution for the data-generating process, which can be seen in the prior and
posterior given in Equation (1) and (2), respectively, and it is also apparent
in the decomposition given in Equation (3). In some cases, it might be
appropriate; however, there is arguably more situations where it is inadequate,
including the running example.

# Mixing prior

Instead of using a Dirichlet process as a direct prior for the given data, it
can be used as a prior for mixing distributions from a given family. The
resulting posterior will then naturally inherit the properties of the family,
such as continuity. The general structure is as follows:

$$
\begin{align}
x_i | \theta_i & \sim P_x \left( \theta_i \right), \text{ for } i = 1, \dots, n; \tag{6} \\
\theta_i | P_\theta & \sim P_\theta, \text{ for } i = 1, \dots, n; \text{ and} \\
P_\theta & \sim \text{Dirichlet Process}(\lambda P_0). \\
\end{align}
$$

The $$i$$th data point, $$x_i$$, is distributed according to distribution
$$P_x$$ with parameters $$\theta_i$$. For instance, $$P_x$$ could refer to the
Gaussian family with $$\theta_i = (\mu_i, \sigma_i)$$ identifying a particular
member of the family by its mean and standard deviation. Parameters $$\{
\theta_i \}_{i = 1}^n$$ are unknown and distributed according to distribution
$$P_\theta$$. Distribution $$P_\theta$$ is not known either and gets a Dirichlet
process prior with measure $$\lambda P_0$$.

It can be seen in Equation (6) that each data point can potentially have its own
unique set of parameters. However, this is not what usually happens in practice.
If $$\lambda$$ is reasonably small, the vast majority of the stick—the one we
explained how to break in the previous section—tends to be consumed by a small
number of pieces. This makes many data points share the same parameters, which
is akin to clustering. In fact, clustering is a prominent use case for the
Dirichlet process.

## Inference

Unlike the previous model, there is no conjugacy in this case, and hence the
posterior is not a Dirichlet process. There is, however, a simple Markov chain
Monte Carlo sampling strategy based on the stick-breaking construction. It
belongs to the class of Gibbs samplers and is as follows.

Similarly to Equation (3), we have the following decomposition:

$$
P_m(\cdot) = \sum_{i = 1}^\infty p_i P_x(\cdot | \theta_i)
$$

where $$P_m$$ is the probability measure of the mixture. As before, the infinite
decomposition has to be made finite to be usable in practice:

$$
P_m(\cdot) = \sum_{i = 1}^m p_i P_x(\cdot | \theta_i).
$$

Here, $$m$$ represents an upper limit on the number of mixture components. Each
data point $$x_i$$, for $$i = 1, \dots, n$$, is mapped to one of the $$m$$
components, which we denote by $$k_i \in \{ 1, \dots, m \}$$. In other words,
$$k_i$$ takes values from 1 to $$m$$ and gives the index of the component of
the $$i$$th observation.

There are $$m + m \times |\theta| + n$$ parameters to be inferred where
$$|\theta|$$ denotes the number of parameters of $$P_x$$. These parameters are
$$\{ p_i \}_{i = 1}^m$$, $$\{ \theta_i \}_{i = 1}^m$$, and $$\{ k_i \}_{i =
1}^n$$. As usual in Gibbs sampling, the parameters assume arbitrary but
compatible initial values. The sampler has the following three steps.

First, given $$\{ p_i \}$$, $$\{ \theta_i \}$$, and $$\{ x_i \}$$, the mapping
of the observations to the mixture components, $$\{ k_i \}$$, is updated as
follows:

$$
k_i \sim \text{Categorical}\left(
  m,
  \left\{ \frac{p_j P_x(x_i | \theta_j)}{\sum_{l = 1}^m p_l P_x(x_i | \theta_l)} \right\}_{j = 1}^m
\right), \text{ for } i = 1, \dots, n.
$$

That is, $$k_i$$ is a draw from a categorical distribution with $$m$$ categories
whose unnormalized probabilities are given by $$p_j P_x(x_i | \theta_j)$$, for
$$j = 1, \dots, m$$.

Second, given $$\{ k_i \}$$, the probabilities of the mixture components, $$\{ p_i
\}$$, are updated using the stick-breaking construction described earlier. This
time, however, the beta distribution for sampling $$\{ q_i \}$$ in Equation (4)
is replaced with the following:

$$
q_i \sim \text{Beta}\left( 1 + n_i, \lambda + \sum_{j = i + 1}^m n_j \right), \text{ for } i = 1, \dots, m,
$$

where

$$
n_i = \sum_{j = 1}^n I_{\{i\}}(k_j), \text{ for } i = 1, \dots, m,
$$

is the number of data points that are currently allocated to component $$i$$.
Here, $$I_A$$ is the indicator function of a set $$A$$. As before, in order for
the $$p_i$$’s to sum up to one, it is common to set $$q_m := 1$$.

Third, given $$\{ k_i \}$$ and $$\{ x_i \}$$, the parameters of the mixture
components, $$\{ \theta_i \}$$, are updated. This is done by sampling from the
posterior distribution of each component. In this case, the posterior is a prior
of choice that is updated using the data points that are currently allocated to
the corresponding component. In order to streamline this step, a conjugate prior
for the data distribution, $$P_x$$, is commonly utilized, which we shall
illustrate shortly.

To recapitulate, a single draw from the posterior is obtained in a number of
steps where parameters or groups of parameters are updated in turn, while
treating the other parameters as known. This Gibbs procedure is very flexible.
Other parameters can be inferred too, instead of setting them to fixed values.
An important example is the prior volume, $$\lambda$$. This parameter controls
the formation of clusters, and one might let the data decide what the value
should be, in which case a step similar to the third one is added to the
procedure to update $$\lambda$$. This will be also illustrated below.

## Illustration

For concreteness, consider the following choices:

$$
\begin{align}
\theta_i &= (\mu_i, \sigma_i), \text{ for } i = 1, \dots, n; \\
P_x (\theta_i) &= \text{Gaussian}(\mu_i, \sigma_i^2), \text{ for } i = 1, \dots, n; \text{ and} \\
P_0(\cdot) &= \text{Gaussian–Scaled-Inverse-}\chi^2(\, \cdot \, | \mu_0, \kappa_0, \nu_0, \sigma_0^2).
\end{align}
$$

In the above, $$\text{Gaussian–Scaled-Inverse-}\chi^2(\cdot)$$ refers to the
probability measure of a bivariate distribution composed of a conditional
Gaussian and an unconditional scaled inverse chi-squared distribution. Some
intuition about this distribution can be built via the following decomposition:

$$
\begin{align}
\mu | \sigma^2 & \sim \text{Gaussian}\left(\mu_0, \frac{\sigma^2}{\kappa_0}\right) \text{ and} \\
\sigma^2 & \sim \text{Scaled-Inverse-}\chi^2(\nu_0, \sigma_0^2).
\end{align}
$$

This prior is a conjugate prior for a Gaussian data distribution with unknown
mean and variance, which we assume here. This means that the posterior is also a
Gaussian–scaled-inverse-chi-squared distribution. Given a data set with
$$n$$ observations $$x_1, \dots, x_n$$, the four parameters of the prior
are updated simultaneously (not sequentially) as follows:

$$
\begin{align}
\mu_0 & := \frac{\kappa_0}{\kappa_0 + n} \mu_0 + \frac{n}{\kappa_0 + n} \mu_x, \\
\kappa_0 & := \kappa_0 + n, \\
\nu_0 & := \nu_0 + n, \text{ and} \\
\sigma_0^2 & := \frac{1}{\nu_0 + n} \left( \nu_0 \sigma_0^2 + ss_x + \frac{\kappa_0 n}{\kappa_0 + n}(\mu_x - \mu_0)^2 \right)
\end{align}
$$

where $$\mu_x = \sum_{i = 1}^n x_i / n$$ and $$ss_x = \sum_{i = 1}^n (x_i -
\mu_x)^2$$. It can be seen that $$\kappa_0$$ and $$\nu_0$$ act as counters of
the number of observations; $$\mu_0$$ is a weighted sum of two means; and
$$\nu_0 \sigma_0^2$$ is a sum of two sums of squares and a third term increasing
the uncertainty due to the difference in the means.

```{r mixture-prior-mu, include = FALSE}
set.seed(42)
sample_Ptheta_prior(1000, mu0 = mean(data$x), kappa0 = 0.01) %>%
  ggplot() +
  geom_vline(aes(xintercept = mean(data$x), color = 'Observation'),
             size = 1,
             show.legend = FALSE) +
  stat_density(aes(mu, color = 'Model'),
               geom = 'line',
               position = 'identity',
               size = 1) +
  scale_color() +
  labs(x = TeX('Mean of a component, $\\mu_i$ (1000 km/s)'),
       y = 'Probability density') +
  theme(legend.position = 'top',
        legend.title = element_blank())
```

![](/assets/images/2021-01-25-dirichlet-process/mixture-prior-mu-1.svg)

```{r mixture-prior-sigma, include = FALSE}
set.seed(42)
sample_Ptheta_prior(1000, mu0 = mean(data$x), kappa0 = 0.01) %>%
  ggplot() +
  geom_vline(aes(xintercept = sd(data$x), color = 'Observation'),
             size = 1,
             show.legend = FALSE) +
  stat_density(aes(sigma, color = 'Model'),
               geom = 'line',
               position = 'identity',
               size = 1) +
  scale_color() +
  labs(x = TeX('Standard deviation of a component, $\\sigma_i$ (1000 km/s)'),
       y = 'Probability density') +
  theme(legend.position = 'top',
        legend.title = element_blank())
```

![](/assets/images/2021-01-25-dirichlet-process/mixture-prior-sigma-1.svg)

```{r mixture-prior-lambda, include = FALSE}
set.seed(42)
tibble(lambda = sample_Plambda_prior(1000)) %>%
  ggplot() +
  geom_density(aes(lambda), color = 'gray30', size = 1) +
  labs(x = TeX('Prior volume, $\\lambda$'),
       y = 'Probability density')
```

![](/assets/images/2021-01-25-dirichlet-process/mixture-prior-lambda-1.svg)

```{r mixture-prior-check, include = FALSE}
cache_path <- file.path(cache_directory, 'mixture-prior.RData')
if (!file.exists(cache_path)) {
  set.seed(42)
  draws0 <- sample_DPM(data$x, 25, 1000, kappa0 = 0.01, prior_only = TRUE)
  save(draws0, file = cache_path)
} else {
  load(cache_path)
}

check_predictive(sample(tail(draws0, length(draws0) / 2), 2), data)
```

![](/assets/images/2021-01-25-dirichlet-process/mixture-prior-check-1.svg)

```{r mixture-posterior-check, include = FALSE}
cache_path <- file.path(cache_directory, 'mixture-posterior.RData')
if (!file.exists(cache_path)) {
  set.seed(42)
  draws <- sample_DPM(data$x, 25, 1000, kappa0 = 0.01)
  save(draws, file = cache_path)
} else {
  load(cache_path)
}

check_predictive(sample(tail(draws, length(draws) / 2), 2), data)
```

![](/assets/images/2021-01-25-dirichlet-process/mixture-posterior-check-1.svg)

```{r mixture-posterior-summary, include = FALSE}
cache_path <- file.path(cache_directory, 'mixture-posterior-summarized')
if (!file.exists(cache_path)) {
  set.seed(42)
  draws_summarized <- summarize_inference(tail(draws, length(draws) / 2), data)
  save(draws_summarized, file = cache_path)
} else {
  load(cache_path)
}

plot_inference(draws_summarized, data)
```

![](/assets/images/2021-01-25-dirichlet-process/mixture-posterior-summary-1.svg)

```{r mixture-posterior-k, include = FALSE}
tibble(value = sapply(draws, function(draw) length(unique(draw$k)))) %>%
  mutate(draw = row_number()) %>%
  ggplot(aes(draw, value)) +
  geom_line(color = 'gray30', size = 0.5) +
  ylim(1, 25) +
  labs(x = 'Iteration',
       y = 'Number of components')
```

![](/assets/images/2021-01-25-dirichlet-process/mixture-posterior-k-1.svg)

```{r mixture-posterior-lambda, include = FALSE}
tibble(value = sapply(draws, function(draw) draw$lambda)) %>%
  mutate(draw = row_number()) %>%
  ggplot(aes(draw, value)) +
  geom_line(color = 'gray30', size = 0.5) +
  labs(x = 'Iteration',
       y = TeX('Prior volume, $\\lambda$'))
```

![](/assets/images/2021-01-25-dirichlet-process/mixture-posterior-lambda-1.svg)

# Acknowledgments

I would like to thank [Mattias Villani] for the insightful and informative
graduate course in statistics titled “[Advanced Bayesian learning][Villani
(2020)],” which was the inspiration behind writing this article.

# References

* Andrew Gelman et al., _[Bayesian Data Analysis][Gelman (2014)]_, Chapman and
  Hall/CRC, 2014.
* Kathryn Roeder, “[Density estimation with confidence sets exemplified by
  superclusters and voids in galaxies][Roeder (1990)],” Journal of the American
  Statistical Association, 1990.
* Rick Durrett, _[Probability: Theory and Examples][Durrett (2010)]_, Cambridge
  University Press, 2010.

# Footnotes

[Durrett (2010)]: https://services.math.duke.edu/~rtd/PTE/pte.html
[Gelman (2014)]: http://www.stat.columbia.edu/~gelman/book/
[Mattias Villani]: https://www.mattiasvillani.com/
[Roeder (1990)]: https://doi.org/10.2307/2289993
[Villani (2020)]: https://github.com/mattiasvillani/AdvBayesLearnCourse
[galaxies]: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/galaxies.html

[notebook]: https://github.com/IvanUkhov/blog/blob/master/_posts/2021-01-25-dirichlet-process.Rmd
[scripts]: https://github.com/IvanUkhov/blog/tree/master/_scripts/2021-01-25-dirichlet-process
