<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://blog.ivanukhov.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.ivanukhov.com/" rel="alternate" type="text/html" /><updated>2021-02-05T12:18:19+00:00</updated><id>https://blog.ivanukhov.com/feed.xml</id><title type="html">Good news, everyone!</title><subtitle>On to data science</subtitle><author><name>Ivan Ukhov</name></author><entry><title type="html">Breaking sticks, or estimation of probability distributions using the Dirichlet process</title><link href="https://blog.ivanukhov.com/2021/01/25/dirichlet-process.html" rel="alternate" type="text/html" title="Breaking sticks, or estimation of probability distributions using the Dirichlet process" /><published>2021-01-25T06:00:00+00:00</published><updated>2021-01-25T06:00:00+00:00</updated><id>https://blog.ivanukhov.com/2021/01/25/dirichlet-process</id><content type="html" xml:base="https://blog.ivanukhov.com/2021/01/25/dirichlet-process.html">&lt;p&gt;Recall the last time you wanted to understand the distribution of given data.
One alternative was to plot a histogram. However, it resulted in frustration due
to the choice of the number of bins to use, which led to drastically different
outcomes. Another alternative was kernel density estimation. Despite having a
similar choice to make, it has the advantage of producing smooth estimates,
which are more realistic for continuous quantities with regularities. However,
kernel density estimation was unsatisfactory too: it did not aid in
understanding the underlying structure of the data and, moreover, provided no
means of quantifying the uncertainty associated with the results. In this
article, we discuss a Bayesian approach to the estimation of data-generating
distributions that addresses the aforementioned concerns.&lt;/p&gt;

&lt;p&gt;The approach we shall discuss is based on the family of Dirichlet processes. How
specifically such processes are constructed will be described in the next
section; here, we focus on the big picture.&lt;/p&gt;

&lt;p&gt;A Dirichlet process is a stochastic process, that is, an indexed sequence of
random variables. Each realization of this process is a discrete probability
distribution, which makes the process a distribution over distributions,
similarly to a Dirichlet distribution. The process has only one parameter: a
measure \(\nu: \mathcal{B} \to [0, \infty]\) in a suitable finite measure space
\((\mathcal{X}, \mathcal{B}, \nu)\) where \(\mathcal{X}\) is a set, and
\(\mathcal{B}\) is a \(\sigma\)-algebra on \(\mathcal{X}\). We shall adopt the
following notation:&lt;/p&gt;

\[P \sim \text{Dirichlet Process}(\nu)\]

&lt;p&gt;where \(P\) is a &lt;em&gt;random&lt;/em&gt; probability distribution that is distributed according
to the Dirichlet process. Note that measure \(\nu\) does not have to be a
probability measure; that is, \(\nu(\mathcal{X}) = 1\) is not required. To
obtain a probability measure, one can divide \(\nu\) by the total volume
\(\lambda = \nu(\mathcal{X})\):&lt;/p&gt;

\[P_0(\cdot) = \frac{1}{\lambda} \nu(\cdot).\]

&lt;p&gt;Since this normalization is always possible, it is common and convenient to
replace \(\nu\) with \(\lambda P_0\) and consider the process to be
parametrized by two quantities instead of one:&lt;/p&gt;

\[P \sim \text{Dirichlet Process}(\lambda P_0).\]

&lt;p&gt;Parameter \(\lambda\) is referred to as the concentration parameter of the
process.&lt;/p&gt;

&lt;p&gt;There are two major alternatives of using the Dirichlet process for estimating
distributions: as a direct prior for the data at hand and as a mixing prior. We
begin with the former.&lt;/p&gt;

&lt;h1 id=&quot;direct-prior&quot;&gt;Direct prior&lt;/h1&gt;

&lt;p&gt;Given a data set of \(n\) observations \(\{ x_i \}_{i = 1}^n\), a Dirichlet
process can be used as a prior:&lt;/p&gt;

\[\begin{align}
x_i | P_x &amp;amp; \sim P_x, \text{ for } i = 1, \dots, n; \text{ and} \\
P_x &amp;amp; \sim \text{Dirichlet Process}(\lambda P_0). \tag{1}
\end{align}\]

&lt;p&gt;It is important to realize that the \(x_i\)’s are assumed to be distributed
&lt;em&gt;not&lt;/em&gt; according to the Dirichlet process but according to a distribution drawn
from the Dirichlet process. Parameter \(\lambda\) allows one to control the
strength of the prior: the larger it is, the more shrinkage toward the prior is
induced.&lt;/p&gt;

&lt;h2 id=&quot;inference&quot;&gt;Inference&lt;/h2&gt;

&lt;p&gt;Due to the conjugacy property of the Dirichlet process in the above setting, the
posterior is also a Dirichlet process and has the following simple form:&lt;/p&gt;

\[P_x | \{ x_i \}_{i = 1}^n
\sim \text{Dirichlet Process}\left( \lambda P_0 + \sum_{i = 1}^n \delta_{x_i} \right). \tag{2}\]

&lt;p&gt;That is, the total volume and normalized measure are updated as follows:&lt;/p&gt;

\[\begin{align}
\lambda &amp;amp; := \lambda + n \quad \text{and} \\
P_0 &amp;amp; := \frac{\lambda}{\lambda + n} P_0 + \frac{1}{\lambda + n} \sum_{i = 1}^n \delta_{x_i}.
\end{align}\]

&lt;p&gt;Here, \(\delta_x(\cdot)\) is the Dirac measure, meaning that \(\delta_x(X) = 1\)
if \(x \in X\) for any \(X \subseteq \mathcal{X}\), and otherwise, it is zero.
It can be seen in Equation (2) that the base measure has simply been augmented
with unit masses placed at the \(n\) observed data points.&lt;/p&gt;

&lt;p&gt;The main question now is, How to draw samples from a Dirichlet process given
\(\lambda\) and \(P_0\)?&lt;/p&gt;

&lt;p&gt;As noted earlier, a draw from a Dirichlet process is a discrete probability
distribution \(P_x\). The probability measure of this distribution admits the
following representation:&lt;/p&gt;

\[P_x(\cdot) = \sum_{i = 1}^\infty p_i \delta_{x_i}(\cdot) \tag{3}\]

&lt;p&gt;where \(\{ p_i \}\) is a set of probabilities that sum up to one, and \(\{ x_i
\}\) is a set of points in \(\mathcal{X}\). Such a draw can be obtained using
the so-called stick-breaking construction, which prescribes \(\{ p_i \}\) and
\(\{ x_i \}\). To begin with, for practical computations, the infinite summation
is truncated to retain the only first \(m\) elements:&lt;/p&gt;

\[P_x(\cdot) = \sum_{i = 1}^m p_i \delta_{x_i}(\cdot).\]

&lt;p&gt;Atoms \(\{ x_i \}_{i = 1}^m\) are drawn independently from the normalized base
measure \(P_0\). The calculation of probabilities \(\{ p_i \}\) is more
elaborate, and this is where the construction and this article get their name,
“stick breaking.” Imagine a stick of unit length, representing the total
probability. The procedure is to keep breaking the stick into two parts where,
for each iteration, the left part yields \(p_i\), and the right one, the
remainder, is carried over to the next iteration. How much to break off is
decided on by drawing \(m\) independent realizations from a carefully chosen
beta distribution:&lt;/p&gt;

\[q_i \sim \text{Beta}(1, \lambda), \text{ for } i = 1, \dots, m. \tag{4}\]

&lt;p&gt;All of them lie in the unit interval and are the proportions to break off of the
remainder. When \(\lambda = 1\), these proportions (of the reminder) are
uniformly distributed. When \(\lambda &amp;lt; 1\), the probability mass is shifted to
the right, which means that there are likely to be a small number of large
pieces, covering virtually the entire stick. When \(\lambda &amp;gt; 1\), the
probability mass is shifted to the left, which means that there are likely to be
a large number of small pieces, struggling to reach the end of the stick.&lt;/p&gt;

&lt;p&gt;Formally, the desired probabilities are given by the following expression:&lt;/p&gt;

\[p_i = q_i \prod_{j = 1}^{i - 1} (1 - q_j), \text{ for } i = 1, \dots, m,\]

&lt;p&gt;which, as noted earlier, are the left parts of the remainder of the stick during
each iteration. For instance, \(p_1 = q_1\), \(p_2 = q_2 (1 - q_1)\), and so on.
Due to the truncation, the probabilities \(\{ p_i \}_{i = 1}^m\) do not sum up
to one, and it is common to set \(q_m := 1\) so that \(p_m\) takes up the
remaining probability mass.&lt;/p&gt;

&lt;p&gt;To recapitulate, a single draw from a Dirichlet process is obtained in two
steps: prescribe atoms \(\{ x_i \}\) via draws from the normalized base measure
and prescribe the corresponding probabilities \(\{ p_i \}\) via the
stick-breaking construction. The two give a complete description of a discrete
probability distribution. Recall that this distribution is still a single draw.
By repeating this process many times, one obtains the distribution of this
distribution, which can be used to, for instance, quantify uncertainty in the
estimation.&lt;/p&gt;

&lt;h2 id=&quot;illustration&quot;&gt;Illustration&lt;/h2&gt;

&lt;p&gt;It is time to demonstrate how the Dirichlet process behaves as a direct prior.
To this end, we shall use a &lt;a href=&quot;https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/galaxies.html&quot;&gt;data set&lt;/a&gt; containing velocities of “82
galaxies from 6 well-separated conic sections of an unfilled survey of the
Corona Borealis region.” It was studied in &lt;a href=&quot;https://doi.org/10.2307/2289993&quot;&gt;Roeder (1990)&lt;/a&gt;, which gives us a
reference point.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For the curious reader, the source code of this &lt;a href=&quot;https://github.com/IvanUkhov/blog/blob/master/_posts/2021-01-25-dirichlet-process.Rmd&quot;&gt;notebook&lt;/a&gt; along with
auxiliary &lt;a href=&quot;https://github.com/IvanUkhov/blog/tree/master/_scripts/2021-01-25-dirichlet-process&quot;&gt;scripts&lt;/a&gt; that are used for performing all the calculations
presented below can be found on GitHub.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The empirical cumulative distribution function of the velocity is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/data-cdf-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Already here, it is apparent that the distribution is multimodal: there are two
distinct regions, one to the left and one to the right, where the curve is flat,
meaning there are no observations there. The proverbial histogram gives a
confirmation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/data-histogram-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It can be seen that there is a handful of galaxies moving relatively slowly and
relatively fast compared to the majority located somewhere in the middle around
twenty thousand kilometers per second. For completeness, kernel density
estimation results in the following plot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/data-kde-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How many clusters of galaxies are there? What are their average velocities? How
uncertain are these estimates? Our goal is to answer these questions by virtue
of the Dirichlet process.&lt;/p&gt;

&lt;p&gt;Now that the intention is to apply the presented theory in practice, we have to
make all choices we have conveniently glanced over. Specifically, \(P_0\) has to
be chosen, and we shall use the following:&lt;/p&gt;

\[P_0(\cdot) = \text{Gaussian}(\, \cdot \, | \mu_0, \sigma_0^2). \tag{5}\]

&lt;p&gt;In the above, \(\text{Gaussian}(\cdot)\) refers to the probability measure of a
Gaussian distribution with parameters \(\mu_0\) and \(\sigma_0\). In addition to
these two, there is one more: \(\lambda\). We shall set \(\mu_0\) and
\(\sigma_0\) to 20 and 5, respectively—which correspond roughly to the mean and
standard deviation of the data—and present results for different \(\lambda\)’s
to investigate how the prior volume affects shrinkage toward the prior.&lt;/p&gt;

&lt;p&gt;First, we do not condition on the data to get a better understanding of the
prior itself, which corresponds to Equation (1). The following figure shows a
single draw from four Dirichlet processes with different \(\lambda\)’s (the gray
curves show the cumulative distribution function of the data as a reference):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/direct-prior-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It can be seen that the larger the prior volume, the smoother the curve. This is
because larger \(\lambda\)’s “break” the stick into more pieces, allowing the
normalized base measure to be extensively sampled, which, in the limit,
converges to this very measure; see Equation (5).&lt;/p&gt;

&lt;p&gt;Now, conditioning on the observed velocities of galaxies—that is, sampling as
shown in Equation (2)—we obtain the following draws from the posterior Dirichlet
distributions with different \(\lambda\)’s:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/direct-posterior-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the prior volume is small, virtually no data points come from \(P_0\);
instead, they are mostly uniform draws from the observed data set, leading to a
curve that is nearly indistinguishable from the one of the data (the top curve).
As \(\lambda\) gets larger, the prior gets stronger, and the estimate gets
shrunk toward it, up to a point where the observations appear to be entirely
ignored (the bottom curve).&lt;/p&gt;

&lt;p&gt;The above model has a serious limitation: it assumes a discrete probability
distribution for the data-generating process, which can be seen in the prior and
posterior given in Equation (1) and (2), respectively, and it is also apparent
in the decomposition given in Equation (3). In some cases, it might be
appropriate; however, there is arguably more situations where it is inadequate,
including the running example.&lt;/p&gt;

&lt;h1 id=&quot;mixing-prior&quot;&gt;Mixing prior&lt;/h1&gt;

&lt;p&gt;Instead of using a Dirichlet process as a direct prior for the given data, it
can be used as a prior for mixing distributions from a given family. The
resulting posterior will then naturally inherit the properties of the family,
such as continuity. The general structure is as follows:&lt;/p&gt;

\[\begin{align}
x_i | \theta_i &amp;amp; \sim P_x \left( \theta_i \right), \text{ for } i = 1, \dots, n; \tag{6} \\
\theta_i | P_\theta &amp;amp; \sim P_\theta, \text{ for } i = 1, \dots, n; \text{ and} \\
P_\theta &amp;amp; \sim \text{Dirichlet Process}(\lambda P_0). \\
\end{align}\]

&lt;p&gt;The \(i\)th data point, \(x_i\), is distributed according to distribution
\(P_x\) with parameters \(\theta_i\). For instance, \(P_x\) could refer to the
Gaussian family with \(\theta_i = (\mu_i, \sigma_i)\) identifying a particular
member of the family by its mean and standard deviation. Parameters \(\{
\theta_i \}_{i = 1}^n\) are unknown and distributed according to distribution
\(P_\theta\). Distribution \(P_\theta\) is not known either and gets a Dirichlet
process prior with measure \(\lambda P_0\).&lt;/p&gt;

&lt;p&gt;It can be seen in Equation (6) that each data point can potentially have its own
unique set of parameters. However, this is not what usually happens in practice.
If \(\lambda\) is reasonably small, the vast majority of the stick—the one we
explained how to break in the previous section—tends to be consumed by a small
number of pieces. This makes many data points share the same parameters, which
is akin to clustering. In fact, clustering is a prominent use case for the
Dirichlet process.&lt;/p&gt;

&lt;h2 id=&quot;inference-1&quot;&gt;Inference&lt;/h2&gt;

&lt;p&gt;Unlike the previous model, there is no conjugacy in this case, and hence the
posterior is not a Dirichlet process. There is, however, a simple Markov chain
Monte Carlo sampling strategy based on the stick-breaking construction. It
belongs to the class of Gibbs samplers and is as follows.&lt;/p&gt;

&lt;p&gt;Similarly to Equation (3), we have the following decomposition:&lt;/p&gt;

\[P_m(\cdot) = \sum_{i = 1}^\infty p_i P_x(\cdot | \theta_i)\]

&lt;p&gt;where \(P_m\) is the probability measure of the mixture. As before, the infinite
decomposition has to be made finite to be usable in practice:&lt;/p&gt;

\[P_m(\cdot) = \sum_{i = 1}^m p_i P_x(\cdot | \theta_i).\]

&lt;p&gt;Here, \(m\) represents an upper limit on the number of mixture components. Each
data point \(x_i\), for \(i = 1, \dots, n\), is mapped to one of the \(m\)
components, which we denote by \(k_i \in \{ 1, \dots, m \}\). In other words,
\(k_i\) takes values from 1 to \(m\) and gives the index of the component of
the \(i\)th observation.&lt;/p&gt;

&lt;p&gt;There are \(m + m \times |\theta| + n\) parameters to be inferred where
\(|\theta|\) denotes the number of parameters of \(P_x\). These parameters are
\(\{ p_i \}_{i = 1}^m\), \(\{ \theta_i \}_{i = 1}^m\), and \(\{ k_i \}_{i =
1}^n\). As usual in Gibbs sampling, the parameters assume arbitrary but
compatible initial values. The sampler has the following three steps.&lt;/p&gt;

&lt;p&gt;First, given \(\{ p_i \}\), \(\{ \theta_i \}\), and \(\{ x_i \}\), the mapping
of the observations to the mixture components, \(\{ k_i \}\), is updated as
follows:&lt;/p&gt;

\[k_i \sim \text{Categorical}\left(
  m,
  \left\{ \frac{p_j P_x(x_i | \theta_j)}{\sum_{l = 1}^m p_l P_x(x_i | \theta_l)} \right\}_{j = 1}^m
\right), \text{ for } i = 1, \dots, n.\]

&lt;p&gt;That is, \(k_i\) is a draw from a categorical distribution with \(m\) categories
whose unnormalized probabilities are given by \(p_j P_x(x_i | \theta_j)\), for
\(j = 1, \dots, m\).&lt;/p&gt;

&lt;p&gt;Second, given \(\{ k_i \}\), the probabilities of the mixture components, \(\{ p_i
\}\), are updated using the stick-breaking construction described earlier. This
time, however, the beta distribution for sampling \(\{ q_i \}\) in Equation (4)
is replaced with the following:&lt;/p&gt;

\[q_i \sim \text{Beta}\left( 1 + n_i, \lambda + \sum_{j = i + 1}^m n_j \right), \text{ for } i = 1, \dots, m,\]

&lt;p&gt;where&lt;/p&gt;

\[n_i = \sum_{j = 1}^n I_{\{i\}}(k_j), \text{ for } i = 1, \dots, m,\]

&lt;p&gt;is the number of data points that are currently allocated to component \(i\).
Here, \(I_A\) is the indicator function of a set \(A\). As before, in order for
the \(p_i\)’s to sum up to one, it is common to set \(q_m := 1\).&lt;/p&gt;

&lt;p&gt;Third, given \(\{ k_i \}\) and \(\{ x_i \}\), the parameters of the mixture
components, \(\{ \theta_i \}\), are updated. This is done by sampling from the
posterior distribution of each component. In this case, the posterior is a prior
of choice that is updated using the data points that are currently allocated to
the corresponding component. To streamline this step, a conjugate prior for the
data distribution, \(P_x\), is commonly utilized, which we shall illustrate
shortly.&lt;/p&gt;

&lt;p&gt;To recapitulate, a single draw from the posterior is obtained in a number of
steps where parameters or groups of parameters are updated in turn, while
treating the other parameters as known. This Gibbs procedure is very flexible.
Other parameters can be inferred too, instead of setting them to fixed values.
An important example is the concentration parameter, \(\lambda\). This parameter
controls the formation of clusters, and one might let the data decide what the
value should be, in which case a step similar to the third one is added to the
procedure to update \(\lambda\). This will be also illustrated below.&lt;/p&gt;

&lt;h2 id=&quot;illustration-1&quot;&gt;Illustration&lt;/h2&gt;

&lt;p&gt;We continue working with the galaxy data. For concreteness, consider the
following choices:&lt;/p&gt;

\[\begin{align}
\theta_i &amp;amp;= (\mu_i, \sigma_i), \text{ for } i = 1, \dots, n; \\
P_x (\theta_i) &amp;amp;= \text{Gaussian}(\mu_i, \sigma_i^2), \text{ for } i = 1, \dots, n; \text{ and} \\
P_0(\cdot) &amp;amp;= \text{Gaussian–Scaled-Inverse-}\chi^2(\, \cdot \, | \mu_0, \kappa_0, \nu_0, \sigma_0^2).
\end{align} \tag{7}\]

&lt;p&gt;In the above, \(\text{Gaussian–Scaled-Inverse-}\chi^2(\cdot)\) refers to the
probability measure of a bivariate distribution composed of a conditional
Gaussian and an unconditional scaled inverse chi-squared distribution. Some
intuition about this distribution can be built via the following decomposition:&lt;/p&gt;

\[\begin{align}
\mu_i | \sigma_i^2 &amp;amp; \sim \text{Gaussian}\left(\mu_0, \frac{\sigma_i^2}{\kappa_0}\right) \text{ and} \\
\sigma_i^2 &amp;amp; \sim \text{Scaled-Inverse-}\chi^2(\nu_0, \sigma_0^2).
\end{align} \tag{8}\]

&lt;p&gt;This prior is a conjugate prior for a Gaussian data distribution with unknown
mean and variance, which we assume here. This means that the posterior is also a
Gaussian–scaled-inverse-chi-squared distribution. Given a data set with
\(n\) observations \(x_1, \dots, x_n\), the four parameters of the prior
are updated simultaneously (not sequentially) as follows:&lt;/p&gt;

\[\begin{align}
\mu_0 &amp;amp; := \frac{\kappa_0}{\kappa_0 + n} \mu_0 + \frac{n}{\kappa_0 + n} \mu_x, \\
\kappa_0 &amp;amp; := \kappa_0 + n, \\
\nu_0 &amp;amp; := \nu_0 + n, \text{ and} \\
\sigma_0^2 &amp;amp; := \frac{1}{\nu_0 + n} \left( \nu_0 \sigma_0^2 + ss_x + \frac{\kappa_0 n}{\kappa_0 + n}(\mu_x - \mu_0)^2 \right)
\end{align}\]

&lt;p&gt;where \(\mu_x = \sum_{i = 1}^n x_i / n\) and \(ss_x = \sum_{i = 1}^n (x_i -
\mu_x)^2\). It can be seen that \(\kappa_0\) and \(\nu_0\) act as counters of
the number of observations; \(\mu_0\) is a weighted sum of two means; and
\(\nu_0 \sigma_0^2\) is a sum of two sums of squares and a third term increasing
the uncertainty due to the difference in the means. In the Gibbs sampler, each
component (each cluster of galaxies) will have its own posterior based on the
data points that are assigned to that component during each iteration of the
process. Therefore, \(n\), \(\mu_x\), and \(ss_x\) will generally be different
for different components and, moreover, will vary from iteration to iteration.&lt;/p&gt;

&lt;p&gt;We set \(\mu_0\) to 20, which is roughly the mean of the data, and \(\nu_0\) to
3, which is the smallest integer that allows the scaled chi-squared distribution
to have a finite expectation. The choice of \(\kappa_0\) and \(\sigma_0\) is
more subtle. Recall Equation (8). What we would like from the prior is to allow
for free formation of clusters in a region generously covering the support of
the data. To this end, the uncertainty in the mean, \(\mu_i\), has to be high;
however, it should not come from \(\sigma_i\), since it would produce very
diffuse clusters. We set \(\kappa_0\) to 0.01 to magnify the variance of
\(\mu_i\) without affecting \(\sigma_i\), and \(\sigma_0\) to 1 to keep clusters
compact.&lt;/p&gt;

&lt;p&gt;Now, let us take a look at what the above choices entail. The following figure
illustrates the prior for the mean of a component:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/mixture-prior-mu-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The negative part is unrealistic for velocity; however, it is rarely a problem
in practice. What is important is that there is a generous coverage of the
plausible values. The following figure shows the prior for the standard
deviation of a component:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/mixture-prior-sigma-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The bulk is below the standard deviation of the data; however, this is by
choice: we expect more than one cluster of galaxies with similar velocities.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, we intend to include \(\lambda\) in the inference. First,
we put the following prior:&lt;/p&gt;

\[\lambda \sim \text{Gamma}(\alpha_0, \beta_0). \tag{9}\]

&lt;p&gt;Note this is the rate parameterization of the Gamma family. Conditionally, this
is a conjugate prior with the following update rule for the two parameters:&lt;/p&gt;

\[\begin{align}
\alpha_0 &amp;amp; := \alpha_0 + m - 1 \quad \text{and} \\
\beta_0 &amp;amp; := \beta_0 - \sum_{i = 1}^{m - 1} \ln(1 - q_i)
\end{align}\]

&lt;p&gt;where \(\{ q_i \}\) come from the stick-breaking construction. This is a fourth
step in the Gibbs sampler. We set \(\alpha_0\) and \(\beta_0\) to 2 and 0.1,
respectively, which entails the following prior assumption about \(\lambda\):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/mixture-prior-lambda-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The parameter is allowed to vary freely from small to large values, as desired.&lt;/p&gt;

&lt;p&gt;Having chosen all priors and their hyperparameters, we are ready to investigate
the behavior of the entire model; see Equations (6), (7), and (9). In what
follows, we shall limit the number of mixture components to 25; that is, \(m =
25\). Furthermore, we shall perform 2000 Gibbs iterations and discard the first
half as a warm-up period. As before, we start without conditioning on the data
to observe draws from the prior itself. The following figure shows two sample
draws:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/mixture-prior-check-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It can be seen that clusters of galaxies can appear anywhere in the region of
interest and can be of various sizes. We conclude that the prior is adequate.
When taking the observed velocities into account, we obtain a full posterior
distribution in the form of 1000 draws. The following shows two random draws:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/mixture-posterior-check-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Indeed, mixture components have started to appear in the regions where there are
observations.&lt;/p&gt;

&lt;p&gt;Before we proceed to the final summary of results, it is prudent to inspect
sample chains for a few parameters in order to ensure there are not problems
with convergence to the stationary distribution. The following shows the number
of occupied components among the 25 permitted:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/mixture-posterior-k-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The chain fluctuates around a fixed level without any prominent pattern, as it
should. One can plot the actual marginal posterior distribution for the number
of components; however, it is already clear that the distribution of the number
of clusters of galaxies is mostly between 5 and 10 with a median of 7.&lt;/p&gt;

&lt;p&gt;As for the concentration parameter, \(\lambda\), the chain is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/mixture-posterior-lambda-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The behavior is uneventful, which is a good sign.&lt;/p&gt;

&lt;p&gt;Let us now take a look at the posterior distributions of the first seven
components highlighted earlier (note the different scales on the vertical axes):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/mixture-posterior-mu-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The components clearly change roles, which can be seen by the multimodal nature
of the distributions. Component 1 is most often at 10 (times \(10^6\) m/s);
however, it also peaks between 24 and 25 and even above 30. Components 2 and 3
are the most certain ones, which is due to a relatively large number of samples
present in the corresponding region. They seem to exchanges roles and capture
velocities of around 20 and 23. Components 4 and 5, on the other hand, appear to
play the same role. Unlike Component 1, they are most likely to be found at
around 33. Components 6 and 7 are similar too. They seem to be responsible for
the small formation to the left and right next to the bulk in the middle (at
16); recall the histogram of the data. The small formation on the other side of
the bulk at around 26 is captured as well, which is mostly done by Component 6.&lt;/p&gt;

&lt;p&gt;Lastly, we summarize the inference using the following figure where the median
distribution and a 95% uncertainty band—composed of distributions at the 0.025
and 0.975 quantiles—are plotted:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-25-dirichlet-process/mixture-posterior-summary-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this view, only five components are visible to the naked eye. The median
curve matches well the findings in &lt;a href=&quot;https://doi.org/10.2307/2289993&quot;&gt;Roeder (1990)&lt;/a&gt;. Judging by the width of the
uncertainty band, there is a lot of plausible alternatives, and it is important
to communicate this uncertainty to those who base decisions on the inference.
The ability to quantify uncertainty with such ease is a prominent advantage of
Bayesian inference.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this article, the family of Dirichlet processes has been presented in the
context of Bayesian inference. More specifically, it has been shown how a
Dirichlet process can be utilized as a prior for an unknown discrete
distribution and as a prior for mixing distributions from a given family. In
both cases, it has been illustrated how to perform inference via a finite
approximation and the stick-breaking construction.&lt;/p&gt;

&lt;p&gt;Clearly, the overall procedure is more complicated than counting observations
falling in a number of fixed bins, which is what a histogram does, or placing
kernels all over the place, which is what a kernel density estimator does.
However, “anything in life worth having is worth working for.” The advantages of
the Bayesian approach include the ability to incorporate prior knowledge, which
is crucial in situations with little data, and the ability to propagate and
quantify uncertainty, which is a must.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Recall that the source code of this &lt;a href=&quot;https://github.com/IvanUkhov/blog/blob/master/_posts/2021-01-25-dirichlet-process.Rmd&quot;&gt;notebook&lt;/a&gt; along with auxiliary &lt;a href=&quot;https://github.com/IvanUkhov/blog/tree/master/_scripts/2021-01-25-dirichlet-process&quot;&gt;scripts&lt;/a&gt;
that were used for performing the calculations presented above can be found on
GitHub. Any feedback is welcome!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h1&gt;

&lt;p&gt;I would like to thank &lt;a href=&quot;https://www.mattiasvillani.com/&quot;&gt;Mattias Villani&lt;/a&gt; for the insightful and informative
graduate course in Bayesian statistics titled “&lt;a href=&quot;https://github.com/mattiasvillani/AdvBayesLearnCourse&quot;&gt;Advanced Bayesian
learning&lt;/a&gt;,” which was the inspiration behind writing this
article, and for his guidance regarding the implementation.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Andrew Gelman et al., &lt;em&gt;&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/book/&quot;&gt;Bayesian Data Analysis&lt;/a&gt;&lt;/em&gt;, Chapman and
Hall/CRC, 2014.&lt;/li&gt;
  &lt;li&gt;Kathryn Roeder, “&lt;a href=&quot;https://doi.org/10.2307/2289993&quot;&gt;Density estimation with confidence sets exemplified by
superclusters and voids in galaxies&lt;/a&gt;,” Journal of the American
Statistical Association, 1990.&lt;/li&gt;
  &lt;li&gt;Rick Durrett, &lt;em&gt;&lt;a href=&quot;https://services.math.duke.edu/~rtd/PTE/pte.html&quot;&gt;Probability: Theory and Examples&lt;/a&gt;&lt;/em&gt;, Cambridge
University Press, 2010.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ivan Ukhov</name></author><summary type="html">Recall the last time you wanted to understand the distribution of given data. One alternative was to plot a histogram. However, it resulted in frustration due to the choice of the number of bins to use, which led to drastically different outcomes. Another alternative was kernel density estimation. Despite having a similar choice to make, it has the advantage of producing smooth estimates, which are more realistic for continuous quantities with regularities. However, kernel density estimation was unsatisfactory too: it did not aid in understanding the underlying structure of the data and, moreover, provided no means of quantifying the uncertainty associated with the results. In this article, we discuss a Bayesian approach to the estimation of data-generating distributions that addresses the aforementioned concerns.</summary></entry><entry><title type="html">Heteroscedastic Gaussian process regression</title><link href="https://blog.ivanukhov.com/2020/06/22/gaussian-process.html" rel="alternate" type="text/html" title="Heteroscedastic Gaussian process regression" /><published>2020-06-22T06:00:00+00:00</published><updated>2020-06-22T06:00:00+00:00</updated><id>https://blog.ivanukhov.com/2020/06/22/gaussian-process</id><content type="html" xml:base="https://blog.ivanukhov.com/2020/06/22/gaussian-process.html">&lt;p&gt;Gaussian process regression is a nonparametric Bayesian technique for modeling
relationships between variables of interest. The vast flexibility and rigor
mathematical foundation of this approach make it the default choice in many
problems involving small- to medium-sized data sets. In this article, we
illustrate how Gaussian process regression can be utilized in practice. To make
the case more compelling, we consider a setting where linear regression would be
inadequate. The focus will be &lt;em&gt;not&lt;/em&gt; on getting the job done as fast as possible
but on learning the technique and understanding the choices being made.&lt;/p&gt;

&lt;h1 id=&quot;data&quot;&gt;Data&lt;/h1&gt;

&lt;p&gt;Consider the following example taken from &lt;a href=&quot;http://www.stat.tamu.edu/~carroll/semiregbook&quot;&gt;&lt;em&gt;Semiparametric
Regression&lt;/em&gt;&lt;/a&gt; by Ruppert &lt;em&gt;et al.&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-06-22-gaussian-process/data-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The figure shows 221 observations collected in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Lidar&quot;&gt;light detection and
ranging&lt;/a&gt; experiment. Each observation can be interpreted as the sum of
the true underlying response at the corresponding distance and random noise. It
can be clearly seen that the variance of the noise varies with the distance: the
spread is substantially larger toward the right-hand side. This phenomenon is
known as heteroscedasticity. Homoscedasticity (the absence of
heteroscedasticity) is one of the key assumptions of linear regression. Applying
linear regression to the above problem would yield suboptimal results. The
estimates of the regression coefficients would still be unbiased; however, the
standard errors of the coefficients would be incorrect and hence misleading. A
different modeling technique is needed in this case.&lt;/p&gt;

&lt;p&gt;The above data set will be our running example. For formally and slightly more
generally, we assume that there is a data set of \(m\) observations:&lt;/p&gt;

\[\left\{
  (\mathbf{x}_i, y_i): \,
  \mathbf{x}_i \in \mathbb{R}^d; \,
  y_i \in \mathbb{R}; \,
  i = 1, \dots, m
\right\}\]

&lt;p&gt;where the independent variable, \(\mathbf{x}\), is \(d\)-dimensional, and the
dependent variable, \(y\), is scalar. In the running example, \(d\) is 1, and
\(m\) is 221. It is time for modeling.&lt;/p&gt;

&lt;h1 id=&quot;model&quot;&gt;Model&lt;/h1&gt;

&lt;p&gt;To begin with, consider the following model with additive noise:&lt;/p&gt;

\[y_i = f(\mathbf{x}_i) + \epsilon_i, \text{ for } i = 1, \dots, m. \tag{1}\]

&lt;p&gt;In the above, \(f: \mathbb{R}^d \to \mathbb{R}\) represents the true but unknown
underlying function, and \(\epsilon_i\) represents the perturbation of the
\(i\)th observation by random noise. In the classical linear-regression setting,
the unknown function is modeled as a linear combination of (arbitrary
transformations of) the \(d\) covariates. Instead of assuming any particular
functional form, we put a Gaussian process prior on the function:&lt;/p&gt;

\[f(\mathbf{x}) \sim \text{Gaussian Process}\left( 0, k(\mathbf{x}, \mathbf{x}') \right).\]

&lt;p&gt;The above notation means that, before observing any data, the function is a draw
from a Gaussian process with zero mean and a covariance function \(k\). The
covariance function dictates the degree of correlation between two arbitrary
locations \(\mathbf{x}\) and \(\mathbf{x}'\) in \(\mathbb{R}^d\). For instance,
a frequent choice for \(k\) is the squared-exponential covariance function:&lt;/p&gt;

\[k(\mathbf{x}, \mathbf{x}')
= \sigma_\text{process}^2 \exp\left( -\frac{\|\mathbf{x} - \mathbf{x}'\|_2^2}{2 \, \ell_\text{process}^2} \right)\]

&lt;p&gt;where \(\|\cdot\|_2\) stands for the Euclidean norm, \(\sigma_\text{process}^2\)
is the variance (to see this, substitute \(\mathbf{x}\) for \(\mathbf{x}'\)),
and \(\ell_\text{process}\) is known as the length scale. While the variance
parameter is intuitive, the length-scale one requires an illustration. The
parameter controls the speed with which the correlation fades with the distance.
The following figure shows 10 random draws for \(\ell_\text{process} = 0.1\):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-06-22-gaussian-process/prior-process-short-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With \(\ell_\text{process} = 0.5\), the behavior changes to the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-06-22-gaussian-process/prior-process-long-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It can be seen that it takes a greater distance for a function with a larger
length scale (&lt;em&gt;top&lt;/em&gt;) to change to the same extent compared to a function with a
smaller length scale (&lt;em&gt;bottom&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Let us now return to Equation (1) and discuss the error terms, \(\epsilon_i\).
In linear regression, they are modeled as independent identically distributed
Gaussian random variables:&lt;/p&gt;

\[\epsilon_i \sim \text{Gaussian}\left( 0, \sigma_\text{noise}^2 \right),
\text{ for } i = 1, \dots, m. \tag{2}\]

&lt;p&gt;This is also the approach one can take with Gaussian process regression;
however, one does not have to. There are reasons to believe the problem at hand
is heteroscedastic, and it should be reflected in the model. To this end, the
magnitude of the noise is allowed to vary with the covariates:&lt;/p&gt;

\[\epsilon_i | \mathbf{x}_i \sim \text{Gaussian}\left(0, \sigma^2_{\text{noise}, i}\right),
\text{ for } i = 1, \dots, m. \tag{3}\]

&lt;p&gt;The error terms are still independent (given the covariates) but not identically
distributed. At this point, one has to make a choice about the dependence of
\(\sigma_{\text{noise}, i}\) on \(\mathbf{x}_i\). This dependence could be
modeled with another Gaussian process with an appropriate link function to
ensure \(\sigma_{\text{noise}, i}\) is nonnegative. Another reasonable choice is
a generalized linear model, which is what we shall use:&lt;/p&gt;

\[\ln \sigma^2_{\text{noise}, i} = \alpha_\text{noise} + \boldsymbol{\beta}^\intercal_\text{noise} \, \mathbf{x}_i,
\text{ for } i = 1, \dots, m, \tag{4}\]

&lt;p&gt;where \(\alpha\) is the intercept of the regression line, and
\(\boldsymbol{\beta} \in \mathbb{R}^d\) contains the slopes.&lt;/p&gt;

&lt;p&gt;Thus far, a model for the unknown function \(f\) and a model for the noise have
been prescribed. In total, there are \(d + 3\) parameters:
\(\sigma_\text{process}\), \(\ell_\text{process}\), \(\alpha_\text{noise}\), and
\(\beta_{\text{noise}, i}\) for \(i = 1, \dots, d\). The first two are
positive, and the rest are arbitrary. The final piece is prior distributions for
these parameters.&lt;/p&gt;

&lt;p&gt;The variance of the coveriance function, \(\sigma^2_\text{process}\),
corresponds to the amount of variance in the data that is explained by the
Gaussian process. It poses no particular problem and can be tackled with a
half-Gaussian or a half-Student’s t distribution:&lt;/p&gt;

\[\sigma_\text{process} \sim \text{Half-Gaussian}\left( 0, 1 \right).\]

&lt;p&gt;The notation means that the standard Gaussian distribution is truncated at zero
and renormalized. The nontrivial mass around zero implied by the prior is
considered to be beneficial in this case.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;A prior for the length scale of the covariance function,
\(\ell_\text{process}\), should be chosen with care. Small values—especially,
those below the resolution of the data—give the Gaussian process extreme
flexibility and easily leads to overfitting. Moreover, there are numerical
ramifications of the length scale approaching zero as well: the quality of
Hamiltonian Monte Carlo sampling degrades.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; The bottom line is that a prior
penalizing values close to zero is needed. A reasonable choice is an inverse
gamma distribution:&lt;/p&gt;

\[\ell_\text{process} \sim \text{Inverse Gamma}\left( 1, 1 \right).\]

&lt;p&gt;To understand the implications, let us perform a prior predictive check for this
component in isolation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-06-22-gaussian-process/prior-process-length-scale-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It can be seen that the density is very low in the region close to zero, while
being rather permissive to the right of that region, especially considering the
scale of the distance in the data; recall the very first figure. Consequently,
the choice is adequate.&lt;/p&gt;

&lt;p&gt;The choice of priors for the parameters of the noise is complicated by the
nonlinear link function; see Equation (4). What is important to realize is that
small amounts of noise correspond to negative values in the linear space, which
is probably what one should be expecting given the scale of the response.
Therefore, the priors should allow for large negative values. Let us make an
educated assumption and perform a prior predictive check to understand the
consequences. Consider the following:&lt;/p&gt;

\[\begin{align}
\alpha_\text{noise} &amp;amp; \sim \text{Gaussian}\left( -1, 1 \right) \text{ and} \\
\beta_{\text{noise}, i} &amp;amp; \sim \text{Gaussian}\left( 0, 1 \right),
\text{ for } i = 1, \dots, d.\\
\end{align}\]

&lt;p&gt;The density of \(\sigma_\text{noise}\) without considering the regression slopes
is depicted below (note the logarithmic scale on the horizontal axis):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-06-22-gaussian-process/prior-noise-sigma-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The variability in the intercept, \(\alpha_\text{noise}\), allows the standard
deviation, \(\sigma_\text{noise}\), to comfortably vary from small to large
values, keeping in mind the scale of the response. Here are two draws from the
prior distribution of the noise, including Equations (3) and (4):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-06-22-gaussian-process/prior-noise-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The large ones are perhaps unrealistic and could be addressed by further
shifting the distribution of the intercept. However, they should not cause
problems for the inference.&lt;/p&gt;

&lt;p&gt;Putting everything together, the final model is as follows:&lt;/p&gt;

\[\begin{align}
y_i
&amp;amp; = f(\mathbf{x}_i) + \epsilon_i,
\text{ for } i = 1, \dots, m; \\

f(\mathbf{x})
&amp;amp; \sim \text{Gaussian Process}\left( 0, k(\mathbf{x}, \mathbf{x}') \right); \\

k(\mathbf{x}, \mathbf{x}')
&amp;amp; = \sigma_\text{process}^2 \exp\left( -\frac{\|\mathbf{x} - \mathbf{x}'\|_2^2}{2 \, \ell_\text{process}^2} \right); \\

\epsilon_i | \mathbf{x}_i
&amp;amp; \sim \text{Gaussian}\left( 0, \sigma^2_{\text{noise}, i} \right),
\text{ for } i = 1, \dots, m; \\

\ln \sigma^2_{\text{noise}, i}
&amp;amp; = \alpha_\text{noise} + \boldsymbol{\beta}_\text{noise}^\intercal \, \mathbf{x}_i,
\text{ for } i = 1, \dots, m; \\

\sigma_\text{process}
&amp;amp; \sim \text{Half-Gaussian}\left( 0, 1 \right); \\

\ell_\text{process}
&amp;amp; \sim \text{Inverse Gamma}\left( 1, 1 \right); \\

\alpha_\text{noise}
&amp;amp; \sim \text{Gaussian}\left( -1, 1 \right); \text{ and} \\

\beta_{\text{noise}, i}
&amp;amp; \sim \text{Gaussian}\left( 0, 1 \right),
\text{ for } i = 1, \dots, d.\\
\end{align}\]

&lt;p&gt;This concludes the modeling part. The remaining two steps are to infer the
parameters and to make predictions using the posterior predictive distribution.&lt;/p&gt;

&lt;h1 id=&quot;inference&quot;&gt;Inference&lt;/h1&gt;

&lt;p&gt;The model is analytically intractable; one has to resort to sampling or
variational methods for inferring the parameters. We shall use Hamiltonian
Markov chain Monte Carlo sampling via &lt;a href=&quot;https://mc-stan.org/&quot;&gt;Stan&lt;/a&gt;. The model can be seen in the
following listing, where the notation closely follows the one used throughout
the article:&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;transformed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rep_vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ell_process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;real&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cov_exp_quad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ell_process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_noise_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_noise&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cholesky_decompose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_noise_squared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_normal_cholesky&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;sigma_process&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ell_process&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inv_gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;alpha_noise&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;beta_noise&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parameters&lt;/code&gt; block, one can find the \(d + 3\) parameters identified
earlier. In regards to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; block, it is worth noting that there is no
any Gaussian process distribution in Stan. Instead, a multivariate Gaussian
distribution is utilized to model \(f\) at \(\mathbf{X} = (\mathbf{x}_i)_{i =
1}^m \in \mathbb{R}^{m \times d}\) and eventually \(\mathbf{y} = (y_i)_{i =
1}^m\), which is for a good reason. Even though a Gaussian process is an
infinite-dimensional object, in practice, one always works with finite amounts
of data. For instance, in the running example, there are only 221 data points.
By definition, a Gaussian process is a stochastic process with the condition
that any finite collection of points from this process has a multivariate
Gaussian distribution. This fact combined with the conditional independence of
the process and the noise given the covariates yields the following and explains
the usage of a multivariate Gaussian distribution:&lt;/p&gt;

\[\mathbf{y} | \mathbf{X}, \sigma_\text{process}, \ell_\text{process}, \alpha_\text{noise}, \boldsymbol{\beta}_\text{noise}
\sim \text{Multivariate Gaussian}\left( \mathbf{0}, \mathbf{K} + \mathbf{D} \right)\]

&lt;p&gt;where \(\mathbf{K} \in \mathbb{R}^{m \times m}\) is a covariance matrix computed
by evaluating the covariance function \(k\) at all pairs of locations in the
observed data, and \(\mathbf{D} = \text{diag}(\sigma^2_{\text{noise}, i})_{i =
1}^m \in \mathbb{R}^{m \times m}\) is a diagonal matrix of the variances of the
noise at the corresponding locations.&lt;/p&gt;

&lt;p&gt;After running the inference, the following posterior distributions are obtained:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-06-22-gaussian-process/posterior-parameters-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The intervals are at the bottom of the densities are 66% and 95% equal-tailed
probability intervals, and the dots indicate the medians. Let us also take a
look at the 95% probability interval for the noise with respect to the distance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-06-22-gaussian-process/posterior-predictive-noise-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As expected, the variance of the noise increases with the distance.&lt;/p&gt;

&lt;h1 id=&quot;prediction&quot;&gt;Prediction&lt;/h1&gt;

&lt;p&gt;Suppose there are \(n\) locations \(\mathbf{X}_\text{new} =
(\mathbf{x}_{\text{new}, i})_{i = 1}^n \in \mathbb{R}^{n \times d}\) where one
wishes to make predictions. Let \(\mathbf{f}_\text{new} \in \mathbb{R}^n\) be
the values of \(f\) at those locations. Assuming all the data and parameters
given, the joint distribution of \(\mathbf{y}\) and \(\mathbf{f}_\text{new}\) is
as follows:&lt;/p&gt;

\[\left[
  \begin{matrix}
    \mathbf{y} \\
    \mathbf{f}_\text{new}
  \end{matrix}
\right]
\sim \text{Multivariate Gaussian}\left(
  \mathbf{0},
  \left[
    \begin{matrix}
      \mathbf{K} + \mathbf{D} &amp;amp; k(\mathbf{X}, \mathbf{X}_\text{new}) \\
      k(\mathbf{X}_\text{new}, \mathbf{X}) &amp;amp; k(\mathbf{X}_\text{new}, \mathbf{X}_\text{new})
    \end{matrix}
  \right]
\right)\]

&lt;p&gt;where, with a slight abuse of notation, \(k(\cdot, \cdot)\) stands for a
covariance matrix computed by evaluating the covariance function \(k\) at the
specified locations, which is analogous to \(\mathbf{K}\). It is well known (see
&lt;a href=&quot;http://www.gaussianprocess.org/gpml&quot;&gt;Rasmussen et al. 2006&lt;/a&gt;, for instance) that the marginal
distribution of \(\mathbf{f}_\text{new}\) is a multivariate Gaussian with the
following mean vector and covariance matrix, respectively:&lt;/p&gt;

\[\begin{align}
E(\mathbf{f}_\text{new})
&amp;amp; = k(\mathbf{X}_\text{new}, \mathbf{X})(\mathbf{K} + \mathbf{D})^{-1} \, \mathbf{y} \quad \text{and} \\
\text{cov}(\mathbf{f}_\text{new})
&amp;amp; = k(\mathbf{X}_\text{new}, \mathbf{X}_\text{new})
- k(\mathbf{X}_\text{new}, \mathbf{X})(\mathbf{K} + \mathbf{D})^{-1} k(\mathbf{X}, \mathbf{X}_\text{new}).
\end{align}\]

&lt;p&gt;The final component is the noise, as per Equation (1). The noise does not change
the mean of the multivariate Gaussian distribution but does magnify the
variance:&lt;/p&gt;

\[\begin{align}
E(\mathbf{y}_\text{new})
&amp;amp; = k(\mathbf{X}_\text{new}, \mathbf{X})(\mathbf{K} + \mathbf{D})^{-1} \, \mathbf{y} \quad \text{and} \\
\text{cov}(\mathbf{y}_\text{new})
&amp;amp; = k(\mathbf{X}_\text{new}, \mathbf{X}_\text{new})
- k(\mathbf{X}_\text{new}, \mathbf{X})(\mathbf{K} + \mathbf{D})^{-1} k(\mathbf{X}, \mathbf{X}_\text{new})
+ \text{diag}(\sigma^2_\text{noise}(\mathbf{X}_\text{new}))
\end{align}\]

&lt;p&gt;where \(\text{diag}(\sigma^2_\text{noise}(\cdot))\) stands for a diagonal matrix
composed of the noise variance evaluated at the specified locations, which is
analogous to \(\mathbf{D}\).&lt;/p&gt;

&lt;p&gt;Given a set of draws from the joint posterior distribution of the parameters and
the last two expressions, it is now straightforward to draw samples from the
posterior predictive distribution of the response: for each draw of the
parameters, one has to evaluate the mean vector and the covariance matrix and
sample the corresponding multivariate Gaussian distribution. The result is given
in the following figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-06-22-gaussian-process/posterior-predictive-heteroscedastic-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graph shows the mean value of the posterior predictive distribution given by
the black line along with a 95% equal-tailed probability band about the mean. It
can be seen that the uncertainty in the predictions is adequately captured along
the entire support. Naturally, the full predictive posterior distribution is
available at any location of interest.&lt;/p&gt;

&lt;p&gt;Before we conclude, let us illustrate what would happen if the data were modeled
as having homogeneous noise. To this end, the variance of the noise is assumed
to be independent of the covariates, as in Equation (2). After repeating the
inference and prediction processes, the following is obtained:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-06-22-gaussian-process/posterior-predictive-homoscedastic-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The inference is inadequate, which can be seen by the probability band: the
variance is largely overestimated on the left-hand side and underestimated on
the right-hand side. This justifies well the choice of heteroscedastic
regression presented earlier.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this article, it has been illustrated how a functional relationship can be
modeled using a Gaussian process as a prior. Particular attention has been
dedicated to adequately capturing error terms in the presence of
heteroscedasticity. In addition, a practical implementation has been discussed,
and the experimental results have demonstrated the appropriateness of this
approach.&lt;/p&gt;

&lt;p&gt;For the curious reader, the source code of this &lt;a href=&quot;https://github.com/IvanUkhov/blog/blob/master/_posts/2020-06-22-gaussian-process.Rmd&quot;&gt;notebook&lt;/a&gt; along with a number
of auxiliary &lt;a href=&quot;https://github.com/IvanUkhov/blog/tree/master/_scripts/2020-06-22-gaussian-process&quot;&gt;scripts&lt;/a&gt;, such as the definition of the model in Stan, can be
found on GitHub.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h1&gt;

&lt;p&gt;I would like to thank &lt;a href=&quot;https://www.mattiasvillani.com/&quot;&gt;Mattias Villani&lt;/a&gt; for the insightful and informative
graduate course in statistics titled “&lt;a href=&quot;https://github.com/mattiasvillani/AdvBayesLearnCourse&quot;&gt;Advanced Bayesian learning&lt;/a&gt;,” which was the inspiration behind writing this article.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Carl Rasmussen &lt;em&gt;et al.&lt;/em&gt;, &lt;a href=&quot;http://www.gaussianprocess.org/gpml&quot;&gt;&lt;em&gt;Gaussian Processes for Machine
Learning&lt;/em&gt;&lt;/a&gt;, the MIT Press, 2006.&lt;/li&gt;
  &lt;li&gt;David Ruppert &lt;em&gt;et al.&lt;/em&gt;, &lt;a href=&quot;http://www.stat.tamu.edu/~carroll/semiregbook&quot;&gt;&lt;em&gt;Semiparametric Regression&lt;/em&gt;&lt;/a&gt;, Cambridge
University Press, 2003.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;“&lt;a href=&quot;https://mc-stan.org/docs/2_19/stan-users-guide/fit-gp-section.html#priors-for-marginal-standard-deviation&quot;&gt;Priors for marginal standard deviation&lt;/a&gt;,”
  Stan User’s Guide, 2020. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;“&lt;a href=&quot;https://mc-stan.org/docs/2_19/stan-users-guide/fit-gp-section.html#priors-for-length-scale&quot;&gt;Priors for length-scale&lt;/a&gt;,” Stan User’s Guide,
  2020. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Ivan Ukhov</name></author><summary type="html">Gaussian process regression is a nonparametric Bayesian technique for modeling relationships between variables of interest. The vast flexibility and rigor mathematical foundation of this approach make it the default choice in many problems involving small- to medium-sized data sets. In this article, we illustrate how Gaussian process regression can be utilized in practice. To make the case more compelling, we consider a setting where linear regression would be inadequate. The focus will be not on getting the job done as fast as possible but on learning the technique and understanding the choices being made.</summary></entry><entry><title type="html">What is the easiest way to compare two data sets?</title><link href="https://blog.ivanukhov.com/2020/04/10/comparison.html" rel="alternate" type="text/html" title="What is the easiest way to compare two data sets?" /><published>2020-04-10T06:00:00+00:00</published><updated>2020-04-10T06:00:00+00:00</updated><id>https://blog.ivanukhov.com/2020/04/10/comparison</id><content type="html" xml:base="https://blog.ivanukhov.com/2020/04/10/comparison.html">&lt;p&gt;One has probably come across this problem numerous times. There are two versions
of a tabular data set with a lot of columns of different types, and one wants to
quickly identify any differences between the two. For example, the pipeline
providing data to a predictive model might have been updated, and the goal is to
understand if there have been any side effects of this update for the training
data.&lt;/p&gt;

&lt;p&gt;One solution is to start to iterate over the columns of the two tables,
computing five-number summaries and plotting histograms or identifying distinct
values and plotting bar charts, depending on the column’s type. However, this
can quickly get out of hand and evolve into an endeavor for the rest of the day.&lt;/p&gt;

&lt;p&gt;An alternative is to leverage the amazing tools that already exist in the data
community.&lt;/p&gt;

&lt;h1 id=&quot;solution&quot;&gt;Solution&lt;/h1&gt;

&lt;p&gt;The key takeaway is the following three lines of code, excluding the import:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow_data_validation&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dv&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;statistics_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate_statistics_from_dataframe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;statistics_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate_statistics_from_dataframe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;visualize_statistics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lhs_statistics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;statistics_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;rhs_statistics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;statistics_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is all it takes to get a versatile dashboard embedded right into a cell of
a Jupyter notebook. The visualization itself is based on &lt;a href=&quot;https://pair-code.github.io/facets&quot;&gt;Facets&lt;/a&gt;, and it is
conveniently provided by &lt;a href=&quot;https://www.tensorflow.org/tfx/data_validation/get_started&quot;&gt;TensorFlow Data Validation&lt;/a&gt; (which does not have much
to do with TensorFlow and can be used stand-alone).&lt;/p&gt;

&lt;p&gt;It is pointless to try to describe in words what the dashboard can do; instead,
here is a demonstration taken from &lt;a href=&quot;https://pair-code.github.io/facets&quot;&gt;Facets&lt;/a&gt; where the tool is applied the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Census+Income&quot;&gt;UCI
Census Income&lt;/a&gt; data set:&lt;/p&gt;

&lt;div id=&quot;facets-overview-container&quot;&gt;&lt;/div&gt;

&lt;p&gt;Go ahead and give a try to all the different controls!&lt;/p&gt;

&lt;p&gt;In this case, it is helpful to toggle the “percentages” checkbox, since the data
sets are of different sizes. Then it becomes apparent that the two partitions
are fairly balanced. The only problem is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Target&lt;/code&gt;, which represents income,
happened to be encoded incorrectly in the partition for testing.&lt;/p&gt;

&lt;p&gt;Lastly, an example in a Jupyter notebook can be found on &lt;a href=&quot;https://github.com/chain-rule/example-comparison/blob/master/census.ipynb&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;It can be difficult to navigate and particularly challenging to compare wide
data sets. A lot of effort can be put into this exercise. However, the landscape
of open-source tools has a lot to offer too. Facets is one such example. The
library and its straightforward availability via TensorFlow Data Validation are
arguably less known. This short note can hopefully rectify this to some extent.&lt;/p&gt;</content><author><name>Ivan Ukhov</name></author><summary type="html">One has probably come across this problem numerous times. There are two versions of a tabular data set with a lot of columns of different types, and one wants to quickly identify any differences between the two. For example, the pipeline providing data to a predictive model might have been updated, and the goal is to understand if there have been any side effects of this update for the training data.</summary></entry><entry><title type="html">Bayesian inference of the net promoter score via multilevel regression with poststratification</title><link href="https://blog.ivanukhov.com/2020/02/03/net-promoter.html" rel="alternate" type="text/html" title="Bayesian inference of the net promoter score via multilevel regression with poststratification" /><published>2020-02-03T07:00:00+00:00</published><updated>2020-02-03T07:00:00+00:00</updated><id>https://blog.ivanukhov.com/2020/02/03/net-promoter</id><content type="html" xml:base="https://blog.ivanukhov.com/2020/02/03/net-promoter.html">&lt;p&gt;Customer surveys are naturally prone to biases. One prominent example is
participation bias, which arises when individuals decide not to respond to the
survey, and this pattern is not random. For instance, new customers might reply
less eagerly than those who are senior. This renders the obtained responses
unrepresentative of the target population. In this article, we tackle
participation bias for the case of the net promoter survey by means of
multilevel regression and poststratification.&lt;/p&gt;

&lt;p&gt;More specifically, the discussion here is a sequel to “&lt;a href=&quot;/2019/08/19/net-promoter.html&quot;&gt;A Bayesian approach to
the inference of the net promoter score&lt;/a&gt;,” where we built a
hierarchical model for inferring the net promoter score for an arbitrary
segmentation of a customer base. The reader is encouraged to skim over that
article to recall the mechanics of the score and the structure of the model that
was constructed. In that article, there was an assumption made that the sample
was representative of the population, which, as mentioned earlier, is often not
the case. In what follows, we mitigate this problem using a technique called
poststratification. The technique works by matching proportions observed in the
sample with those observed in the population with respect to several dimensions,
such as age, country, and gender. However, in order to be able to poststratify,
the model has to have access to all these dimensions at once, which the model
built earlier is not suited for. To enable this, we switch gears to multilevel
multinomial regression.&lt;/p&gt;

&lt;h1 id=&quot;problem&quot;&gt;Problem&lt;/h1&gt;

&lt;p&gt;Suppose the survey is to measure the net promoter score for a population that
consists of \(N\) customers. The score is to be reported with respect to
individual values of \(M\) grouping variables where variable \(i\) has \(m_i\)
possible values, for \(i = 1, \dots, M\). For instance, it might be important to
know the score for different age groups, in which case the variable would be the
customer’s age with values such as 18–25, 26–35, and so on. This implies that,
in total, \(\sum_i m_i\) scores have to be estimated.&lt;/p&gt;

&lt;p&gt;Depending on the size of the business, one might or might not try to reach out
to all customers, except for those who have opted out of communications.
Regardless of the decision, the resulting sample size, which is denoted by
\(n\), is likely to be substantially smaller than \(N\), as the response rate is
typically low. Therefore, there is uncertainty about the opinion of those who
abstained or were not targeted.&lt;/p&gt;

&lt;p&gt;More importantly, a random sample is desired; however, certain subpopulations of
customers might end up being significantly overrepresented due to participation
bias, driving the score astray. Let us quantify this concern. We begin by taking
the Cartesian product of the aforementioned \(M\) variables. This results in \(K
= \prod_i m_i\) distinct combinations of the variables’ values, which are
referred to as cells in what follows. For each cell, the number of detractors,
neutrals, and promoters observed in the sample are computed and denoted by
\(d_i\), \(u_i\), and \(p_i\), respectively. The number of respondents in call
\(i\) is then&lt;/p&gt;

\[n_i = d_i + u_i + p_i \tag{1}\]

&lt;p&gt;for \(i = 1, \dots, K\). For convenience, all counts are arranged in the
following matrix:&lt;/p&gt;

\[y = \left(
\begin{matrix}
y_1 \\
\vdots \\
y_i \\
\vdots \\
y_K
\end{matrix}
\right)
= \left(
\begin{matrix}
d_1 &amp;amp; u_1 &amp;amp; p_1 \\
\vdots &amp;amp; \vdots &amp;amp; \vdots \\
d_i &amp;amp; u_i &amp;amp; p_i \\
\vdots &amp;amp; \vdots &amp;amp; \vdots \\
d_K &amp;amp; u_K &amp;amp; p_K
\end{matrix}
\right). \tag{2}\]

&lt;p&gt;Given \(y\), the observed net promoter score for value \(j\) of variable \(i\)
can be evaluated as follows:&lt;/p&gt;

\[s^i_j = 100 \times \frac{\sum_{k \in I^i_j}(p_k - d_k)}{\sum_{k \in I^i_j} n_k} \tag{3}\]

&lt;p&gt;where \(I^i_j\) is an index set traversing cells with variable \(i\) set to
value \(j\), which has the effect of marginalizing out other variables
conditioned on the chosen value of variable \(i\), that is, on value \(j\).&lt;/p&gt;

&lt;p&gt;We can now compare \(n_i\), computed according to Equation (1), with its
counterpart in the population (the total number of customers who belong to cell
\(i\)), which is denoted by \(N_i\), taking into consideration the sample size
\(n\) and the population size \(N\). Problems occur when the ratios within one
or more of the following tuples largely disagree:&lt;/p&gt;

\[\left(\frac{n_i}{n}, \frac{N_i}{N}\right) \tag{4}\]

&lt;p&gt;for \(i = 1, \dots, K\). When this happens, the scores given by Equation (3) or
any analyses oblivious of this disagreement cannot be trusted, since they
misrepresent the population. (It should be noted, however, that equality within
each tuple does not guarantee the absence of participation bias, since there
might be other, potentially unobserved, dimensions along which there are
deviations.)&lt;/p&gt;

&lt;p&gt;The survey has been conducted, and there are deviations. What do we do with all
these responses that have come in? Should we discard and run a new survey,
hoping that, this time, it would be different?&lt;/p&gt;

&lt;h1 id=&quot;solution&quot;&gt;Solution&lt;/h1&gt;

&lt;p&gt;The fact that the sample covers only a fraction of the population is, of course,
no news, and the solution is standard: one has to infer the net promoter score
for the population given the sample and domain knowledge. This is what was done
in the &lt;a href=&quot;/2019/08/19/net-promoter.html&quot;&gt;previous article&lt;/a&gt; for one grouping variable. However, due to
participation bias, additional measures are needed as follows.&lt;/p&gt;

&lt;p&gt;Taking inspiration from political science, we proceed in two steps.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Using an adequate model, \(K = \prod_i m_i\) net promoter scores are
inferred—one for each cell, that is, for each combination of the values of
the grouping variables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The \(\prod_i m_i\) “cell-scores” are combined to produce \(\sum_i m_i\)
“value-scores”—one for each value of each variable. This is done in such a
way that the contribution of each cell to the score is equal to the relative
size of that cell in the population given by Equation (4).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The two steps are discussed in the following two subsections.&lt;/p&gt;

&lt;h2 id=&quot;modeling&quot;&gt;Modeling&lt;/h2&gt;

&lt;p&gt;Step 1 can, in principle, be undertaken by any model of choice. A prominent
candidate is multilevel multinomial regression, which is what we shall explore.
&lt;em&gt;Multilevel&lt;/em&gt; refers to having a hierarchical structure where parameters on a
higher level give birth to parameters on a lower level, which, in particular,
enables information exchange through a common ancestor. &lt;em&gt;Multinomial&lt;/em&gt; refers to
the distribution used for modeling the response variable. The family of
multinomial distributions is appropriate, since we work with counts of events
falling into one of several categories: detractors, neutrals, and promoters; see
Equation (2). The response for each cell is then as follows:&lt;/p&gt;

\[y_i | \theta_i \sim \text{Multinomial}(n_i, \theta_i)\]

&lt;p&gt;where \(n_i\) is given by Equation (1), and&lt;/p&gt;

\[\theta_i = \left\langle\theta^d_i, \theta^u_i, \theta^p_i\right\rangle\]

&lt;p&gt;is a simplex (sums up to one) of probabilities of the three categories.&lt;/p&gt;

&lt;p&gt;Multinomial regression belongs to the class of generalized linear models. This
means that the inference takes place in a linear domain, and that \(\theta_i\)
is obtained by applying a deterministic transformation to the corresponding
linear model or models; the inverse of this transformation is known as the link
function. In the case of multinomial regression, the aforementioned
transformation is the softmax function, which is a generalization of the
logistic function allowing more than two categories:&lt;/p&gt;

\[\theta_i = \text{Softmax}\left(\mu_i\right)\]

&lt;p&gt;where&lt;/p&gt;

\[\mu_i = \left(0, \mu^u_i, \mu^p_i\right)\]

&lt;p&gt;is the average log-odds of the three categories with respect to a reference
category, which, by conventions, is taken to be the first one, that is,
detractors. The first entry is zero, since \(\ln(1) = 0\). Therefore, there are
only two linear models: one is for neutrals (\(\mu^u_i\)), and one is for
promoters (\(\mu^p_i\)).&lt;/p&gt;

&lt;p&gt;Now, there are many alternatives when it comes to the two linear parts. In this
article, we use the following architecture. Both the model for neutrals and the
one for promoters have the same structure, and for brevity, only the former is
described. For the log-odds of neutrals, the model is&lt;/p&gt;

\[\mu^u_i = b^u + \sum_{j = 1}^M \delta^{uj}_{I_j[i]}\]

&lt;p&gt;where&lt;/p&gt;

\[\delta^{uj} = \left(\delta^{uj}_1, \dots, \delta^{uj}_{m_j}\right)\]

&lt;p&gt;is a vector of deviations from intercept \(b^u\) specific to grouping variable
\(j\) (one entry for each value of the variable), and \(I_j[i]\) yields the
index of the value that cell \(i\) has, for \(i = 1, \dots, K\) and \(j = 1,
\dots, M\).&lt;/p&gt;

&lt;p&gt;Let us now turn to the multilevel aspect. For each grouping variable, the
corresponding values, represented by the elements of \(\delta^{uj}\), are
allowed to be different but assumed to have something in common and thus
originate from a common distribution. To this end, they are assigned
distributions with a shared parameter as follows:&lt;/p&gt;

\[\delta^{uj}_i | \sigma^{uj} \sim \text{Gaussian}\left(0, \sigma^{uj}\right)\]

&lt;p&gt;for \(i = 1, \dots, m_j\). The mean is zero, since \(\delta^{uj}_i\) represents
a deviation.&lt;/p&gt;

&lt;p&gt;Lastly, we have to decide on prior distributions of the intercept, \(b^u\), and
the standard deviations, \(\sigma^{uj}\) for \(j = 1, \dots, M\). The intercept
is given the following prior:&lt;/p&gt;

\[b^u \sim \text{Student’s t}(5, 0, 1).\]

&lt;p&gt;The mean is zero in order to center at even odds. Regarding the standard
deviations, they are given the following prior:&lt;/p&gt;

\[\sigma^{uj} \sim \text{Half-Student’s t}(5, 0, 1).\]

&lt;p&gt;In order to understand the implications of these prior choices, let us take a
look at the prior distribution assuming two grouping variables:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-02-03-net-promoter/prior-distribution-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The left and right dashed lines demarcate tail regions that, for practical
purposes, can be thought of as “never” and “always,” respectively. For instance,
log-odds of five or higher are so extreme that detractors are rendered nearly
non-existent when compared to neutrals. These regions are arguably unrealistic.
The prior does not exclude these possibilities; however, it does not favor them
either. The vast majority of the probability mass is still in the middle around
zero.&lt;/p&gt;

&lt;p&gt;The overall model is then as follow:&lt;/p&gt;

\[\begin{align}
&amp;amp; y_i | \theta_i \sim \text{Multinomial}(n_i, \theta_i),
\text{ for } i = 1, \dots, K; \\
&amp;amp; \theta_i = \text{Softmax}\left(\mu_i\right),
\text{ for } i = 1, \dots, K; \\
&amp;amp; \mu_i = (0, \mu^u_i, \mu^p_i),
\text{ for } i = 1, \dots, K; \\
&amp;amp; \mu^u_i = b^u + \sum_{j = 1}^M \delta^{uj}_{I_j[i]},
\text{ for } i = 1, \dots, K; \\
&amp;amp; \mu^p_i = b^p + \sum_{j = 1}^M \delta^{pj}_{I_j[i]},
\text{ for } i = 1, \dots, K; \\
&amp;amp; b^u \sim \text{Student’s t}(5, 0, 1); \\
&amp;amp; b^p \sim \text{Student’s t}(5, 0, 1); \\
&amp;amp; \delta^{uj}_k | \sigma^{uj} \sim \text{Gaussian}\left(0, \sigma^{uj}\right),
\text{ for } j = 1, \dots, M \text{ and } k = 1, \dots, m_j; \tag{5a} \\
&amp;amp; \delta^{pj}_k | \sigma^{pj} \sim \text{Gaussian}\left(0, \sigma^{pj}\right),
\text{ for } j = 1, \dots, M \text{ and } k = 1, \dots, m_j; \tag{5b} \\
&amp;amp; \sigma^{uj} \sim \text{Half-Student’s t}(5, 0, 1),
\text{ for } j = 1, \dots, M; \text{ and} \\
&amp;amp; \sigma^{pj} \sim \text{Half-Student’s t}(5, 0, 1),
\text{ for } j = 1, \dots, M.
\end{align}\]

&lt;p&gt;The model has \(2 \times (1 + \sum_i m_i + M)\) parameters in total. The
structure that can be seen in Equations (5a) and (5b) is what makes the model
multilevel. This is an important feature, since it allows for information
sharing between the individual values of the grouping variables. In particular,
this has a regularizing effect on the estimates, which is also known as
shrinkage resulting from partial pooling.&lt;/p&gt;

&lt;p&gt;Having defined the model, the posterior distribution can now be obtained by
means of Markov chain Monte Carlo sampling. This procedure is standard and can
be performed using, for instance, Stan or a higher-level package, such as
&lt;a href=&quot;https://github.com/paul-buerkner/brms&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;brms&lt;/code&gt;&lt;/a&gt;, which is what is exemplified in the Implementation section. The result
is a collection of draws of the parameters from the posterior distribution. For
each draw of the parameters, a draw of the net promoter score can be computed
using the following formula:&lt;/p&gt;

\[s_i = 100 \times (\theta^p_i - \theta^d_i) \tag{6}\]

&lt;p&gt;for \(i = 1, \dots, K\). This means that we have obtained a (joint) posterior
distribution of the net promoter score over the \(K\) cells. It is now time to
combine the scores for the cells on the level of the values of the \(M\)
grouping variables, which results in \(\sum_i m_i\) scores in total.&lt;/p&gt;

&lt;h2 id=&quot;poststratification&quot;&gt;Poststratification&lt;/h2&gt;

&lt;p&gt;Step 2 is poststratification, whose purpose is to correct for potential
deviations of the sample from the population; recall the discussion around
Equation (4). The foundation laid in the previous subsection makes the work here
straightforward. The idea is as follows. Each draw from the posterior
distribution consists of \(K\) values for the net promoter score, one for each
cell. All one has to do in order to correct for a mismatch in proportions is to
take a weighted average of these scores where the weights are the counts
observed in the population:&lt;/p&gt;

\[s^i_j = \frac{\sum_{k \in I^i_j} N_k \, s_k}{\sum_{k \in I^i_j} N_k}\]

&lt;p&gt;where \(I^i_j\) is as in Equation (3), for \(i = 1, \dots, M\) and \(j = 1,
\dots, m_i\). The above gives a poststratified draw from the posterior
distribution of the net promoter score for variable \(i\) and value \(j\). In
practice, depending on the tool used, one might perform the poststratification
procedure differently, such as predicting counts of detractors, neutrals, and
promoters in the cells given their in-population sizes and then aggregating
those counts and following the definition of the net promoter score.&lt;/p&gt;

&lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt;

&lt;p&gt;In what follows, we consider a contrived example with the sole purpose of
illustrating how the presented workflow can be implemented in practice. To this
end, we generate some data with two grouping variables, age and seniority, and
then perform inference using &lt;a href=&quot;https://github.com/paul-buerkner/brms&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;brms&lt;/code&gt;&lt;/a&gt;, which leverages Stan under the hood. For
a convenient manipulation of posterior draws, &lt;a href=&quot;https://github.com/mjskay/tidybayes&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tidybayes&lt;/code&gt;&lt;/a&gt; is used as well.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;brms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tidybayes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tidyverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set.seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mc.cores&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detectCores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Load data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; list(&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# =&amp;gt;   population = tibble(age, seniority, cell_size),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# =&amp;gt;   sample = tibble(age, seniority, cell_size,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# =&amp;gt;                   cell_counts = (detractors, neutrals, promoters))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; )&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Modeling&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;priors&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'student_t(5, 0, 1)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Intercept'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpar&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'muneutral'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'student_t(5, 0, 1)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Intercept'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpar&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'mupromoter'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'student_t(5, 0, 1)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'sd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpar&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'muneutral'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'student_t(5, 0, 1)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'sd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpar&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'mupromoter'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;brmsformula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell_counts&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seniority&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;brm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multinomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;priors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
             &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;control&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adapt_delta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Poststratification&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_predicted_draws&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.category&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.draw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summarize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;promoter&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detractor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_hdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The final aggregation is given for age; it is similar for seniority. It can be
seen in the above listing that modern tools allow for rather complex ideas to be
expressed and explored in a very laconic way.&lt;/p&gt;

&lt;p&gt;The curious reader is encouraged to run the above code. The appendix contains a
function for generating synthetic data. It should be noted, however, that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;brms&lt;/code&gt;
and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tidybayes&lt;/code&gt; should be of versions greater than 2.11.1 and 2.0.1,
respectively, which, at the time of writing, are available for installation only
on GitHub. The appendix contains instructions for updating the packages.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this article, we have discussed a multilevel multinomial model for inferring
the net promoter score with respect to several grouping variables in accordance
with the business needs. It has been argued that poststratification is an
essential stage of the inference process, since it mitigates the deleterious
consequences of participation bias on the subsequent decision-making.&lt;/p&gt;

&lt;p&gt;There are still some aspects that could be improved. For instance, there is a
natural ordering to the three categories of customers, detractors, neutrals, and
promoters; however, it is currently ignored. Furthermore, there is some
information thrown away when customer-level scores, which range from zero to
ten, are aggregated on the category level. Lastly, the net promoter survey often
happens in periodic waves, which calls for a single model capturing and learning
from changes over time.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h1&gt;

&lt;p&gt;I would like to thank &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/&quot;&gt;Andrew Gelman&lt;/a&gt; for the guidance on multilevel modeling
and &lt;a href=&quot;https://paul-buerkner.github.io/&quot;&gt;Paul-Christian Bürkner&lt;/a&gt; for the help with understanding the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;brms&lt;/code&gt; package.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Andrew Gelman et al., “&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/unpublished/MRT(1).pdf&quot;&gt;Using multilevel regression and poststratification to
estimate dynamic public opinion&lt;/a&gt;,” 2018.&lt;/li&gt;
  &lt;li&gt;Andrew Gelman and Jennifer Hill, &lt;em&gt;&lt;a href=&quot;https://doi.org/10.1017/CBO9780511790942&quot;&gt;Data Analysis Using Regression and
Multilevel/Hierarchical Models&lt;/a&gt;&lt;/em&gt;, Cambridge University Press, 2006.&lt;/li&gt;
  &lt;li&gt;Andrew Gelman and Thomas Little, “&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/poststrat3.pdf&quot;&gt;Poststratification into many categories
using hierarchical logistic regression&lt;/a&gt;,” Survey Methodology, 1997.&lt;/li&gt;
  &lt;li&gt;Paul-Christian Bürkner, “&lt;a href=&quot;http://dx.doi.org/10.18637/jss.v080.i01&quot;&gt;brms: An R package for Bayesian multilevel models
using Stan&lt;/a&gt;,” Journal of Statistical Software, 2017.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;appendix&quot;&gt;Appendix&lt;/h1&gt;

&lt;p&gt;The following listing defines a function that makes the illustrative example
given in the Implementation section self-sufficient. By default, the population
contains one million customers, and the sample contains one percent. There are
two grouping variables: age with six values and seniority with seven values.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age_values&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'18–25'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'26–35'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'36–45'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'46–55'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'56–65'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'66+'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age_probabilities&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Seniority&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seniority_values&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'6M'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'1Y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'2Y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'3Y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'4Y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'5Y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'6Y+'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seniority_probabilities&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Score&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_values&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_probabilities&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Generate a population&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tibble&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age_probabilities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seniority&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seniority_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seniority_probabilities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Take a sample from the population&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_probabilities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;category&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;case_when&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'detractor'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'promoter'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'neutral'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Summarize the population&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seniority&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'cell_size'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Summarize the sample&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seniority&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summarize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detractors&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;category&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'detractor'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
              &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;neutrals&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;category&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'neutral'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
              &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;promoters&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;category&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'promoter'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell_size&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detractors&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;neutrals&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;promoters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Bind counts of neutrals, detractors, and promoters (needed for brms)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell_counts&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cbind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detractors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;neutrals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;promoters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colnames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'detractor'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'neutral'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'promoter'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Remove unused columns&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detractors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;neutrals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;promoters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Lastly, the following snippet shows how to update &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;brms&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tidybayes&lt;/code&gt; from
GitHub:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;packageVersion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'brms'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'2.11.2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remotes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install_github&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'paul-buerkner/brms'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upgrade&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'never'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;packageVersion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'tidybayes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'2.0.1.9000'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remotes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install_github&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'mjskay/tidybayes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upgrade&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'never'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Ivan Ukhov</name></author><summary type="html">Customer surveys are naturally prone to biases. One prominent example is participation bias, which arises when individuals decide not to respond to the survey, and this pattern is not random. For instance, new customers might reply less eagerly than those who are senior. This renders the obtained responses unrepresentative of the target population. In this article, we tackle participation bias for the case of the net promoter survey by means of multilevel regression and poststratification.</summary></entry><entry><title type="html">Ingestion of sequential data from BigQuery into TensorFlow</title><link href="https://blog.ivanukhov.com/2019/11/08/sequential-data.html" rel="alternate" type="text/html" title="Ingestion of sequential data from BigQuery into TensorFlow" /><published>2019-11-08T07:00:00+00:00</published><updated>2019-11-08T07:00:00+00:00</updated><id>https://blog.ivanukhov.com/2019/11/08/sequential-data</id><content type="html" xml:base="https://blog.ivanukhov.com/2019/11/08/sequential-data.html">&lt;p&gt;How hard can it be to ingest sequential data into a &lt;a href=&quot;https://www.tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; model? As
always, the answer is, “It depends.” Where are the sequences in question stored?
Can they fit in main memory? Are they of the same length? In what follows, we
shall build a flexible and scalable workflow for feeding sequential observations
into a TensorFlow graph starting from &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt; as the data warehouse.&lt;/p&gt;

&lt;p&gt;To make the discussion tangible, consider the following problem. Suppose the
goal is to predict the peak temperature at an arbitrary weather station present
in the &lt;a href=&quot;https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn&quot;&gt;Global Historical Climatology Network&lt;/a&gt; for each day between June 1 and
August 31. More concretely, given observations from June 1 up to an arbitrary
day before August 31, the objective is to complete the sequence until August 31.
For instance, if we find ourselves in Stockholm on June 12, we ask for the
maximum temperatures from June 12 to August 31 given the temperature values
between June 1 to June 11 at a weather station in Stockholm.&lt;/p&gt;

&lt;p&gt;To set the expectations right, in this article, we are not going to build a
predictive model but to cater for its development by making the data from the
aforementioned database readily available in a TensorFlow graph. The final chain
of states and operations is as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Historical temperature measurements from the Global Historical Climatology
Network are stored in a &lt;a href=&quot;https://console.cloud.google.com/marketplace/details/noaa-public/ghcn-d&quot;&gt;public data set&lt;/a&gt; in BigQuery. Each row
corresponds to a weather station and a date. There are missing observations
due to such reasons as measurements not passing quality checks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Relevant measurements are grouped in BigQuery by the weather station and
year. Therefore, each row corresponds to a weather station and a year,
implying that all information about a particular example (a specific weather
station on a specific year) is gathered in one place.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The sequences are read, analyzed, and transformed by &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow&lt;/a&gt;.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The data are split into a training, a validation, and a testing set of
examples.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The training set is used to compute statistics needed for transforming the
measurements to a form suitable for the subsequent modeling.
Standardization is used as an example.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The training and validation sets are transformed using the statistics
computed with respect to the training set in order to avoid performing
these computations during the training-with-validation phase. The
corresponding transform is available for the testing phase.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The processed training and validation examples and the raw testing examples
are written by Dataflow to &lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;Cloud Storage&lt;/a&gt; in the &lt;a href=&quot;https://www.tensorflow.org/tutorials/load_data/tfrecord&quot;&gt;TFRecord&lt;/a&gt; format, which is
a format native to TensorFlow.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The files containing TFRecords are read by the &lt;a href=&quot;https://www.tensorflow.org/guide/data&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.data&lt;/code&gt;&lt;/a&gt; API of TensorFlow
and eventually transformed into a data set of appropriately padded batches of
examples.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The above workflow is not as simple as reading data from a Pandas DataFrame
comfortably resting in main memory; however, it is much more scalable. This
pipeline can handle arbitrary amounts of data. Moreover, it operates on
complete examples, not on individual measurements.&lt;/p&gt;

&lt;p&gt;In the rest of the article, the aforementioned steps will be described in more
detail. The corresponding source code can be found in the following repository
on GitHub:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chain-rule/example-weather-forecast&quot;&gt;example-weather-forecast&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;data&quot;&gt;Data&lt;/h1&gt;

&lt;p&gt;It all starts with data. The data come from the Global Historical Climatology
Network, which is &lt;a href=&quot;https://console.cloud.google.com/marketplace/details/noaa-public/ghcn-d&quot;&gt;available in BigQuery&lt;/a&gt; for public use. Steps 1 and 2
in the list above are covered by the &lt;a href=&quot;https://github.com/chain-rule/example-weather-forecast/blob/master/configs/training/data.sql&quot;&gt;following query&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;WITH&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;-- Select relevant measurements&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data_1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;-- Find the date of the previous observation&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;LAG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OVER&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;station_year&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;latitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;longitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;-- Convert to degrees Celsius&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;`bigquery-public-data.ghcn_d.ghcnd_201*`&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;INNER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;JOIN&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;`bigquery-public-data.ghcn_d.ghcnd_stations`&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;USING&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;-- Take years from 2010 to 2019&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;CAST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_TABLE_SUFFIX&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INT64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;-- Take months from June to August&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;EXTRACT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;MONTH&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;-- Take the maximum temperature&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'TMAX'&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;-- Take observations passed spatio-temporal quality-control checks&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qflag&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;WINDOW&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;station_year&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;EXTRACT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;YEAR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;-- Group into examples (a specific station and a specific year)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data_2&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;MIN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;latitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;longitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;-- Compute gaps between observations&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ARRAY_AGG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;DATE_DIFF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IFNULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ARRAY_AGG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data_1&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;longitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;EXTRACT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;YEAR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;-- Partition into training, validation, and testing sets&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;CASE&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;WHEN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;EXTRACT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;YEAR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2019&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;THEN&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'analysis,training'&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;WHEN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MOD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;ABS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FARM_FINGERPRINT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;THEN&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'validation'&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;ELSE&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'testing'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;END&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;mode&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;data_2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The query fetches peak temperatures, denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;temperature&lt;/code&gt;, for all available
weather stations between June and August in 2010–2019. The crucial part is the
usage of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARRAY_AGG&lt;/code&gt;, which is what makes it possible to gather all relevant
data about a specific station and a specific year in the same row. The number of
days since the previous measurement, which is denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;duration&lt;/code&gt;, is also
computed. Ideally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;duration&lt;/code&gt; should always be one (except for the first day,
which has no predecessor); however, this is not the case, which makes the
resulting time series vary in length.&lt;/p&gt;

&lt;p&gt;In addition, in order to illustrate the generality of this approach, two
contextual (that is, non-sequential) explanatory variables are added: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;latitude&lt;/code&gt;
and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;longitude&lt;/code&gt;. They are scalars stored side by side with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;duration&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;temperature&lt;/code&gt;, which are arrays.&lt;/p&gt;

&lt;p&gt;Another important moment in the final &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SELECT&lt;/code&gt; statement, which defines a column
called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mode&lt;/code&gt;. This column indicates what each example is used for, allowing one
to use the same query for different purposes and to avoid inconsistencies due to
multiple queries. In this case, observations prior to 2019 are reserved for
training, while the rest is split pseudo-randomly and reproducibly into two
approximately equal parts: one is for validation, and one is for testing. This
last operation is explained in detail in “&lt;a href=&quot;https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning&quot;&gt;Repeatable sampling of data sets in
BigQuery for machine learning&lt;/a&gt;” by Lak Lakshmanan.&lt;/p&gt;

&lt;h1 id=&quot;preprocessing&quot;&gt;Preprocessing&lt;/h1&gt;

&lt;p&gt;In this section, we cover Steps 4 and 5 in the list given at the beginning. This
job is done by &lt;a href=&quot;https://www.tensorflow.org/tfx&quot;&gt;TensorFlow Extended&lt;/a&gt;, which is a library for building
machine-learning pipelines. Internally, it relies on &lt;a href=&quot;https://beam.apache.org/&quot;&gt;Apache Beam&lt;/a&gt; as a language
for defining pipelines. Once an adequate pipeline is created, it can be executed
using an executor, and the executor that we shall use is &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Cloud Dataflow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Before we proceed to the pipeline itself, the construction process is
orchestrated by a &lt;a href=&quot;https://github.com/chain-rule/example-weather-forecast/blob/master/configs/training/preprocessing.json&quot;&gt;configuration file&lt;/a&gt;, which will
be referred to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config&lt;/code&gt; in the pipeline code (to be discussed shortly):&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;path&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;configs/training/data.sql&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;schema&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;latitude&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;kind&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;float32&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;transform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;z&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;longitude&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;kind&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;float32&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;transform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;z&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;duration&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;kind&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;float32&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;transform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;z&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;temperature&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;kind&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;float32&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;transform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;z&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;modes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;analysis&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;training&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;transform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;analysis&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;shuffle&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;validation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;transform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;analysis&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;testing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;transform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;identity&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is worth noting that this way of working with a separate configuration file
is not something standard that comes with TensorFlow or Beam. It is a
convenience that we build for ourselves in order to keep the main logic reusable
and extendable without touching the code.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; block describes where the data can be found and provides a schema for
the columns that are used. (Recall the SQL query given earlier and note that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;id&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;date&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partition&lt;/code&gt; are omitted.) For instance, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;latitude&lt;/code&gt; is a scale
of type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FLOAT32&lt;/code&gt;, while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;temperature&lt;/code&gt; is a sequence of type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FLOAT32&lt;/code&gt;. Both are
standardized to have a zero mean and a unit standard deviation, which is
indicated by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;transform&quot;: &quot;z&quot;&lt;/code&gt; and is typically needed for training neural
networks.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;modes&lt;/code&gt; block defines four passes over the data, corresponding to four
operating modes. In each mode, a specific subset of examples is considered,
which is given by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mode&lt;/code&gt; column returned by the query. There are two types
of modes: analysis and transform; recall Step 3. Whenever the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transform&lt;/code&gt; key is
present, it is a transform mode; otherwise, it is an analysis mode. In this
example, there are one analysis and three transform modes.&lt;/p&gt;

&lt;p&gt;Below is an excerpt from a &lt;a href=&quot;https://github.com/chain-rule/example-weather-forecast/blob/master/forecast/pipeline.py&quot;&gt;Python class&lt;/a&gt; responsible for building
the pipeline:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# config = ...
# schema = ...
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Read the SQL code
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'path'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Create a BigQuery source
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BigQuerySource&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;use_standard_sql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Create metadata needed later
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_feature_spec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset_metadata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DatasetMetadata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_feature_spec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Read data from BigQuery
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'read'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Loop over modes whose purpose is analysis
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform_functions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'modes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'transform'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Select examples that belong to the current mode
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;data_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'-filter'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Analyze the examples
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;transform_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'-analyze'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tt_beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AnalyzeDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_analyze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_locate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'transform'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Store the transform function
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;transform_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'-write-transform'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform_fn_io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WriteTransformFn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Loop over modes whose purpose is transformation
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'modes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'transform'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Select examples that belong to the current mode
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;data_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'-filter'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Shuffle examples if needed
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'shuffle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_&lt;/span&gt; \
            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'-shuffle'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Reshuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Transform the examples using an appropriate transform function
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'transform'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'identity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;coder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coders&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ExampleProtoCoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meta_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'transform'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; \
            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'-transform'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tt_beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TransformDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;coder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coders&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ExampleProtoCoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_locate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'examples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'part'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Store the transformed examples as TFRecords
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;data_&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'-encode'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'-write-examples'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tfrecordio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WriteToTFRecord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At the very beginning, a BigQuery source is created, which is then branched out
according to the operating modes found in the configuration file. Specifically,
the first for-loop corresponds to the analysis modes, and the second for-loop
goes over the transform modes. The former ends with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WriteTransformFn&lt;/code&gt;, which
saves the resulting transform, and the latter ends with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WriteToTFRecord&lt;/code&gt;, which
writes the resulting examples as TFRecords.&lt;/p&gt;

&lt;p&gt;The distinction between the contextual and sequential features is given by the
&lt;a href=&quot;https://github.com/chain-rule/example-weather-forecast/blob/master/forecast/schema.py&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;schema&lt;/code&gt;&lt;/a&gt; object created based on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;schema&lt;/code&gt; block in the
configuration file. The call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;schema.to_feature_spec()&lt;/code&gt; shown above alternates
between &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.io.FixedLenFeature&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/io/VarLenFeature&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.io.VarLenFeature&lt;/code&gt;&lt;/a&gt; and produces a
feature specification that is understood by TensorFlow and TensorFlow Extended.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/chain-rule/example-weather-forecast&quot;&gt;repository&lt;/a&gt; provides a wrapper for executing the
pipeline on Cloud Dataflow. The following figure shows the flow of the data with
respect to the four operating modes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019-11-08-sequential-data/dataflow.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The outcome is a hierarchy of files on Cloud Storage:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
└── data/
    └── training/
        └── 2019-11-01-12-00-00/
            ├── analysis/
            │   └── transform/
            │       ├── transform_fn/...
            │       └── transform_metadata/...
            ├── testing/
            │   └── examples/
            │       ├── part-000000-of-00004
            │       ├── ...
            │       └── part-000003-of-00004
            ├── training/
            │   └── examples/
            │       ├── part-000000-of-00006
            │       ├── ...
            │       └── part-000005-of-00006
            └── validation/
                └── examples/
                    ├── part-000000-of-00004
                    ├── ...
                    └── part-000003-of-00004
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data/training&lt;/code&gt; contains all data needed for the training phase, which
collectively refers to training entwined with validation and followed by
testing. Moving forward, this hierarchy is meant to accommodate the application
phase as well by populating a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data/application&lt;/code&gt; entry next to the
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data/training&lt;/code&gt; one. It can also accommodate trained models and the results of
applying these models by having a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; entry with a structure similar to the
one of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; entry.&lt;/p&gt;

&lt;p&gt;In the listing above, the files whose name starts with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;part-&lt;/code&gt; are the ones
containing TFRecords. It can be seen that, for each mode, the corresponding
examples have been split into multiple files, which is done for more efficient
access during the usage stage discussed in the next section.&lt;/p&gt;

&lt;h1 id=&quot;execution&quot;&gt;Execution&lt;/h1&gt;

&lt;p&gt;At this point, the data have made it all the way to the execution phase,
referring to training, validation, and testing; however, the data are yet to be
injected into a TensorFlow graph, which is the topic of this section. As before,
relevant parameters are kept in a &lt;a href=&quot;https://github.com/chain-rule/example-weather-forecast/blob/master/configs/training/execution.json&quot;&gt;separate configuration file&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;schema&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;latitude&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;kind&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;float32&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;longitude&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;kind&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;float32&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;duration&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;kind&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;float32&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;temperature&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;kind&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;float32&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;modes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;training&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;spec&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;transformed&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;shuffle_macro&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;buffer_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;interleave&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cycle_length&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;num_parallel_calls&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;shuffle_micro&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;buffer_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;map&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;num_parallel_calls&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;batch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;batch_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;prefetch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;buffer_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;repeat&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;validation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;spec&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;transformed&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;shuffle_macro&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;buffer_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;interleave&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cycle_length&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;num_parallel_calls&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;map&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;num_parallel_calls&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;batch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;batch_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;prefetch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;buffer_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;repeat&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;testing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;spec&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;original&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;interleave&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cycle_length&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;num_parallel_calls&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;map&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;num_parallel_calls&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;batch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;batch_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;prefetch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;buffer_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It can be seen that the file contains only one block: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt;. This is sufficient
for the purposes of this article; however, it is also meant to cover the
construction of the model in mind, including its hyperparameters, and the
execution process, including the optimizer and evaluation metrics.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; block is similar to the one we saw before. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;modes&lt;/code&gt;
describes various calls to the &lt;a href=&quot;https://www.tensorflow.org/guide/data&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.data&lt;/code&gt;&lt;/a&gt; API related to shuffling, batching,
and so on. Those who are familiar with the API will probably immediately
recognize them. It is now instructive to go straight to the Python code.&lt;/p&gt;

&lt;p&gt;Below is an excerpt from a &lt;a href=&quot;https://github.com/chain-rule/example-weather-forecast/blob/master/forecast/data.py&quot;&gt;Python class&lt;/a&gt; responsible for building the
pipeline on the TensorFlow side:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# config = ...
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# List all files matching a given pattern
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pattern&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'examples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'part-*'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pattern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Shuffle the files if needed
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'shuffle_macro'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'shuffle_macro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Convert the files into datasets of examples stored as TFRecords and
# amalgamate these datasets into one dataset of examples
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;interleave&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TFRecordDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'interleave'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Shuffle the examples if needed
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'shuffle_micro'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'shuffle_micro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Preprocess the examples with respect to a given spec, pad the examples
# and form batches of different sizes, and postprocess the batches
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'map'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padded_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padded_shapes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'batch'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_postprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'map'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Prefetch the batches if needed
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'prefetch'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'prefetch'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Repeat the data once the source is exhausted if needed
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'repeat'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repeat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'repeat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The pipeline is self-explanatory. It is simply a chain of operations stacked on
top of each other. It is, however, worth taking a closer look at the
preprocessing and postprocessing mappings, which can be seen before and after
the padding step, respectively:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;proto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;spec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'transform'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; \
        &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformed_feature_spec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_single_example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;proto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contextual_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Convert the sequential columns from sparse to dense
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequential_names&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_postprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contextual&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sequential&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Convert the sequential columns from dense to sparse
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sparse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequential_names&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contextual&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Currently, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.data&lt;/code&gt; does not support padding sparse tensors, which is the
representation used for sequential features in TensorFlow. In the running
example about forecasting weather, such features are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;duration&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;temperature&lt;/code&gt;. This is the reason such features are converted to their dense
counterparts in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_preprocess&lt;/code&gt;. However, the final representation has to be
sparse still. Therefore, the sequential features are converted back to the
sparse format in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_postprocess&lt;/code&gt;. Hopefully, this back-and-forth conversion will
be rendered obsolete in future versions.&lt;/p&gt;

&lt;p&gt;Having executed the above steps, we have an instance of &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/data/Dataset&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt;,
which is the ultimate goal, as it is the standard way of ingesting data into a
TensorFlow graph. At this point, one might create a Keras model leveraging
&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/layers/DenseFeatures&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.layers.DenseFeatures&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/experimental/SequenceFeatures&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.experimental.SequenceFeatures&lt;/code&gt;&lt;/a&gt;
for constructing the input layer and then pass the data set to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fit&lt;/code&gt;
function of the model. A &lt;a href=&quot;https://github.com/chain-rule/example-weather-forecast/blob/master/forecast/model.py&quot;&gt;skeleton&lt;/a&gt; for this part can be found in the
repository.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this article, we have discussed a scalable approach to the ingestion of
sequential observations from BigQuery into a TensorFlow graph. The key tools
that have been used to this end are TensorFlow Extended in combination with
Cloud Dataflow and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.data&lt;/code&gt; API of TensorFlow.&lt;/p&gt;

&lt;p&gt;In addition, the provided code has been written to be general and easily
customizable. It has been achieved by separating the configuration part from the
implementation one.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Lak Lakshmanan, “&lt;a href=&quot;https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning&quot;&gt;Repeatable sampling of data sets in BigQuery for machine
learning&lt;/a&gt;,” 2016.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ivan Ukhov</name></author><summary type="html">How hard can it be to ingest sequential data into a TensorFlow model? As always, the answer is, “It depends.” Where are the sequences in question stored? Can they fit in main memory? Are they of the same length? In what follows, we shall build a flexible and scalable workflow for feeding sequential observations into a TensorFlow graph starting from BigQuery as the data warehouse.</summary></entry><entry><title type="html">Sample size determination using historical data and simulation</title><link href="https://blog.ivanukhov.com/2019/09/25/bootstrap.html" rel="alternate" type="text/html" title="Sample size determination using historical data and simulation" /><published>2019-09-25T06:00:00+00:00</published><updated>2019-09-25T06:00:00+00:00</updated><id>https://blog.ivanukhov.com/2019/09/25/bootstrap</id><content type="html" xml:base="https://blog.ivanukhov.com/2019/09/25/bootstrap.html">&lt;p&gt;In order to test a hypothesis, one has to design and execute an adequate
experiment. Typically, it is neither feasible nor desirable to involve the whole
population. Instead, a relatively small subset of the population is studied, and
given the outcome for this small sample, relevant conclusions are drawn with
respect to the population. An important question to answer is then, What is the
minimal sample size needed for the experiment to succeed? In what follows, we
answer this question using solely historical data and computer simulation,
without invoking any classical statistical procedures.&lt;/p&gt;

&lt;p&gt;Although, as we shall see, the ideas are straightforward, direct calculations
were impossible to perform before computers. To be able to answer this kind of
questions back then, statisticians developed mathematical theories in order to
approximate the calculations for specific situations. Since nothing else was
possible, these approximations and the various terms and conditions under which
they operate made up a large part of traditional textbooks and courses in
statistics. However, the advent of today’s computing power has enabled one to
estimate required sample sizes in a more direct and intuitive way, with the only
prerequisites being an understanding of statistical inference, the availability
of historical data describing the status quo, and the ability to write a few
lines of code in a programming language.&lt;/p&gt;

&lt;h1 id=&quot;problem&quot;&gt;Problem&lt;/h1&gt;

&lt;p&gt;For concreteness, consider the following scenario. We run an online business and
hypothesize that a specific change in promotion campaigns, such as making them
personalized, will have a positive effect on a specific performance metric, such
as the average deposit. In order to investigate if it is the case, we decide to
perform a two-sample test. There are the following two competing hypotheses.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The null hypothesis postulates that the change has no effect on the metric.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The alternative hypothesis postulates that the change has a positive effect on
the metric.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There will be two groups: a control group and a treatment group. The former will
be exposed to the current promotion policy, while the latter to the new one.
There are also certain requirements imposed on the test. First, we have a level
of statistical significance \(\alpha\) and a level of practical significance
\(\delta\) in mind. The former puts a limit on the false-positive rate, and the
latter indicates the smallest effect that we still care about; anything smaller
is as good as zero for any practical purpose. In addition, we require the test
to have a prescribed false-negative rate \(\beta\), ensuring that the test has
enough statistical power.&lt;/p&gt;

&lt;p&gt;For our purposes, the test is considered well designed if it is capable of
detecting a difference as small as \(\delta\) so that the false-positive and
false-negative rates are controlled to levels \(\alpha\) and \(\beta\),
respectively. Typically, parameters \(\alpha\) and \(\delta\) are held constant,
and the desired false-positive rate \(\beta\) is attained by varying the number
of participants in each group, which we denote by \(n\). Note that we do not
want any of the parameters to be smaller than the prescribed values, as it would
be wasteful.&lt;/p&gt;

&lt;p&gt;So what should the sample size be for the test to be well designed?&lt;/p&gt;

&lt;h1 id=&quot;solution&quot;&gt;Solution&lt;/h1&gt;

&lt;p&gt;Depending on the distribution of the data and on the chosen metric, one might or
might not be able to find a suitable test among the standard ones, while
ensuring that the test’s assumptions can safely be considered satisfied. More
importantly, a textbook solution might not be the most intuitive one, which, in
particular, might lead to misuse of the test. It is the understanding that
matters.&lt;/p&gt;

&lt;p&gt;Here we take a more pragmatic and rather general approach that circumvents the
above concerns. It requires only historical data and basic programming skills.
Despite its simplicity, the method below goes straight to the core of what the
famed statistical tests are doing behind all the math. The approach belongs to
the class of so-called bootstrap techniques and is as follows.&lt;/p&gt;

&lt;p&gt;Suppose we have historical data on customers’ behavior under the current
promotion policy, which is commonplace in practice. An important realization is
that this data set represents what we expect to observe in the control group. It
is also what is expected of the treatment group provided that the null
hypothesis is true, that is, when the proposed change has no effect. This
realization enables one to simulate what would happen if each group was limited
to an arbitrary number of participants. Then, by varying this size parameter, it
is possible to find the smallest value that makes the test well designed, that
is, make the test satisfy the requirements on \(\alpha\), \(\beta\), and
\(\delta\), as discussed in the previous section.&lt;/p&gt;

&lt;p&gt;This is all. The rest is an elaboration of the above idea.&lt;/p&gt;

&lt;p&gt;The simulation entails the following. To begin with, note that what we are
interested in testing is the difference between the performance metric applied
to the treatment group and the same metric applied to the control group, which
is referred to as the test statistic:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test statistic = Metric(Treatment sample) - Metric(Control sample).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Treatment sample&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Control sample&lt;/code&gt; stand for sets of observations, and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Metric(Sample)&lt;/code&gt; stands for computing the performance metric given such a
sample. For instance, each observation could be the total deposit of a customer,
and the metric could be the average value:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Metric(Sample) = Sum of observations / Number of observations.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note, however, that it is an example; the metric can be arbitrary, and this is a
huge advantage of this approach to sample size determination based on data and
simulation.&lt;/p&gt;

&lt;p&gt;Large positive values of the test statistic speak in favor of the treatment
(that is, the new promotion policy in our example), while those that are close
to zero suggest that the treatment is futile.&lt;/p&gt;

&lt;p&gt;A sample of \(n\) observations corresponding to the status quo (that is, the
current policy in our example) can be easily obtained by drawing \(n\) data
points with replacement from the historical data:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Sample = Choose random with replacement(Data, N).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This expression is used for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Control sample&lt;/code&gt; under both the null and alternative
hypotheses. As alluded to earlier, this is also how &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Treatment sample&lt;/code&gt; is
obtained under the null. Regarding the alternative hypothesis being true, one
has to express the hypothesized outcome as a distribution for the case of the
minimal detectable difference, \(\delta\). The simplest and reasonable solution
is to sample the data again, apply the metric, and then adjust the result to
reflect the alternative hypothesis:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Metric(Choose random with replacement(Data, N)) + Delta.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, again, one is free to change the logic under the alternative according to
the situation at hand. For instance, instead of an additive effect, one could
simulate a multiplicative one.&lt;/p&gt;

&lt;p&gt;The above is a way to simulate a single instance of the experiment under either
the null or alternative hypothesis; the result is a single value for the test
statistic. The next step is to estimate how the test statistic would vary if the
experiment was repeated many times in the two scenarios. This simply means that
the procedure should be repeated multiple times:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Repeat many times {
  Sample 1 = Choose random with replacement(Data, N)
  Sample 2 = Choose random with replacement(Data, N)
  Metric 1 = Metric(Sample 1)
  Metric 2 = Metric(Sample 2)
  Test statistic under null = Metric 1 - Metric 2

  Sample 3 = Choose random with replacement(Data, N)
  Sample 4 = Choose random with replacement(Data, N)
  Metric 3 = Metric(Sample 3) + Delta
  Metric 4 = Metric(Sample 4)
  Test statistic under alternative = Metric 3 - Metric 4
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This yields a collection of values for the test statistic under the null
hypothesis and a collection of values for the test statistic under the
alternative hypothesis. Each one contains realizations from the so-called
sampling distribution in the corresponding scenario. The following figure gives
an illustration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019-09-25-bootstrap/sampling-distribution-1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The blue shape is the sampling distribution under the null hypothesis, and the
red one is the sampling distribution under the alternative hypothesis. We shall
come back to this figure shortly.&lt;/p&gt;

&lt;p&gt;These two distributions of the test statistic are what we are after, as they
allow one to compute the false-positive rate and eventually choose a sample
size. First, given \(\alpha\), the sampling distribution under the null (the
blue one) is used in order to find a value beyond which the probability mass is
equal to \(\alpha\):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Critical value = Quantile([Test statistic under null], 1 - alpha).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Quantile&lt;/code&gt; computes the quantile specified by the second argument given a set of
observations. This quantity is called the critical value of the test. In the
figure above, it is denoted by a dashed line. When the test statistic falls to
the right of the critical value, we reject the null hypothesis; otherwise, we
fail to reject it. Second, the sampling distribution in the case of the
alternative hypothesis being true (the red one) is used in order to compute the
false-negative rate:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Attained beta = Mean([Test statistic under alternative &amp;lt; Critical value]).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It corresponds to the probability mass of the sampling distribution under the
alternative to the left of the critical value. In the figure, it is the red area
to the left of the dashed line.&lt;/p&gt;

&lt;p&gt;The final step is to put the above procedure in an optimization loop that
minimizes the distance between the target and attained \(\beta\)’s with respect
to the sample size:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Optimize N until Attained beta is close to Target beta {
  Repeat many times {
    Test statistic under null = ...
    Test statistic under alternative = ...
  }
  Critical value = ...
  Attained beta = ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This concludes the calculation of the size that the control and treatment groups
should have in order for the upcoming test in promotion campaigns to be well
designed in terms of the level of statistical significance \(\alpha\), the
false-negative rate \(\beta\), and the level of practical significance
\(\delta\).&lt;/p&gt;

&lt;p&gt;An example of how this technique could be implemented in practice can be found
in the appendix.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this article, we have discussed an approach to sample size determination that
is based on historical data and computer simulation rather than on mathematical
formulae tailored for specific situations. It is general and straightforward to
implement. More importantly, the technique is intuitive, since it directly
follows the narrative of null hypothesis significance testing. It does require
prior knowledge of the key concepts in statistical inference. However, this
knowledge is arguably essential for those who are involved in scientific
experimentation. It constitutes the core of statistical literacy.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h1&gt;

&lt;p&gt;This article was inspired by a blog post authored by &lt;a href=&quot;http://allendowney.blogspot.com/2011/05/there-is-only-one-test.html&quot;&gt;Allen Downey&lt;/a&gt; and a talk
given by &lt;a href=&quot;https://www.youtube.com/watch?v=5Dnw46eC-0o&quot;&gt;John Rauser&lt;/a&gt;. I also would like to thank &lt;a href=&quot;http://users.stat.umn.edu/~rend0020/&quot;&gt;Aaron Rendahl&lt;/a&gt; for his
feedback on the introduction to the method presented here and for his help with
the implementation given in the appendix.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Allen Downey, “&lt;a href=&quot;http://allendowney.blogspot.com/2011/05/there-is-only-one-test.html&quot;&gt;There is only one test!&lt;/a&gt;,” 2011.&lt;/li&gt;
  &lt;li&gt;John Rauser, “&lt;a href=&quot;https://www.youtube.com/watch?v=5Dnw46eC-0o&quot;&gt;Statistics without the agonizing pain&lt;/a&gt;,” 2014.&lt;/li&gt;
  &lt;li&gt;Joseph Lee Rodgers, “&lt;a href=&quot;https://doi.org/10.1207/S15327906MBR3404_2&quot;&gt;The bootstrap, the jackknife, and the randomization
test: A sampling taxonomy&lt;/a&gt;,” Multivariate Behavioral Research,
2010.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;appendix&quot;&gt;Appendix&lt;/h1&gt;

&lt;p&gt;The following listing shows an implementation of the bootstrap approach in R:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tidyverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set.seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Artificial data for illustration&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_count&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tibble&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rlnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Performance metric&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Statistical significance&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# False-negative rate&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Practical significance&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;simulate&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replication_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Function for drawing a single sample of size sample_size&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_one&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Function for drawing replication_count samples of size sample_size&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_many&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replication_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Simulation under the null hypothesis&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;control_null&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_many&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;treatment_null&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_many&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;difference_null&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;treatment_null&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;control_null&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Simulation under the alternative hypothesis&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;control_alternative&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_many&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;treatment_alternative&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_many&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;difference_alternative&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;treatment_alternative&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;control_alternative&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Computation of the critical value&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;critical_value&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;difference_null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Computation of the false-negative rate&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;difference_alternative&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;critical_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;difference_null&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;difference_null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;difference_alternative&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;difference_alternative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;critical_value&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;critical_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Number of replications&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replication_count&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Interval of possible values for the sample size&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search_interval&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Root finding to attain the desired value by varying the sample size&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;simulate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;as.integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replication_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_size&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;as.integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniroot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;interval&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search_interval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The illustrative figure shown in the solution section displays the sampling
distribution of the test statistic under the null and alternative for the sample
size found by this code snippet.&lt;/p&gt;</content><author><name>Ivan Ukhov</name></author><summary type="html">In order to test a hypothesis, one has to design and execute an adequate experiment. Typically, it is neither feasible nor desirable to involve the whole population. Instead, a relatively small subset of the population is studied, and given the outcome for this small sample, relevant conclusions are drawn with respect to the population. An important question to answer is then, What is the minimal sample size needed for the experiment to succeed? In what follows, we answer this question using solely historical data and computer simulation, without invoking any classical statistical procedures.</summary></entry><entry><title type="html">A Bayesian approach to the inference of the net promoter score</title><link href="https://blog.ivanukhov.com/2019/08/19/net-promoter.html" rel="alternate" type="text/html" title="A Bayesian approach to the inference of the net promoter score" /><published>2019-08-19T06:00:00+00:00</published><updated>2019-08-19T06:00:00+00:00</updated><id>https://blog.ivanukhov.com/2019/08/19/net-promoter</id><content type="html" xml:base="https://blog.ivanukhov.com/2019/08/19/net-promoter.html">&lt;p&gt;The net promoter score is a widely adopted metric for gauging customers’
satisfaction with a product. The popularity of the score is arguably attributed
to the simplicity of measurement and the intuitiveness of interpretation.
Moreover, it is claimed to be correlated with revenue growth, which, ignoring
causality, makes it even more appealing. In this article, we leverage Bayesian
statistics in order to infer the net promoter score for an arbitrary
segmentation of a customer base. The outcome of the inference is a distribution
over all possible values of the score weighted by probabilities, which provides
exhaustive information for the subsequent decision-making.&lt;/p&gt;

&lt;p&gt;A bare-bones net promoter survey is composed of only one question: “How likely
are you to recommend us to a friend?” The answer is an integer ranging from 0 to
10 inclusively. If the grade is between 0 and 6 inclusively, the person in
question is said to be a detractor. If it is 7 or 8, the person is said to be a
neutral. Lastly, if it is 9 or 10, the person is deemed a promoter. The net
promoter score itself is then the percentage of promoters minus the percentage
of detractors. The minimum and maximum attainable values of the score are −100
and 100, respectively. In this case, the greater, the better.&lt;/p&gt;

&lt;p&gt;As it is usually the case with surveys, a small but representative subset of
customers is reached out to, and the collected responses are then used to draw
conclusions about the target population of customers. Our objective is to
facilitate this last step by estimating the net promoter score given a set of
responses and necessarily quantify and put front and center the uncertainty in
our estimates.&lt;/p&gt;

&lt;p&gt;Before we proceed, since a net promoter survey is an observational study, which
is prone to such biases as participation and response biases, great care must be
taken when analyzing the results. In this article, however, we focus on the
inference of the net promoter score under the assumption that the given sample
of responses is representative of the target population.&lt;/p&gt;

&lt;h1 id=&quot;problem&quot;&gt;Problem&lt;/h1&gt;

&lt;p&gt;In practice, one is interested to know the net promoter scope for different
subpopulations of customers, such as countries of operation and age groups,
which is the scenario that we shall target. To this end, suppose that there are
\(m\) segments of interest, and each customer belongs to strictly one of them.
The results of a net promoter survey can then be summarized using the following
\(m \times 3\) matrix:&lt;/p&gt;

\[y = \left(
\begin{matrix}
d_1 &amp;amp; n_1 &amp;amp; p_1 \\
\vdots &amp;amp; \vdots &amp;amp; \vdots \\
d_i &amp;amp; n_i &amp;amp; p_i \\
\vdots &amp;amp; \vdots &amp;amp; \vdots \\
d_m &amp;amp; n_m &amp;amp; p_m
\end{matrix}
\right)\]

&lt;p&gt;where \(d_i\), \(n_i\), and \(p_i\) denote the number of detractors, neutrals,
and promoters in segment \(i\), respectively. For segment \(i\), the &lt;em&gt;observed&lt;/em&gt;
net promoter score can be computed as follows:&lt;/p&gt;

\[\hat{s}_i = 100 \times \frac{p_i - d_i}{d_i + n_i + p_i}.\]

&lt;p&gt;However, this observed score is a single scalar value calculated using \(d_i +
n_i + p_i\) data points, which is only a subset of the corresponding
subpopulation. It may or may not correspond well to the actual net promoter
score of that subpopulation. We have no reason to trust it, since the above
estimate alone does not tell us anything about the uncertainty associated with
it. Uncertainty quantification is essential for sound decision-making, which is
what we are after.&lt;/p&gt;

&lt;p&gt;Ideally, for each segment, given the observed data, we would like to have a
distribution of all possible values of the score with probabilities attached.
Such a probability distribution would be exhaustive information, from which any
other statistic could be easily derived. Here we tackle the problem by means of
Bayesian inference, which we discuss next.&lt;/p&gt;

&lt;h1 id=&quot;solution&quot;&gt;Solution&lt;/h1&gt;

&lt;p&gt;In order to perform Bayesian inference of the net promoter score, we need to
decide on an adequate Bayesian model for the problem at hand. Recall first that
we are interested in inferring scores for several segments. Even though there
might be segment-specific variations in the product, such as special offers in
certain countries, or in customers’ perception of the product, such as
age-related preferences, it is conceptually the same product that the customers
were asked to evaluate. It is then sensible to expect the scores in different
segments to have something in common. With this in mind, we construct a
hierarchical model with parameters shared by the segments.&lt;/p&gt;

&lt;p&gt;First, let&lt;/p&gt;

\[\theta_i = (\theta_{id}, \theta_{in}, \theta_{ip}) \in \langle 0, 1 \rangle^3\]

&lt;p&gt;be a triplet of parameters corresponding to the proportion of detractors,
neutrals, and promoters in segment \(i\), respectively, with the constraint that
they have to sum up to one. The constraint makes the triplet a simplex, which is
what is emphasized by the angle brackets on the right-hand side. These are the
main parameters we are interested in inferring. If the true value of
\(\theta_i\) was known, the net promoter score would be computed as follows:&lt;/p&gt;

\[\hat{s}_i = 100 \times (\theta_{ip} - \theta_{id}).\]

&lt;p&gt;Parameter \(\theta_i\) can also be thought of as a vector of probabilities of
observing one of the three types of customers in segment \(i\), that is,
detractors, neutrals, and promoters. Then the natural model for the observed
data is a multinomial distribution with \(d_i + n_i + p_i\) trials and
probabilities \(\theta_i\):&lt;/p&gt;

\[y_i | \theta_i \sim \text{Multinomial}(d_i + n_i + p_i, \theta_i)\]

&lt;p&gt;where \(y_i\) refers to the \(i\)th row of matrix \(y\) introduced earlier. The
family of multinomial distributions is a generalization of the family of
binomial distributions to more than two outcomes.&lt;/p&gt;

&lt;p&gt;The above gives a data distribution. In order to complete the modeling part, we
need to decide on a prior probability distribution for \(\theta_i\). Each
\(\theta_i\) is a simplex of probabilities. In such a case, a reasonable choice
is a Dirichlet distribution:&lt;/p&gt;

\[\theta_i | \phi \sim \text{Dirichlet}(\phi)\]

&lt;p&gt;where \(\phi = (\phi_d, \phi_n, \phi_p)\) is a vector of strictly positive
parameters. This family of distributions is a generalization of the family of
beta distributions to more than two categories. Note that \(\phi\) is the same
for all segments, which is what enables information sharing. In particular, it
means that the less reliable estimates for segments with fewer observations will
be shrunk toward the more reliable estimates for segments with more
observations. In other words, with this architecture, segments with fewer
observations are able to draw strength from those with more observations.&lt;/p&gt;

&lt;p&gt;How about \(\phi\)? This triplet is a characteristic of the product irrespective
of the segment. Its individual components can be utilized in order to encode
one’s prior knowledge about the net promoter score. Specifically, \(\phi_d\),
\(\phi_n\), and \(\phi_p\) could be set to imaginary observations of detractors,
neutrals, and promoters, respectively, reflecting one’s beliefs prior to
conducting the survey. The higher these imaginary counts are, the more certain
one claims to be about the true score. One could certainly set these
hyperparameters to fixed values; however, a more comprehensive solution is to
infer them from the data as well, giving the model more flexibility by making it
hierarchical. In addition, an inspection of \(\phi\) afterward can provide
insights into the overall satisfaction with the product.&lt;/p&gt;

&lt;p&gt;We now need to specify a prior, or rather a hyperprior, for \(\phi\). We proceed
under the assumption that we have little knowledge about the true score. Even if
there were surveys in the past, it is still a valid choice, especially when the
product evolves rapidly, rendering prior surveys marginally relevant.&lt;/p&gt;

&lt;p&gt;Now, it is more convenient to think in terms of expected values and variances
instead of imaginary counts, which is what \(\phi\) represents. Let us find an
alternative parameterization of the Dirichlet distribution. The expected value
of this distribution is as follows:&lt;/p&gt;

\[\mu = (\mu_d, \mu_n, \mu_p) = \frac{\phi}{\phi_d + \phi_n + \phi_p} \in \langle 0, 1 \rangle^3.\]

&lt;p&gt;It can be seen that it is a simplex of proportions of detractors, neutrals, and
promoters of the whole population, which is similar to \(\theta_i\) describing
segment \(i\). Regarding the variance,&lt;/p&gt;

\[\sigma^2 = \frac{1}{\phi_d + \phi_n + \phi_p}\]

&lt;p&gt;is considered to capture it sufficiently well. Solving the system of the last
two equations for \(\phi\) yields the following result:&lt;/p&gt;

\[\phi = \frac{\mu}{\sigma^2}.\]

&lt;p&gt;The prior for \(\theta_i\) can then be rewritten as follows:&lt;/p&gt;

\[\theta_i | \mu, \sigma \sim \text{Dirichlet}\left(\frac{\mu}{\sigma^2}\right).\]

&lt;p&gt;This new parameterization requires two hyperpriors: one is for \(\mu\), and one
is for \(\sigma\). For \(\mu\), a reasonable choice is a uniform distribution
(over a simplex), and for \(\sigma\), a half-Cauchy distribution:&lt;/p&gt;

\[\begin{align}
&amp;amp; \mu \sim \text{Uniform}(\langle 0, 1 \rangle^3) \text{ and} \\
&amp;amp; \sigma \sim \text{Half-Cauchy}(0, 1).
\end{align}\]

&lt;p&gt;The two distributions are relatively week, which is intended in order to let the
data speak for themselves. At this point, all parameters have been defined. Of
course, one could go further if the problem at hand had a deeper structure;
however, in this case, it is arguably not justifiable.&lt;/p&gt;

&lt;p&gt;The final model is as follows:&lt;/p&gt;

\[\begin{align}
y_i | \theta_i &amp;amp; \sim \text{Multinomial}(d_i + n_i + p_i, \theta_i), \\
\theta_i | \mu, \sigma &amp;amp; \sim \text{Dirichlet}(\mu / \sigma^2), \\
\mu &amp;amp; \sim \text{Uniform}(\langle 0, 1 \rangle^3), \text{ and} \\
\sigma &amp;amp; \sim \text{Half-Cauchy}(0, 1).
\end{align}\]

&lt;p&gt;The posterior distribution factorizes as follows:&lt;/p&gt;

\[p(\theta_1, \dots, \theta_m, \mu, \sigma | y) \propto
p(y | \theta_1, \dots, \theta_m) \,
p(\theta_1 | \mu, \sigma) \cdots
p(\theta_m | \mu, \sigma) \,
p(\mu) \,
p(\sigma),\]

&lt;p&gt;which relies on the usual assumption of independence given the parameters. One
could make a few simplifications by, for instance, leveraging the conjugacy of
the Dirichlet distribution with respect to the multinomial distribution;
however, it is not needed in practice, as we shall see shortly.&lt;/p&gt;

&lt;p&gt;The above posterior distribution is our ultimate goal. It is the one that gives
us a complete picture of what the true net promoter score in each segment might
be given the available evidence, that is, the responses from the survey. All
that is left is to draw a large enough sample from this distribution and start
to summarize and visualize the results.&lt;/p&gt;

&lt;p&gt;Unfortunately, as one might probably suspect, drawing samples from the posterior
is not an easy task. It does not correspond to any standard distribution and
hence does not have a readily available random number generator. Fortunately,
the topic is sufficiently mature, and there have been developed techniques for
sampling complex distributions, such as the family of Markov chain Monte Carlo
methods. Unfortunately, the most effective and efficient of these techniques are
notoriously complex themselves, and it might be extremely difficult and tedious
to implement and apply them correctly in practice. Fortunately, the need for
versatile tools for modeling and inference with the focus on the problem at hand
and not on implementation details has been recognized and addressed. Nontrivial
scenarios can be tackled with a surprisingly small amount of effort nowadays,
which we illustrate next.&lt;/p&gt;

&lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt;

&lt;p&gt;In this section, we implement the model using the probabilistic programming
language &lt;a href=&quot;https://mc-stan.org/&quot;&gt;Stan&lt;/a&gt;. Stan is straightforward to integrate into one’s workflow, as it
has interfaces for many general-purpose programming languages, including Python
and R. Here we only highlight the main points of the implementation and leave it
to the curious reader to discover Stan on their own.&lt;/p&gt;

&lt;p&gt;The following listing is a complete implementation of the model:&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// The number of segments&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// The number of categories, which is always three&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// The observed counts of detractors, neutrals, and promoters&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;simplex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;simplex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;transformed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cauchy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dirichlet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multinomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It can be seen that the code is very laconic and follows closely the development
given in the previous section, including the notation. It is worth noting that,
in the model block, we seemingly use unconstrained uniform and Cauchy
distributions; however, the constraints are enforced by the definitions of the
corresponding hyperparameters, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mu&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sigma&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This is practically all that is needed; the rest will be taken care of by Stan,
which is actually a lot of work, including an adequate initialization, an
efficient execution, and necessary diagnostics and quality checks. Under the
hood, the sampling of the posterior in Stan is based on the Hamiltonian Monte
Carlo algorithm and the no-U-turn sampler, which are considered to be the
state-of-the-art.&lt;/p&gt;

&lt;p&gt;The output of the sampling procedure is a set of draws from the posterior
distribution, which, again, is exhaustive information about the net promoter
score in the segments of interest. In particular, one can quantify the
uncertainty in and the probability of any statement one makes about the score.
For instance, if a concise summary is needed, one could compute the mean of the
score and accompany it with a high-posterior-density credible interval,
capturing the true value with the desired probability. However, if applicable,
the full distribution should be integrated into the decision-making process.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this article, we have constructed a hierarchical Bayesian model for inferring
the net promoter score for an arbitrary segmentation of a customer base. The
model features shared parameters, which enable information exchange between the
segments. This allows for a more robust estimation of the score, especially in
the case of segments with few observations. The final output of the inference is
a probability distribution over all possible values of the score in each
segment, which lays a solid foundation for the subsequent decision-making. We
have also seen how seamlessly the model can be implemented in practice using
modern tools for statistical inference, such as Stan.&lt;/p&gt;

&lt;p&gt;Lastly, note that the presented model is only one alternative; there are many
other. How would &lt;em&gt;you&lt;/em&gt; model the net promoter score? What changes would you
make? Make sure to leave a comment.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Andrew Gelman et al., &lt;em&gt;&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/book/&quot;&gt;Bayesian Data Analysis&lt;/a&gt;&lt;/em&gt;, Chapman and Hall/CRC,
2014.&lt;/li&gt;
  &lt;li&gt;Andrew Gelman, “&lt;a href=&quot;https://statmodeling.stat.columbia.edu/2009/10/21/some_practical/&quot;&gt;Some practical questions about prior distributions&lt;/a&gt;,”
2009.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ivan Ukhov</name></author><summary type="html">The net promoter score is a widely adopted metric for gauging customers’ satisfaction with a product. The popularity of the score is arguably attributed to the simplicity of measurement and the intuitiveness of interpretation. Moreover, it is claimed to be correlated with revenue growth, which, ignoring causality, makes it even more appealing. In this article, we leverage Bayesian statistics in order to infer the net promoter score for an arbitrary segmentation of a customer base. The outcome of the inference is a distribution over all possible values of the score weighted by probabilities, which provides exhaustive information for the subsequent decision-making.</summary></entry><entry><title type="html">Interactive notebooks in tightly sealed disposable containers</title><link href="https://blog.ivanukhov.com/2019/07/24/notebook.html" rel="alternate" type="text/html" title="Interactive notebooks in tightly sealed disposable containers" /><published>2019-07-24T06:00:00+00:00</published><updated>2019-07-24T06:00:00+00:00</updated><id>https://blog.ivanukhov.com/2019/07/24/notebook</id><content type="html" xml:base="https://blog.ivanukhov.com/2019/07/24/notebook.html">&lt;p&gt;It is truly amazing how interactive notebooks—where a narrative in a spoken
language is entwined with executable chunks of code in a programming
language—have revolutionized the way we work with data and document our thought
processes and findings for others and, equally importantly, for our future
selves. They are ubiquitous and taken for granted. It is hard to imagine where
data enthusiasts would be without them. Most likely, we would be spending too
much time staring at a terminal window, anxiously re-running scripts from start
to finish, printing variables, and saving lots of files with tables and graphs
on disk for further inspection. Interactive notebooks are an essential tool in
the data scientist’s toolbox, and in this article, we are going to make them
readily available for our use with our favorite packages installed and
preferences set up, no matter where we find ourselves working and regardless of
the mess we might have left behind during the previous session.&lt;/p&gt;

&lt;p&gt;Python and R (in alphabetic order) are arguably the primary languages used by
data scientists nowadays. In the context of interactive computations, &lt;a href=&quot;https://ipython.org/&quot;&gt;IPython&lt;/a&gt;
and later on &lt;a href=&quot;https://jupyter.org/&quot;&gt;Project Jupyter&lt;/a&gt; have been of paramount importance for the Python
community (the latter is actually language agnostic). In the R community, this
role has been played by &lt;a href=&quot;https://www.rstudio.com/&quot;&gt;RStudio&lt;/a&gt;. Therefore, having at one’s disposal
&lt;a href=&quot;https://jupyter.org/&quot;&gt;JupyterLab&lt;/a&gt;, which is Project Jupyter’s flagship, and RStudio should make one
well equipped for a wide range of data challenges. As alluded to earlier, the
objective is to have an environment that has a fixed initial state defined by us
and is accessible to us on any machine we might happen to work on. This problem
definition is a perfect fit for containerization. Specifically, we shall build
custom-tailored &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; images for JupyterLab and RStudio and create a few
convenient shortcuts for launching them.&lt;/p&gt;

&lt;p&gt;The code discussed below can be found in the following two repositories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chain-rule/JupyterLab/tree/article&quot;&gt;JupyterLab&lt;/a&gt; and&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chain-rule/RStudio/tree/article&quot;&gt;RStudio&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;jupyterlab&quot;&gt;JupyterLab&lt;/h1&gt;

&lt;p&gt;In order to build a Docker image for JupyterLab, we begin with a
&lt;a href=&quot;https://github.com/chain-rule/JupyterLab/blob/article/Dockerfile&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-docker highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Start with a minimal Python image&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; python:3.7-slim&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Install the desired Python packages&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; requirements.txt /tmp/requirements.txt&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--upgrade&lt;/span&gt; pip
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--upgrade&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--requirement&lt;/span&gt; /tmp/requirements.txt

&lt;span class=&quot;c&quot;&gt;# Configure JupyterLab to use a specific IP address and port&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; ~/.jupyter
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;c.NotebookApp.ip = '0.0.0.0'&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.jupyter/jupyter_notebook_config.py
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;c.NotebookApp.port = 8888&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.jupyter/jupyter_notebook_config.py

&lt;span class=&quot;c&quot;&gt;# Set the working directory&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /home/jupyterlab&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Stort JupyterLab once the container is launched&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENTRYPOINT&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; jupyter lab --allow-root --no-browser&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In words, we take a minimalistic image with the desired version of Python
preinstalled—in this case, it is the &lt;a href=&quot;https://hub.docker.com/_/python&quot;&gt;official Python image&lt;/a&gt;
tagged &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3.7-slim&lt;/code&gt;, which refers to Python 3.7 with any available bug fixes
promptly applied—and add packages that we consider to be important for our work.
These packages are gathered in the usual
&lt;a href=&quot;https://github.com/chain-rule/JupyterLab/blob/article/requirements.txt&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt;&lt;/a&gt;, which might look as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyterlab
matplotlib
numpy
pandas
pylint
pytest
scikit-learn
scipy
seaborn
tensorflow
yapf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first one, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jupyterlab&lt;/code&gt;, is essential; the rest is up to the data
scientist’s taste. An important aspect to note is that, in this example, the
versions of the listed packages are not fixed; hence, the latest available
versions will be taken each time a new image is built. Alternatively, one can
pin them to specific numbers by changing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt;. For instance, one
might write &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensorflow==1.14.0&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensorflow&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Having defined an image, we need a tool for orchestration. We would like to have
a convenient command for actually building the image and, more importantly, a
convenient command for launching a container with that image from an arbitrary
directory. The versatile &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make&lt;/code&gt; to the rescue!&lt;/p&gt;

&lt;div class=&quot;language-make highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# The name of the Docker image
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; jupyterlab
&lt;span class=&quot;c&quot;&gt;# The directory to be mounted to the container
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;root&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${PWD}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Build a new image
&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	docker rmi &lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;
	docker build &lt;span class=&quot;nt&quot;&gt;--tag&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt; .

&lt;span class=&quot;c&quot;&gt;# Start a new container
&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--interactive&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tty&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--publish&lt;/span&gt; 8888:8888 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;${root}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:/home/jupyterlab&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the above &lt;a href=&quot;https://github.com/chain-rule/JupyterLab/blob/article/Makefile&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Makefile&lt;/code&gt;&lt;/a&gt;, we define two commands: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build&lt;/code&gt;
and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build&lt;/code&gt; command instructs Docker to build a new image according
to the recipe in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt; command launches a new container and
mounts the directory specified by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; variable to the file system inside
the container using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--volume&lt;/code&gt; option. It also forwards port 8888 inside the
container, which is the one specified in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;, to port 8888 on the host
machine so that JupyterLab can be reached from the browser.&lt;/p&gt;

&lt;p&gt;Let us now go ahead and try the two commands:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make build
make start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;JupyterLab should come back with usage instructions similar to the following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;...
[I 18:40:15.078 LabApp] The Jupyter Notebook is running at:
[I 18:40:15.078 LabApp] http://e4edba021595:8888/?token=&amp;lt;token&amp;gt;
[I 18:40:15.078 LabApp]  or http://127.0.0.1:8888/?token=&amp;lt;token&amp;gt;
[I 18:40:15.078 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 18:40:15.082 LabApp]

    To access the notebook, open this file in a browser:
        file:///root/.local/share/jupyter/runtime/nbserver-6-open.html
    Or copy and paste one of these URLs:
        http://e4edba021595:8888/?token=&amp;lt;token&amp;gt;
     or http://127.0.0.1:8888/?token=&amp;lt;token&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By clicking on the last link, we end up in a fully fledged JupyterLab.
Congratulations! However, there is one step left. JupyterLab is currently
running in the folder with our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Makefile&lt;/code&gt;, which is not
particularly useful, as each project we might want to work on probably lives in
its own folder elsewhere in the file system. Fortunately, it is easy to fix with
an alias:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;jupyterlab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'make -C /path/to/the/folder/with/the/Makefile root=&quot;${PWD}&quot;'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command should be placed in the start-up script of the shell being
utilized. In the case of Bash, it can be done as follows:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;alias jupyterlab='make -C &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PWD&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; root=&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;\$&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;{PWD}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;'&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, in a new terminal, one should be able to run JupyterLab from any directory
as follows:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /path/to/some/project
jupyterlab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the content of the current working directory (that is,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/path/to/some/project&lt;/code&gt;) is readily available inside JupyterLab. All notebooks
created and modified in the GUI there will be stored directly in this folder,
and they will remain here when the container is shut down.&lt;/p&gt;

&lt;h1 id=&quot;rstudio&quot;&gt;RStudio&lt;/h1&gt;

&lt;p&gt;It is time to get to grips with an image for R notebooks. As before, we begin
with a &lt;a href=&quot;https://github.com/chain-rule/RStudio/blob/article/Dockerfile&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-docker highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Start with an RStudio image&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; rocker/rstudio:latest&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Install the software that R packages require&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apt-get update
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; libxml2-dev texlive texlive-latex-extra zlib1g-dev

&lt;span class=&quot;c&quot;&gt;# Set the working directory&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /home/rstudio&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Install the desired R packages&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; requirements.txt /tmp/requirements.txt&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;install.packages(readLines('/tmp/requirements.txt'), &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;                           repos = 'http://cran.us.r-project.org')&quot;&lt;/span&gt; | R
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Installing RStudio from scratch is not an easy task. Fortunately, we can start
with the &lt;a href=&quot;https://hub.docker.com/r/rocker/rstudio/&quot;&gt;official RStudio image&lt;/a&gt;, which is what is specified at
the top of the file. If desired, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;latest&lt;/code&gt; tag can be changed to a specific
version. The second block of Docker instructions is to provide programs and
libraries that are needed by the R packages that one is planning to install. For
instance, TeX Live is needed for rendering notebooks as PDF documents using
LaTeX. The last block of instructions in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; is for installing the R
packages themselves. As with Python, all necessary packages are gathered in a
single file called &lt;a href=&quot;https://github.com/chain-rule/RStudio/blob/article/requirements.txt&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;devtools
glmnet
plotly
rmarkdown
rstan
testthat
tidytext
tidyverse
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rmarkdown&lt;/code&gt; package is required for notebooks in Markdown. The rest is
intended to be changed according to one’s preferences; although, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tidyverse&lt;/code&gt; is
arguably a must in modern R.&lt;/p&gt;

&lt;p&gt;All right, in order to build the image and launch containers, we create the
following &lt;a href=&quot;https://github.com/chain-rule/RStudio/blob/article/Makefile&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Makefile&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-make highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# The name of the Docker image
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; rstudio
&lt;span class=&quot;c&quot;&gt;# The directory to be mounted to the container
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;root&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${PWD}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Build a new image
&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	docker rmi &lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;
	docker build &lt;span class=&quot;nt&quot;&gt;--tag&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt; .

&lt;span class=&quot;c&quot;&gt;# Start a new container
&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Address:  http://localhost:8787/&quot;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;User:     rstudio&quot;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Password: rstud10&quot;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'Press Control-C to terminate...'&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--interactive&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tty&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--publish&lt;/span&gt; 8787:8787 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;${root}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:/home/rstudio&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--env&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rstud10 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is similar to the one for JupyterLab; however, since the default prompt of
RStudio is not as informative as the one of JupyterLab, we print our own usage
instructions upon &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The final piece is the shortcut for launching RStudio:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;rstudio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'make -C /path/to/the/folder/with/the/Makefile root=&quot;${PWD}&quot;'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the case of Bash, it can be installed as follows:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;alias rstudio='make -C &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PWD&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; root=&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;\$&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;{PWD}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;'&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now it is time to build the image, go to an arbitrary directory, and test the
alias:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make build
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /path/to/some/project
rstudio
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Unlike the JupyterLab image, this one is much slower to build due to R packages
traditionally compiling a lot of C++ code upon installation.&lt;/p&gt;

&lt;p&gt;Lastly, it might be particularly convenient to have one’s GUI preferences (such
as the font size in the editor) and alike be automatically set up upon each
container launch. This can be achieved by realizing that RStudio stores user
preferences in a local folder called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.rstudio&lt;/code&gt;. Then the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt; command can be
adjusted to silently plant a preconfigured &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.rstudio&lt;/code&gt; into the current working
directory, which can be seen in the &lt;a href=&quot;https://github.com/chain-rule/RStudio/tree/article&quot;&gt;repository&lt;/a&gt; accompanying this
article.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Having completed the above steps, we have two Docker images: one is for Python
notebooks via JupyterLab, and one is for R notebooks via RStudio. At the moment,
the images are stored locally; however, they can be pushed to a public or
private image repository, such as &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Docker Hub&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/container-registry/&quot;&gt;Google Container Registry&lt;/a&gt;,
and subsequently pulled on an arbitrary machine having Docker installed.
Alternatively, they can be built on each machine separately. Regardless of the
installation, the crucial point is that our working environment will unshakably
remain in a specific pristine state defined by us.&lt;/p&gt;

&lt;p&gt;Lastly, it is worth noting that similar images can straightforwardly be built
for more specific scenarios. For instance, the following repository provides a
skeleton for building and using a custom &lt;a href=&quot;https://cloud.google.com/datalab/&quot;&gt;Datalab&lt;/a&gt;, which is Google’s wrapper
for Jupyter notebooks that run in the cloud: &lt;a href=&quot;https://github.com/chain-rule/Datalab&quot;&gt;Datalab&lt;/a&gt;.&lt;/p&gt;</content><author><name>Ivan Ukhov</name></author><summary type="html">It is truly amazing how interactive notebooks—where a narrative in a spoken language is entwined with executable chunks of code in a programming language—have revolutionized the way we work with data and document our thought processes and findings for others and, equally importantly, for our future selves. They are ubiquitous and taken for granted. It is hard to imagine where data enthusiasts would be without them. Most likely, we would be spending too much time staring at a terminal window, anxiously re-running scripts from start to finish, printing variables, and saving lots of files with tables and graphs on disk for further inspection. Interactive notebooks are an essential tool in the data scientist’s toolbox, and in this article, we are going to make them readily available for our use with our favorite packages installed and preferences set up, no matter where we find ourselves working and regardless of the mess we might have left behind during the previous session.</summary></entry><entry><title type="html">On the expected utility in conversion rate optimization</title><link href="https://blog.ivanukhov.com/2019/07/08/conversion.html" rel="alternate" type="text/html" title="On the expected utility in conversion rate optimization" /><published>2019-07-08T06:00:00+00:00</published><updated>2019-07-08T06:00:00+00:00</updated><id>https://blog.ivanukhov.com/2019/07/08/conversion</id><content type="html" xml:base="https://blog.ivanukhov.com/2019/07/08/conversion.html">&lt;p&gt;It can be not only extremely useful but also deeply satisfying to occasionally
dust off one’s math skills. In this article, we approach the classical problem
of conversion rate optimization—which is frequently faced by companies operating
online—and derive the expected utility of switching from variant A to variant B
under some modeling assumptions. This information can subsequently be utilized
in order to support the corresponding decision-making process.&lt;/p&gt;

&lt;p&gt;An R implementation of the math below and more can be found in the following
repository:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chain-rule/conversion-rate&quot;&gt;conversion-rate&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, it was written for personal exploratory purposes and has no
documentation at the moment. If you decide to dive in, you will be on your own.&lt;/p&gt;

&lt;h1 id=&quot;problem&quot;&gt;Problem&lt;/h1&gt;

&lt;p&gt;Suppose, as a business, you send communications to your customers in order to
increase their engagement with the product. Furthermore, suppose you suspect
that a certain change to the usual way of working might increase the uplift. In
order to test your hypothesis, you set up an A/B test. The only decision you
care about is whether or not you should switch from variant A to variant B where
variant A is the baseline (the usual way of working). The twist is that, from
the perspective of the business, variant B comes with its own gain if it is the
winner, and its own loss if it is the loser. The goal is to incorporate this
information in the final decision, making necessary assumptions along the way.&lt;/p&gt;

&lt;h1 id=&quot;solution&quot;&gt;Solution&lt;/h1&gt;

&lt;p&gt;Let \(A\) and \(B\) be two random variables modeling the conversion rates of the
two variants, variant A and variant B. Furthermore, let \(p\) be the probability
density function of the joint distribution of \(A\) and \(B\). In what follows,
concrete values assumed by the variables are denoted by \(a\) and \(b\),
respectively.&lt;/p&gt;

&lt;p&gt;Define the utility function as&lt;/p&gt;

\[U(a, b) = G(a, b) I(a &amp;lt; b) + L(a, b) I(a &amp;gt; b)\]

&lt;p&gt;where \(G\) and \(L\) are referred to as the gain and loss functions,
respectively. The gain function takes effect when variant B has a higher
conversion rate than the one of variant A, and the loss function takes effect
when variant A is better than variant B, which is what is enforced by the two
indicator functions (the equality is not essential). The expected utility is
then as follows:&lt;/p&gt;

\[\begin{align}
E(U(A, B))
&amp;amp;= \int_0^1 \int_0^1 U(a, b) p(a, b) \, db \, da \\
&amp;amp;=
\int_0^1 \int_a^1 G(a, b) p(a, b) \, db \, da +
\int_0^1 \int_0^a L(a, b) p(a, b) \, db \, da.
\end{align}\]

&lt;p&gt;We assume further the gain and loss are linear:&lt;/p&gt;

\[\begin{align}
&amp;amp; G(a, b) = w_g (b - a) \text{ and} \\
&amp;amp; L(a, b) = w_l (b - a).
\end{align}\]

&lt;p&gt;In the above, \(w_g\) and \(w_l\) are two non-negative scaling factors, which
can be used to encode business preferences. Then we have that&lt;/p&gt;

\[\begin{align}
E(U(A, B)) =
&amp;amp;
w_g \int_0^1 \int_a^1 b \, p(a, b) \, db \, da -
w_g \int_0^1 \int_a^1 a \, p(a, b) \, db \, da + {} \\
&amp;amp;
w_l \int_0^1 \int_0^a b \, p(a, b) \, db \, da -
w_l \int_0^1 \int_0^a a \, p(a, b) \, db \, da.
\end{align}\]

&lt;p&gt;For convenience, denote the four integrals by \(G_1\), \(G_2\), \(L_1\), and
\(L_2\), respectively, in which case we have that&lt;/p&gt;

\[E(U(A, B)) = w_g \, G_1 - w_g \, G_2 + w_l \, L_1 - w_l \, L_2.\]

&lt;p&gt;Now, suppose the distributions of \(A\) and \(B\) are estimated using Bayesian
inference. In this approach, the prior knowledge of the decision-maker about the
conversion rates of the two variants is combined with the evidence in the form
of data continuously streaming from the A/B test. It is natural to use a
binomial distribution for the data and a beta distribution for the prior
knowledge, which results in a posterior distribution that is also a beta
distribution due to conjugacy.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A posteriori&lt;/em&gt;, we have the following marginal distributions:&lt;/p&gt;

\[\begin{align}
&amp;amp; A \sim \text{Beta}(\alpha_a, \beta_a) \text{ and} \\
&amp;amp; B \sim \text{Beta}(\alpha_b, \beta_b)
\end{align}\]

&lt;p&gt;where \(\alpha_a\) and \(\beta_a\) the shape parameters of \(A\), and
\(\alpha_b\) and \(\beta_b\) of the shape parameters of \(B\). Assuming that the
two random variables are independent given the parameters,&lt;/p&gt;

\[p(a, b) =
p(a) \, p(b) =
\frac{a^{\alpha_a - 1} (1 - a)^{\beta_a - 1}}{B(\alpha_a, \beta_a)}
\frac{b^{\alpha_b - 1} (1 - b)^{\beta_b - 1}}{B(\alpha_b, \beta_b)}.\]

&lt;p&gt;We can now compute the expected utility. The first integral is as follows:&lt;/p&gt;

\[\begin{align}
G_1
&amp;amp;=
\int_0^1 \int_a^1
\frac{a^{\alpha_a - 1} (1 - a)^{\beta_a - 1}}{B(\alpha_a, \beta_a)}
\frac{b^{\alpha_b} (1 - b)^{\beta_b - 1}}{B(\alpha_b, \beta_b)} \, db \, da  \\
&amp;amp;=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
\int_0^1 \int_a^1
\frac{a^{\alpha_a - 1} (1 - a)^{\beta_a - 1}}{B(\alpha_a, \beta_a)}
\frac{b^{\alpha_b} (1 - b)^{\beta_b - 1}}{B(\alpha_b + 1, \beta_b)} \, db \, da \\
&amp;amp;=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
h(\alpha_a, \beta_a, \alpha_b + 1, \beta_b)
\end{align}\]

&lt;p&gt;where, which a slight abuse of notation, \(B\) is the beta function and&lt;/p&gt;

\[h(\alpha_1, \beta_1, \alpha_2, \beta_2) = P(X_1 &amp;lt; X_2)\]

&lt;p&gt;for any&lt;/p&gt;

\[\begin{align}
&amp;amp; X_1 \sim \text{Beta}(\alpha_1, \beta_1) \text{ and} \\
&amp;amp; X_2 \sim \text{Beta}(\alpha_2, \beta_2).
\end{align}\]

&lt;p&gt;The function \(h\) can be computed analytically, as shown in the blog posts
mentioned above. Specifically,&lt;/p&gt;

\[h(\alpha_1, \beta_1, \alpha_2, \beta_2) =
\sum_{i = 0}^{\alpha_2 - 1} \frac{B(\alpha_1 + i, \beta_1 + \beta_2)}{(\beta_2 + i) B(1 + i, \beta_2) B(\alpha_1, \beta_1)}.\]

&lt;p&gt;Similarly,&lt;/p&gt;

\[G_2 =
\frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
h(\alpha_a + 1, \beta_a, \alpha_b, \beta_b).\]

&lt;p&gt;Regarding the last two integrals in the expression of the utility function,&lt;/p&gt;

\[\begin{align}
L_1
&amp;amp;=
\int_0^1 \int_0^a
\frac{a^{\alpha_a - 1} (1 - a)^{\beta_a - 1}}{B(\alpha_a, \beta_a)}
\frac{b^{\alpha_b} (1 - b)^{\beta_b - 1}}{B(\alpha_b, \beta_b)} \, db \, da  \\
&amp;amp;=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
\int_0^1 \int_0^a
\frac{a^{\alpha_a - 1} (1 - a)^{\beta_a - 1}}{B(\alpha_a, \beta_a)}
\frac{b^{\alpha_b} (1 - b)^{\beta_b - 1}}{B(\alpha_b + 1, \beta_b)} \, db \, da \\
&amp;amp;=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
h(\alpha_b + 1, \beta_b, \alpha_a, \beta_a).
\end{align}\]

&lt;p&gt;Also,&lt;/p&gt;

\[L_2 =
\frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
h(\alpha_b, \beta_b, \alpha_a + 1, \beta_a).\]

&lt;p&gt;Assembling the integrals together, we obtain&lt;/p&gt;

\[\begin{align}
E(U(A, B)) =
&amp;amp; w_g \, \frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
h(\alpha_a, \beta_a, \alpha_b + 1, \beta_b) - {} \\
&amp;amp; w_g \, \frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
h(\alpha_a + 1, \beta_a, \alpha_b, \beta_b) + {} \\
&amp;amp; w_l \, \frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
h(\alpha_b + 1, \beta_b, \alpha_a, \beta_a) - {} \\
&amp;amp; w_l \, \frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
h(\alpha_b, \beta_b, \alpha_a + 1, \beta_a).
\end{align}\]

&lt;p&gt;At this point, we could call it a day, but there is some room for
simplification. Note that, in the case of the assumed linear model, we have the
following relationship between \(G\) and \(L\):&lt;/p&gt;

\[\begin{align}
G_1 - G_2
&amp;amp;=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
h(\alpha_a, \beta_a, \alpha_b + 1, \beta_b) -
\frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
h(\alpha_a + 1, \beta_a, \alpha_b, \beta_b) \\
&amp;amp;=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
(1 - h(\alpha_b + 1, \beta_b, \alpha_a, \beta_a)) -
\frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
(1 - h(\alpha_b, \beta_b, \alpha_a + 1, \beta_a)) \\
&amp;amp;=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)} -
\frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)} -
(L_1 - L_2) \\
&amp;amp;=
\Delta - (L_1 - L_2)
\end{align}\]

&lt;p&gt;where \(\Delta\) is the different between the above two ratios of beta
functions. Therefore,&lt;/p&gt;

\[\begin{align}
E(U(A, B))
&amp;amp;= w_g (G_1 - G_2) + w_l (L_1 - L_2) \\
&amp;amp;= w_g (G_1 - G_2) + w_l (\Delta - (G_1 - G_2)) \\
&amp;amp;= (w_g - w_l) (G_1 - G_2) + w_l \, \Delta.
\end{align}\]

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The decision-maker is now better equipped to take action. Having obtained the
posterior distributions of the conversion rates of the two variants, the derived
formula allows one to assess whether variant B is worth switching to,
considering its utility to the business at hand.&lt;/p&gt;

&lt;p&gt;The reason the expected utility \(E(U(A, B))\) can be evaluated in closed form
in this case is the linearity of the utility function \(U(a, b)\). More nuanced
preferences require a different approach. The most flexible candidate is
simulation, which is straightforward and should arguably be the go-to tool
regardless of the availability of a closed-form solution, as it is less
error-prone.&lt;/p&gt;

&lt;p&gt;Please feel free to reach out if you have any thoughts or suggestions.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h1&gt;

&lt;p&gt;This article is largely inspired by a series of excellent blog posts by &lt;a href=&quot;http://www.evanmiller.org/bayesian-ab-testing.html&quot;&gt;Evan
Miller&lt;/a&gt;, &lt;a href=&quot;https://www.chrisstucchio.com/blog/2014/bayesian_ab_decision_rule.html&quot;&gt;Chris Stucchio&lt;/a&gt;, and &lt;a href=&quot;http://varianceexplained.org/r/bayesian-ab-testing/&quot;&gt;David Robinson&lt;/a&gt;, which are strongly recommended.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Chris Stucchio, “&lt;a href=&quot;https://www.chrisstucchio.com/blog/2014/bayesian_ab_decision_rule.html&quot;&gt;Easy evaluation of decision rules in Bayesian A/B
testing&lt;/a&gt;,” 2014.&lt;/li&gt;
  &lt;li&gt;David Robinson, “&lt;a href=&quot;http://varianceexplained.org/r/bayesian-ab-testing/&quot;&gt;Is Bayesian A/B testing immune to peeking? Not
exactly&lt;/a&gt;,” 2015.&lt;/li&gt;
  &lt;li&gt;Evan Miller, “&lt;a href=&quot;http://www.evanmiller.org/bayesian-ab-testing.html&quot;&gt;Formulas for Bayesian A/B testing&lt;/a&gt;,” 2014.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ivan Ukhov</name></author><summary type="html">It can be not only extremely useful but also deeply satisfying to occasionally dust off one’s math skills. In this article, we approach the classical problem of conversion rate optimization—which is frequently faced by companies operating online—and derive the expected utility of switching from variant A to variant B under some modeling assumptions. This information can subsequently be utilized in order to support the corresponding decision-making process.</summary></entry><entry><title type="html">A poor man’s orchestration of predictive models, or do it yourself</title><link href="https://blog.ivanukhov.com/2019/07/01/orchestration.html" rel="alternate" type="text/html" title="A poor man’s orchestration of predictive models, or do it yourself" /><published>2019-07-01T06:00:00+00:00</published><updated>2019-07-01T06:00:00+00:00</updated><id>https://blog.ivanukhov.com/2019/07/01/orchestration</id><content type="html" xml:base="https://blog.ivanukhov.com/2019/07/01/orchestration.html">&lt;p&gt;As a data scientist focusing on developing data products, you naturally want
your work to reach its target audience. Suppose, however, that your company does
not have a dedicated engineering team for productizing data-science code. One
solution is to seek help in other teams, which are surely busy with their own
endeavors, and spend months waiting. Alternatively, you could take the
initiative and do it yourself. In this article, we take the initiative and
schedule the training and application phases of a predictive model using Apache
&lt;a href=&quot;https://airflow.apache.org/&quot;&gt;Airflow&lt;/a&gt;, Google &lt;a href=&quot;https://cloud.google.com/compute/&quot;&gt;Compute Engine&lt;/a&gt;, and &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let us first set expectations for what is assumed to be given and what will be
attained by the end of this article. It is assumed that a predictive model for
supporting business decisions—such as a model for identifying potential churners
or a model for estimating the lifetime value of customers—has already been
developed. This means that a business problem has already been identified and
translated into a concrete question, the data needed for answering the question
have already been collected and transformed into a target variable and a set of
explanatory variables, and a modeling technique has already been selected and
calibrated in order to answer the question by predicting the target variable
given the explanatory variables. For the sake of concreteness, the model is
assumed to be written in Python. We also assume that the company at hand has
chosen Google Cloud Platform as its primary platform, which makes a certain
suite of tools readily available.&lt;/p&gt;

&lt;p&gt;Our goal is then to schedule the model to run in the cloud via Airflow, Compute
Engine, and Docker so that it is periodically retrained (in order to take into
account potential fluctuations in the data distribution) and periodically
applied (in order to actually make predictions), delivering predictions to the
data warehouse in the form of &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt; for further consumption by other
parties.&lt;/p&gt;

&lt;p&gt;It is important to note that this article is not a tutorial on any of the
aforementioned technologies. The reader is assumed to be familiar with Google
Cloud Platform and to have an understanding of Airflow and Docker, as well as to
be comfortable with finding out missing details on their own.&lt;/p&gt;

&lt;p&gt;Lastly, the following two repositories contain the code discussed below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chain-rule/example-prediction&quot;&gt;example-prediction&lt;/a&gt; and&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chain-rule/example-prediction-service&quot;&gt;example-prediction-service&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;preparing-the-model&quot;&gt;Preparing the model&lt;/h1&gt;

&lt;p&gt;The suggested structure of the repository hosting the model is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── configs/
│   ├── application.json
│   └── training.json
├── prediction/
│   ├── __init__.py
│   ├── main.py
│   ├── model.py
│   └── task.py
├── README.md
└── requirements.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here &lt;a href=&quot;https://github.com/chain-rule/example-prediction/tree/master/prediction&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prediction/&lt;/code&gt;&lt;/a&gt; is a Python package, and it is likely to contain many more
files than the ones listed. The &lt;a href=&quot;https://github.com/chain-rule/example-prediction/blob/master/prediction/main.py&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt;&lt;/a&gt; file is the entry point for
command-line invocation, the &lt;a href=&quot;https://github.com/chain-rule/example-prediction/blob/master/prediction/task.py&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task&lt;/code&gt;&lt;/a&gt; module defines the actions that the
package is capable of performing, and the &lt;a href=&quot;https://github.com/chain-rule/example-prediction/blob/master/prediction/model.py&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt;&lt;/a&gt; module defines the model.&lt;/p&gt;

&lt;p&gt;As alluded to above, the primary job of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt; file is to parse command-line
arguments, read a configuration file, potentially set up logging and alike, and
delegate the rest to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task&lt;/code&gt; module. At a later stage, an invocation of an
action might look as follows:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; prediction.main &lt;span class=&quot;nt&quot;&gt;--action&lt;/span&gt; training &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt; configs/training.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we are passing two arguments: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--action&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--config&lt;/code&gt;. The former is to
specify the desired action, and the latter is to supply additional configuration
parameters, such as the location of the training data and the values of the
model’s hyperparameters. Keeping all parameters in a separate file, as opposed
to hard-coding them, makes the code reusable, and passing them all at once as a
single file scales much better than passing each parameter as a separate
argument.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task&lt;/code&gt; module is conceptually as follows (see the repository for the exact
implementation):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Read the training data
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Train the model
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Save the trained model
&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;application&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Read the application data
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Load the trained model
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Make predictions
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Save the predictions
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this example, there are two tasks: training and application. The training
task is responsible for fetching the training data, training the model, and
saving the result in a predefined location for future usage by the application
task. The application task is responsible for fetching the application data
(that is, the data the model is supposed to be applied to), loading the trained
model produced by the training task, making predictions, and saving them in a
predefined location to be picked up for the subsequent delivery to the data
warehouse.&lt;/p&gt;

&lt;p&gt;Likewise, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; module can be simplified as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Estimate the model’s parameters
&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Make predictions using the estimated parameters
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It can be seen that the structure presented above makes very few assumptions
about the model, which makes it generally applicable. It can also be easily
extended to accommodate other actions. For instance, one could have a separate
action for testing the model on unseen data.&lt;/p&gt;

&lt;p&gt;Having structured the model as shown above, it can now be productized, which we
discuss next.&lt;/p&gt;

&lt;h1 id=&quot;wrapping-the-model-into-a-service&quot;&gt;Wrapping the model into a service&lt;/h1&gt;

&lt;p&gt;Now it is time to turn the model into a service. In the scope of this article, a
service is a self-sufficient piece of code that can be executed in the cloud
upon request. To this end, another repository is created, adhering to the
separation-of-concerns design principle. Specifically, by doing so, we avoid
mixing the modeling code with the code specific to a particular environment
where the model happens to be deployed. The suggested structure of the
repository is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── container/
│   ├── Dockerfile
│   ├── run.sh
│   └── wait.sh
├── service/
│   ├── configs/
│   │   ├── application.json
│   │   └── training.json
│   ├── source/                # the first repository as a submodule
│   └── requirements.txt
├── scheduler/
│   ├── configs/
│   │   ├── application.json
│   │   └── training.json
│   ├── application.py         # a symbolic link to graph.py
│   ├── graph.py
│   └── training.py            # a symbolic link to graph.py
├── Makefile
└── README.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/container&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;container/&lt;/code&gt;&lt;/a&gt; folder contains files for building a Docker image for the
service. The &lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/service&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;service/&lt;/code&gt;&lt;/a&gt; folder is the service itself, meaning that these files
will be present in the container and eventually executed. Lastly, the
&lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/scheduler&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduler/&lt;/code&gt;&lt;/a&gt; folder contains files for scheduling the service using Airflow.
The last one will be covered in the next section; here we focus on the first
two.&lt;/p&gt;

&lt;p&gt;Let us start with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;service/&lt;/code&gt;. The first repository (the one discussed in the
previous section) is added to this second repository as a Git submodule living
in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;service/source/&lt;/code&gt;. This means that the model will essentially be embedded in
the service but will conveniently remain an independent entity. At all times,
the service contains a reference to a particular state (a particular commit,
potentially on a dedicated release branch) of the model, guaranteeing that the
desired version of the model is in production. However, when invoking the model
from the service, we might want to use a different set of configuration files
than the ones present in the first repository. To this end, a service-specific
version of the configuration files is created in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;service/configs/&lt;/code&gt;. We might
also want to install additional Python dependencies; hence, there is a separate
file with requirements.&lt;/p&gt;

&lt;p&gt;Now it is time to containerize the service code by building a Docker image. The
relevant files are gathered in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;container/&lt;/code&gt;. The image is defined in
&lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/container/Dockerfile&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;container/Dockerfile&lt;/code&gt;&lt;/a&gt; and is as follows:&lt;/p&gt;

&lt;div class=&quot;language-docker highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Use a minimal Python image&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; python:3.7-slim&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Install Google Cloud SDK as described in&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Copy the service directory to the image&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; service /service&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Copy the run script to the image&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; container/run.sh /run.sh&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Install Python dependencies specific to the predictive model&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--upgrade&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--requirement&lt;/span&gt; /service/source/requirements.txt
&lt;span class=&quot;c&quot;&gt;# Install Python dependencies specific to the service&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--upgrade&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--requirement&lt;/span&gt; /service/requirements.txt

&lt;span class=&quot;c&quot;&gt;# Set the working directory to be the service directory&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /service&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Set the entry point to be the run script&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENTRYPOINT&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /run.sh&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As mentioned earlier, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;service/&lt;/code&gt; gets copied as is (including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;service/source&lt;/code&gt;
with the model), and it will be the working directory inside the container. We
also copy &lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/container/run.sh&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;container/run.sh&lt;/code&gt;&lt;/a&gt;, which becomes the entry point of the container;
this script is executed whenever a container is launched. Let us take a look at
the content of the script (as before, some parts omitted for clarity):&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;process_training&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Invoke training&lt;/span&gt;
  python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; prediction.main &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--action&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt; configs/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;.json
  &lt;span class=&quot;c&quot;&gt;# Set the output location in Cloud Storage&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;local &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;gs://&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;NAME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;timestamp&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Copy the trained model from the output directory to Cloud Storage&lt;/span&gt;
  save output &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;process_application&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Find the latest trained model in Cloud Storage&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Copy the trained model from Cloud Storage to the output directory&lt;/span&gt;
  load &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; output
  &lt;span class=&quot;c&quot;&gt;# Invoke application&lt;/span&gt;
  python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; prediction.main &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--action&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt; configs/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;.json
  &lt;span class=&quot;c&quot;&gt;# Set the output location in Cloud Storage&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;local &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;gs://&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;NAME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;timestamp&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Copy the predictions from the output directory to Cloud Storage&lt;/span&gt;
  save output &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Set the input file in Cloud Storage&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Set the output data set and table in BigQuery&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Ingest the predictions from Cloud Storage into BigQuery&lt;/span&gt;
  ingest &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; player_id:STRING,label:BOOL
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;delete&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Delete a Compute Engine instance called &quot;${NAME}-${VERSION}-${ACTION}&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;ingest&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Ingest a file from Cloud Storage into a table in BigQuery&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;load&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Sync the content of a location in Cloud Storage with a local directory&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;save&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Sync the content of a local directory with a location in Cloud Storage&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;send&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Write into a Stackdriver log called &quot;${NAME}-${VERSION}-${ACTION}&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Invoke the delete function when the script exits regardless of the reason&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;trap &lt;/span&gt;delete EXIT

&lt;span class=&quot;c&quot;&gt;# Report a successful start to Stackdriver&lt;/span&gt;
send &lt;span class=&quot;s1&quot;&gt;'Running the action...'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Invoke the function specified by the ACTION environment variable&lt;/span&gt;
process_&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Report a successful completion to Stackdriver&lt;/span&gt;
send &lt;span class=&quot;s1&quot;&gt;'Well done.'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The script expects a number of environment variables to be set upon each
container launch, which will be discussed shortly. The primary ones are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NAME&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VERSION&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ACTION&lt;/code&gt;, indicating the name of the service, version of the
service, and action to be executed by the service, respectively.&lt;/p&gt;

&lt;p&gt;As we shall see below, the above script interacts with several different
products on Google Cloud Platform. It might then be surprising that there is
only a handful of variables passed to the script. The explanation is that the
convention-over-configuration design paradigm is followed to a great extent
here, meaning that other necessary variables can be derived (save sensible
default values) from the ones given, since there are certain naming conventions
used throughout the project.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;process_training&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;process_application&lt;/code&gt; are responsible for training
and application, respectively. It can be seen that they leverage the
command-line interface by invoking the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt; file, which was discussed in the
previous section. Since containers are stateless, all artifacts are stored in an
external storage, which is a bucket in &lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;Cloud Storage&lt;/a&gt; in our case, and this job
is delegated to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;save&lt;/code&gt; functions used in both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;process_training&lt;/code&gt;
and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;process_application&lt;/code&gt;. In addition, the result of the application action
(that is, the predictions) is ingested into a table in BigQuery using &lt;a href=&quot;https://cloud.google.com/sdk/&quot;&gt;Cloud
SDK&lt;/a&gt;, which can be seen in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ingest&lt;/code&gt; function in &lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/container/run.sh&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;container/run.sh&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The container communicates with the outside world using &lt;a href=&quot;https://cloud.google.com/stackdriver/&quot;&gt;Stackdriver&lt;/a&gt; via the
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;send&lt;/code&gt; function, which writes messages to a log dedicated to the current service
run. The most important message is the one indicating a successful completion,
which is sent at the very end; we use “Well done” for this purpose. This is the
message that will be looked for in order to determine the overall outcome of a
service run.&lt;/p&gt;

&lt;p&gt;Note also that, upon successful or unsuccessful completion, the container
deletes its hosting virtual machine, which is achieved by setting a handler
(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delete&lt;/code&gt;) for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EXIT&lt;/code&gt; event.&lt;/p&gt;

&lt;p&gt;Lastly, let us discuss the commands used for building the image and launching
the actions. This entails a few lengthy invocations of Cloud SDK, which can be
neatly organized in a &lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/Makefile&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Makefile&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-make highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# The name of the service
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?=&lt;/span&gt; example-prediction-service
&lt;span class=&quot;c&quot;&gt;# The version of the service
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;version&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?=&lt;/span&gt; 2019-00-00

&lt;span class=&quot;c&quot;&gt;# The name of the project on Google Cloud Platform
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;project&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?=&lt;/span&gt; example-cloud-project
&lt;span class=&quot;c&quot;&gt;# The zone for operations in Compute Engine
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;zone&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?=&lt;/span&gt; europe-west1-b
&lt;span class=&quot;c&quot;&gt;# The address of Container Registry
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;registry&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?=&lt;/span&gt; eu.gcr.io

&lt;span class=&quot;c&quot;&gt;# The name of the Docker image
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# The name of the instance excluding the action
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;instance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt;-&lt;span class=&quot;nv&quot;&gt;${version}&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	docker rmi &lt;span class=&quot;nv&quot;&gt;${image}&lt;/span&gt; 2&amp;gt; /dev/null &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;
	docker build &lt;span class=&quot;nt&quot;&gt;--file&lt;/span&gt; container/Dockerfile &lt;span class=&quot;nt&quot;&gt;--tag&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${image}&lt;/span&gt; .
	docker tag &lt;span class=&quot;nv&quot;&gt;${image}&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${registry}&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;${project}&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;${image}&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;${version}&lt;/span&gt;
	docker push &lt;span class=&quot;nv&quot;&gt;${registry}&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;${project}&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;${image}&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;${version}&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;training-start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	gcloud compute instances create-with-container &lt;span class=&quot;nv&quot;&gt;${instance}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-training&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--container-image&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${registry}&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;${project}&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;${image}&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;${version}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--container-env&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;NAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;${name}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--container-env&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;${version}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--container-env&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;training &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--container-env&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ZONE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;${zone}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--container-restart-policy&lt;/span&gt; never &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--no-restart-on-failure&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--machine-type&lt;/span&gt; n1-standard-1 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
		&lt;span class=&quot;nt&quot;&gt;--scopes&lt;/span&gt; default,bigquery,compute-rw,storage-rw
		&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-zone&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${zone}&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;training-wait&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	container/wait.sh instance &lt;span class=&quot;nv&quot;&gt;${instance}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-training&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;${zone}&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;training-check&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	container/wait.sh success &lt;span class=&quot;nv&quot;&gt;${instance}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-training&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Similar for application
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we define one command for building images, namely &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build&lt;/code&gt;, and three
commands per action, namely &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check&lt;/code&gt;. In this section, we
discuss &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt; and leave the last two for the next section, as they
are needed specifically for scheduling.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build&lt;/code&gt; command is invoked as follows:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make build
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It has to be used each time a new version of the service is to be deployed. The
command creates a local Docker image according to the recipe in
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;container/Dockerfile&lt;/code&gt; and uploads it to &lt;a href=&quot;https://cloud.google.com/container-registry/&quot;&gt;Container Registry&lt;/a&gt;, which is Google’s
storage for Docker images. For the last operation to succeed, your local Docker
has to be configured appropriately, which boils down to the following lines:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcloud auth login &lt;span class=&quot;c&quot;&gt;# General authentication for Cloud SDK&lt;/span&gt;
gcloud auth configure-docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build&lt;/code&gt; has finished successfully, one should be able to see the newly
created image in &lt;a href=&quot;https://console.cloud.google.com&quot;&gt;Cloud Console&lt;/a&gt; by navigating to Container Registry in the menu
to the left. All future versions of the service will be neatly grouped in a
separate folder in the registry.&lt;/p&gt;

&lt;p&gt;Given that the image is in the cloud, we can start to create virtual machines
running containers with this particular image, which is what the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt; command
is for:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make training-start &lt;span class=&quot;c&quot;&gt;# Similar for application&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Internally, it relies on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcloud compute instances create-with-container&lt;/code&gt;, which
can be seen in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Makefile&lt;/code&gt; listed above. There are a few aspects to note about
this command. Apart from selecting the right image and version
(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--container-image&lt;/code&gt;), one has to make sure to set the environment variables
mentioned earlier, as they control what the container will be doing once
launched. This is achieved by passing a number of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--container-env&lt;/code&gt; options to
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create-with-container&lt;/code&gt;. Here one can also easily scale up and down the host
virtual machine via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--machine-type&lt;/code&gt; option. Lastly, it is important to set
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--scopes&lt;/code&gt; option correctly in order to empower the container to work with
BigQuery, Compute Engine, and Cloud Storage.&lt;/p&gt;

&lt;p&gt;At this point, we have a few handy commands for working with the service. It is
time for scheduling.&lt;/p&gt;

&lt;h1 id=&quot;scheduling-the-service&quot;&gt;Scheduling the service&lt;/h1&gt;

&lt;p&gt;The goal now is to make both training and application be executed periodically,
promptly delivering predictions to the data warehouse. Technically, one could
just keep invoking &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make training-start&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make application-start&lt;/code&gt; on their
local machine, but of course, this is neither convenient nor reliable. Instead,
we would like to have an autonomous scheduler running in the cloud that would,
apart from its primary task of dispatching jobs, manage temporal dependencies
between jobs, keep record of all past and upcoming jobs, and preferably provide
a web-based dashboard for monitoring. One such tool is Airflow, and it is the
one used in this article.&lt;/p&gt;

&lt;p&gt;In Airflow, the work to be performed is expressed as a directed acyclic graph
defined using Python. Our job is to create two such graphs. One is for training,
and one is for application, each with its own periodicity. At this point, it
might seem that each graph should contain only one node calling the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt;
command, which was introduced earlier. However, a more comprehensive solution is
to not only start the service but also wait for its termination and check that
it successfully executed the corresponding logic. It will give us great
visibility on the life cycle of the service in terms of various statistics (for
instance, the duration and outcome of all runs) directly in Airflow.&lt;/p&gt;

&lt;p&gt;The above is the reason we have defined two additional commands in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Makefile&lt;/code&gt;:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt; command ensures that the virtual machine reached
a terminal state (regardless of the outcome), and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check&lt;/code&gt; command ensures
that the terminal state was the one expected. This functionality can be
implemented in different ways. The approach that we use can be seen in
&lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/container/wait.sh&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;container/wait.sh&lt;/code&gt;&lt;/a&gt;, which is invoked by both operations from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Makefile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;process_instance&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'Waiting for the instance to finish...'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;while &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Try to read some information about the instance&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Exit successfully when there is no such instance&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;wait
  &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;process_success&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'Waiting for the success to be reported...'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;while &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Check if the last entry in Stackdriver contains “Well done”&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Exit successfully if the phrase is present&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;wait
  &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'Waiting...'&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;sleep &lt;/span&gt;10
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Invoke the function specified by the first command-line argument and forward&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# the rest of the arguments to this function&lt;/span&gt;
process_&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;:2:10&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The script has two main functions. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;process_instance&lt;/code&gt; function waits for the
virtual machine to finish, and it is currently based on trying to fetch some
information about the machine in question using Cloud SDK. Whenever this
fetching fails, it is an indication of the machine being shut down and
destroyed, which is exactly what is needed in this case. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;process_success&lt;/code&gt;
function waits for the key phrase “Well done” to appear in Stackdriver. However,
this message might never appear, and this is the reason &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;process_success&lt;/code&gt; has a
timeout, unlike &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;process_instance&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;All right, there are now three commands to schedule in sequence: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check&lt;/code&gt;. For instance, for training, the exact command sequence is
the following:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make training-start
make training-wait
make training-check
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We need to create two separate Python files defining two separate Airflow
graphs; however, the graphs will be almost identical except for the triggering
interval and the prefix of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check&lt;/code&gt; commands. It then
makes sense to keep the varying parts in separate configuration files and use
the exact same code for constructing the graphs, adhering to the
do-not-repeat-yourself design principle. The &lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/scheduler/configs&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduler/configs/&lt;/code&gt;&lt;/a&gt; folder
contains the configuration files suggested, and &lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/scheduler/graph.py&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduler/graph.py&lt;/code&gt;&lt;/a&gt; is the
Python script creating a graph:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DAG&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow.operators.bash_operator&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BashOperator&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;configure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Extract the directory containing the current file
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dirname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__file__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Extract the name of the current file without its extension
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splitext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;basename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__file__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Load the configuration file corresponding to the extracted name
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'configs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;construct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_construct_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;start_date&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strptime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'%Y-%m-%d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DAG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_date&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_construct_task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BashOperator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bash_command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dag&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Construct an empty graph
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_construct_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'graph'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Construct the specified tasks
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_construct_task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tasks'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Enforce the specified dependencies between the tasks
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;child&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dependencies'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_downstream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;child&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Load an appropriate configuration file and construct a graph accordingly
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;construct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;configure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;FileNotFoundError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Exit without errors in case the current file has no configuration file
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The script receives no arguments and instead tries to find a suitable
configuration file based on its own name, which can be seen in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configure&lt;/code&gt;
function. Then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduler/training.py&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduler/application.py&lt;/code&gt; can simply
be symbolic links to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduler/graph.py&lt;/code&gt;, avoiding any code repetition. When
they are read by Airflow, each one will have its own name, and it will load its
own configuration if there is one in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduler/configs/&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For instance, for training, &lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/tree/master/scheduler/configs/training.json&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduler/configs/training.json&lt;/code&gt;&lt;/a&gt; is as follows:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;graph&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;dag_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;example-prediction-service-training&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;schedule_interval&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0 0 1 * *&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;start_date&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2019-07-01&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;tasks&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;start&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;code&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;make -C '${ROOT}/..' training-start&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;wait&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;code&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;make -C '${ROOT}/..' training-wait&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;check&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;code&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;make -C '${ROOT}/..' training-check&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;dependencies&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;wait&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;start&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;check&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;wait&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each configuration file contains three main sections: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tasks&lt;/code&gt;, and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dependencies&lt;/code&gt;. The first section prescribes the desired start date,
periodicity, and other parameters specific to the graph itself. In this example,
the graph is triggered on the first day of every month at midnight (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0 0 1 *
*&lt;/code&gt;), which might be a reasonable frequency for retraining the model. The second
section defines what commands should be executed. It can be seen that there is
one task for each of the three actions. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-C '${ROOT}/..'&lt;/code&gt; part is needed in
order to ensure that the right &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Makefile&lt;/code&gt; is used, which is taken care of in
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scheduler/graph.py&lt;/code&gt;. Lastly, the third section dictates the order of execution
by enforcing dependencies. In this case, we are saying that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt; depends on
(should be executed after) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt;, and that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check&lt;/code&gt; depends on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt;, forming
a chain of tasks.&lt;/p&gt;

&lt;p&gt;At this point, the graphs are considered to be complete. In order to make
Airflow aware of them, the repository can be simply cloned into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dags&lt;/code&gt;
directory of Airflow.&lt;/p&gt;

&lt;p&gt;Lastly, Airflow itself can live on a separate instance in Compute Engine.
Alternatively, &lt;a href=&quot;https://cloud.google.com/composer/&quot;&gt;Cloud Composer&lt;/a&gt; provided by Google Cloud Platform can be
utilized for this purpose.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Having reached this point, our predictive model is up and running in the cloud
in an autonomous fashion, delivering predictions to the data warehouse to act
upon. The data warehouse is certainly not the end of the journey, but we stop
here and save the discussion for another time.&lt;/p&gt;

&lt;p&gt;Although the presented workflow gets the job done, it has its own limitations
and weaknesses, which one has to be aware of. The most prominent one is the
communication between a Docker container running inside a virtual machine and
the scheduler, Airflow. Busy waiting for a virtual machine in Compute Engine to
shut down and for Stackdriver to deliver a certain message is arguably not the
most reliable solution. There is also a certain overhead associated with
starting a virtual machine in Compute Engine, downloading an image from
Container Registry, and launching a container. Furthermore, this approach is not
suitable for online prediction, as the service does not expose any API for other
services to integrate with—its job is making periodically batch predictions.&lt;/p&gt;

&lt;p&gt;If you have any suggestions regarding improving the workflow or simply would
like to share your thoughts, please leave a comment below or send an e-mail.
Feel also free to &lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/issues&quot;&gt;create an issue&lt;/a&gt; or &lt;a href=&quot;https://github.com/chain-rule/example-prediction-service/pulls&quot;&gt;open a pull request&lt;/a&gt; on GitHub. Any
feedback is very much appreciated!&lt;/p&gt;

&lt;h1 id=&quot;follow-up&quot;&gt;Follow-up&lt;/h1&gt;

&lt;p&gt;Since its publication, the workflow presented in this article has been
significantly simplified. More specifically, on July 16, 2019, it became
possible to execute arbitrary Docker images on Google &lt;a href=&quot;https://cloud.google.com/ai-platform/&quot;&gt;AI Platform&lt;/a&gt;. The
platform takes care of the whole life cycle of the container, obviating the need
for any wait scripts and ad-hoc communication mechanisms via Stackdriver. Refer
to “&lt;a href=&quot;https://medium.com/google-cloud/how-to-run-serverless-batch-jobs-on-google-cloud-ca45a4e33cb1&quot;&gt;How to run serverless batch jobs on Google Cloud&lt;/a&gt;” by Lak
Lakshmanan for further details.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Lak Lakshmanan, “&lt;a href=&quot;https://medium.com/google-cloud/how-to-run-serverless-batch-jobs-on-google-cloud-ca45a4e33cb1&quot;&gt;How to run serverless batch jobs on Google Cloud&lt;/a&gt;,” 2019.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ivan Ukhov</name></author><summary type="html">As a data scientist focusing on developing data products, you naturally want your work to reach its target audience. Suppose, however, that your company does not have a dedicated engineering team for productizing data-science code. One solution is to seek help in other teams, which are surely busy with their own endeavors, and spend months waiting. Alternatively, you could take the initiative and do it yourself. In this article, we take the initiative and schedule the training and application phases of a predictive model using Apache Airflow, Google Compute Engine, and Docker.</summary></entry></feed>