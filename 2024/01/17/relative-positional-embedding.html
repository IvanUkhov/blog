<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Relative positional embedding for any attention mechanism | Good news, everyone!</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Relative positional embedding for any attention mechanism" />
<meta name="author" content="Ivan Ukhov" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In Shaw et al. (2018), the authors introduce relative positional embedding for self-attention in transformer models, and in Huang et al. (2018), the authors present a memory efficient approach to calculating this embedding in decoder blocks, in which the self-attention is causal. In this article, the approach is generalized to any attention mechanism, should it be self or cross or full or causal." />
<meta property="og:description" content="In Shaw et al. (2018), the authors introduce relative positional embedding for self-attention in transformer models, and in Huang et al. (2018), the authors present a memory efficient approach to calculating this embedding in decoder blocks, in which the self-attention is causal. In this article, the approach is generalized to any attention mechanism, should it be self or cross or full or causal." />
<link rel="canonical" href="https://blog.ivanukhov.com/2024/01/17/relative-positional-embedding.html" />
<meta property="og:url" content="https://blog.ivanukhov.com/2024/01/17/relative-positional-embedding.html" />
<meta property="og:site_name" content="Good news, everyone!" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-17T07:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Relative positional embedding for any attention mechanism" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ivan Ukhov"},"dateModified":"2024-01-17T07:00:00+00:00","datePublished":"2024-01-17T07:00:00+00:00","description":"In Shaw et al. (2018), the authors introduce relative positional embedding for self-attention in transformer models, and in Huang et al. (2018), the authors present a memory efficient approach to calculating this embedding in decoder blocks, in which the self-attention is causal. In this article, the approach is generalized to any attention mechanism, should it be self or cross or full or causal.","headline":"Relative positional embedding for any attention mechanism","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.ivanukhov.com/2024/01/17/relative-positional-embedding.html"},"url":"https://blog.ivanukhov.com/2024/01/17/relative-positional-embedding.html"}</script>
<!-- End Jekyll SEO tag -->
<meta content="assets/favicon.png" property="og:image">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/assets/favicon.png"><link type="application/atom+xml" rel="alternate" href="https://blog.ivanukhov.com/feed.xml" title="Good news, everyone!" /><meta name="keywords" content="large language models, machine learning, positional embedding, transformers"><script async src="https://www.googletagmanager.com/gtag/js?id=G-43DGEP382H"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-43DGEP382H');
  </script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Good news, everyone!</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Relative positional embedding for any attention mechanism</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-01-17T07:00:00+00:00" itemprop="datePublished">January 17, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In <a href="https://arxiv.org/abs/1803.02155">Shaw et al. (2018)</a>, the authors introduce relative positional embedding for
self-attention in transformer models, and in <a href="https://arxiv.org/abs/1809.04281">Huang et al. (2018)</a>, the authors
present a memory efficient approach to calculating this embedding in decoder
blocks, in which the self-attention is causal. In this article, the approach is
generalized to any attention mechanism, should it be self or cross or full or
causal.</p>

<h1 id="background">Background</h1>

<p>The classical attention is formalized as follows:</p>

\[A = \text{softmax}\left( \frac{QK^{T}}{\sqrt{n_d}} \right) V\]

<p>where \(K\), \(V\), and \(Q\) are the keys, values, and queries, respectively.
The keys and values are of shape \(n_s \times n_h \times n_{t_1} \times n_d\)
where \(n_s\) is the batch size (<em>s</em> for space), \(n_h\) is the number of
attention heads, \(n_{t_1}\) is the window size (<em>t</em> for time) of the <em>input</em>
sequence, and \(n_d\) is the head size. The queries are of shape \(n_s \times
n_h \times n_{t_2} \times n_d\) where \(n_{t_2}\) is the window size of the
<em>output</em> sequence.</p>

<p>The relative attention obtains one additional term in the numerator:</p>

\[A = \text{softmax}\left( \frac{QK^T + S}{\sqrt{n_d}} \right) V. \tag{1}\]

<p>In the above, \(S\) is of shape \(n_s \times n_h \times n_{t_2} \times n_{t_1}\)
and calculated based on \(Q\) and a matrix \(E\) of shape \(n_d \times n_{t_3}\)
containing relative positional embeddings. The typical context is causal
self-attention, in which \(n_{t_3}\) is thought of as the maximum allowed length
of the input sequence and set to \(n_{t_1}\), with the interpretation that the
embeddings are running from position \(-n_{t_1} + 1\) (the most distant past) up
to \(0\) (the present moment). Then \(S\) is a specific arrangement of the inner
products between the queries in \(Q\) and the embeddings in \(E\) so as to
respect the arrangement in \(QK^T\).</p>

<p>The original and more memory efficient calculations of \(S\) in the case of
causal attention, are illustrated in the figure below, which is taken from Huang
et al. (2018).</p>

<p><img src="/assets/images/2024-01-17-relative-position/huang.jpeg" alt="" /></p>

<p>The matrix to the very right shows how \(S\) is arranged. Since the use case is
causal attention, the upper triangle above the main diagonal (gray circles) is
irrelevant and can contain arbitrary values, which it does in the algorithm
proposed in Huang et al. (2018). The main diagonal (green circles) contains the
inner products of the queries and the embedding corresponding to position \(0\).
The first subdiagonal (pink circles) contains the inner products of the queries
except for the first one as it has no past, and the embedding corresponding to
position \(-1\). And it continues in this way down to \(-n_{t_1} + 1\), in which
case it is only the last query that is involved, since it comes last in the
sequence and has the longest past.</p>

<p>The calculation given in Huang et al. (2018) reduces the intermediate memory
requirement from \(\mathcal{O}(n_h \, n_d \, n_t^2)\) to \(\mathcal{O}(n_h \,
n_d \, n_t)\) where \(n_t\) is a general sequence length. However, it is limited
to self-attention with causal connectivity, which is what is found in decoder
blocks. It is not suitable for other attention patterns. Therefore, it cannot be
used in, for instance, encoder blocks and decoder blocks with cross-attention,
which usually have non-causal attention. In what follow, the limitation is
lifted.</p>

<h1 id="algorithm">Algorithm</h1>

<p>Let us extend \(E\) to be of shape \(n_d \times (2 n_{t_3} - 1)\) so that it has
an embedding for any relative position not only when looking back in the past
but also forward into the future, with \(n_{t_3}\) being the maximum allowed
length of the input sequence as before, that is, \(t_1 \leq t_3\). Let us also
interpret \(E\)’s columns as running from position \(n_{t_3} - 1\) (the most
distant future) to position \(-n_{t_3} + 1\) (the most distant past). For
instance, when the output sequence is of length \(t_3\) (the longest possible),
the first query (position 0) will be “interested” only in columns \(0\) through
\(n_{t_3} - 1\) inclusively, while the last (position \(n_{t_3} - 1\)) only in
columns \(n_{t_3} - 1\) through \(2 n_{t_3} - 2\) inclusively.</p>

<p>Similarly to Huang et al. (2018), we note that multiplying \(Q\) by \(E\)
results in a matrix that contains all the inner products necessary for
assembling \(S\) in the general case. For instance, for \(t_3 = 4\) and dropping
the batch and head dimensions for clearer visualization, the product is as
follows:</p>

\[QE = \left(
\begin{matrix}
s_{0 + 3} &amp; s_{0 + 2} &amp; s_{0 + 1} &amp; s_{0 + 0} &amp; &amp; &amp; \\
&amp; s_{1 + 2} &amp; s_{1 + 1} &amp; s_{1 + 0} &amp; s_{1 - 1} &amp; &amp; \\
&amp; &amp; s_{2 + 1} &amp; s_{2 + 0} &amp; s_{2 - 1} &amp; s_{2 - 2} &amp; \\
&amp; &amp; &amp; s_{3 + 0} &amp; s_{3 - 1} &amp; s_{3 - 2} &amp; s_{3 - 3} \\
\end{matrix}
\right)\]

<p>where \(s_{i + t}\) denotes query \(i\) embedded to look at relative time \(t\),
that is, the inner product between the query at position \(i\) and the embedding
corresponding to a relative attention shift of \(t\), whose embedding is stored
in column \(n_{t_3} - 1 - t\) of \(E\). For instance, for \(s_{2 - 1}\) with
\(t_3 = 4\) still, the inner product is between row \(2\) of \(Q\) and column
\(4 - 1 - (-1) = 4\) of \(E\).</p>

<p>The target arrangement is then simply the one where we stack the
“interesting” diagonals of \(QE\) on top of each other from diagonal \(0\) (the
main diagonal) at the bottom and diagonal \(t_3 - 1\) (the rightmost relevant
superdiagonal) at the top</p>

\[\bar{S} = \left(
\begin{matrix}
s_{0 + 0} &amp; s_{1 - 1} &amp; s_{2 - 2} &amp; s_{3 - 3} \\
s_{0 + 1} &amp; s_{1 + 0} &amp; s_{2 - 1} &amp; s_{3 - 2} \\
s_{0 + 2} &amp; s_{1 + 1} &amp; s_{2 + 0} &amp; s_{3 - 1} \\
s_{0 + 3} &amp; s_{1 + 2} &amp; s_{2 + 1} &amp; s_{3 + 0} \\
\end{matrix}
\right)\]

<p>and then transpose the result</p>

\[S = \left(
\begin{matrix}
s_{0 + 0} &amp; s_{0 + 1} &amp; s_{0 + 2} &amp; s_{0 + 3} \\
s_{1 - 1} &amp; s_{1 + 0} &amp; s_{1 + 1} &amp; s_{1 + 2} \\
s_{2 - 2} &amp; s_{2 - 1} &amp; s_{2 + 0} &amp; s_{2 + 1} \\
s_{3 - 3} &amp; s_{3 - 2} &amp; s_{3 - 1} &amp; s_{3 + 0} \\
\end{matrix}
\right).\]

<p>More generally, the algorithm can be summarized as follows:</p>

\[S = \text{transpose}\left(
  \text{diagonal}\left(
    QE, \, \text{lower}=0, \, \text{upper}=n_{t_3} - 1
  \right)
\right)\]

<p>where \(\text{diagonal}\) is a function taking a tensor and stacking its
diagonals—specified by a range with two offsets relative to the main
diagonal—from bottom up, and \(\text{transpose}\) is a function taking a tensor
and transposing it. Both functions operators on the last two dimensions of the
given tensor. This resulting matrix can then be plugged into Equation (1) to
complete the calculation.</p>

<p>In case the keys and values are shorter than the maximum allowed relative
position, that is, \(t_1 &lt; t_3\), \(S\) should be truncated to its intended
shape, \(n_s \times n_h \times n_{t_2} \times n_{t_1}\):</p>

\[S = \text{truncate}\left(
  \text{transpose}\left(
    \text{diagonal}\left(
      QE, \, \text{lower}=0, \, \text{upper}=n_{t_3} - 1
    \right)
  \right),
  \text{keep} = n_{t_1}
\right)\]

<p>where \(\text{truncate}\) is a function taking a tensor and keeping only the
specified number of its first elements in the last dimension, discarding the
rest.</p>

<p>It can be seen that the algorithm the same intermediate memory requirement than
the one proposed in Huang at al. (2018), that is, \(\mathcal{O}(n_h \, n_d \,
n_t)\); however, its application score is larger.</p>

<h1 id="implementation">Implementation</h1>

<p>In TensorFlow, the algorithm can be implemented as an embedding layer as
follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RelativePositionalEmbedding</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">head_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">projection</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">head_size</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sequence_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s">"glorot_uniform"</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="n">sequence_length</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">projection</span><span class="p">)</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">diag_part</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sequence_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">S</span>
</code></pre></div></div>

<p>The above layer can be invoked as part of an attention layer as illustrated
below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">head_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">head_size</span> <span class="o">=</span> <span class="n">head_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">RelativePositionalEmbedding</span><span class="p">(</span>
            <span class="n">head_size</span><span class="o">=</span><span class="n">head_size</span><span class="p">,</span>
            <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Q</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># TODO: Add permutation if needed.
</span>        <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">positional_embedding</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">S</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_size</span><span class="o">**-</span><span class="mf">0.5</span>
        <span class="c1"># TODO: Add masking if needed.
</span>        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># TODO: Add dropout if needed.
</span>        <span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="c1"># TODO: Add dropout if needed.
</span>        <span class="k">return</span> <span class="n">A</span>
</code></pre></div></div>

<h1 id="references">References</h1>

<ul>
  <li>Huang et al., “<a href="https://arxiv.org/abs/1809.04281">Music transformer: Generating music with long-term
structure</a>,” Google Brain, 2018.</li>
  <li>Shaw et al., “<a href="https://arxiv.org/abs/1803.02155">Self-attention with relative position representations</a>,” Google Brain, 2018.</li>
</ul>


  </div><div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://blog.ivanukhov.com/2024/01/17/relative-positional-embedding.html';
      this.page.identifier = 'https://blog.ivanukhov.com/2024/01/17/relative-positional-embedding.html';
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = 'https://good-news-everyone.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript><a class="u-url" href="/2024/01/17/relative-positional-embedding.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Good news, everyone!</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ivan Ukhov</li><li><a class="u-email" href="mailto:ivan.ukhov@gmail.com">ivan.ukhov@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/IvanUkhov"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">IvanUkhov</span></a></li><li><a href="https://www.linkedin.com/in/IvanUkhov"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">IvanUkhov</span></a></li><li><a href="https://www.twitter.com/IvanUkhov"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">IvanUkhov</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Solving problems that a software engineer might encounter in practice—or
invent to sharpen their skills in leisure time
</p>
      </div>
    </div>

  </div>

</footer>
<script type="text/javascript">
      function anchor() {
        for (let header of document.querySelectorAll('h1[id], h2[id]')) {
          header.innerHTML += ` <a href="#${header.id}" aria-hidden="true">#</a>`;
        }
      }
      function theme() {
        document.body.className = 'theme-' + (Math.floor(Math.random() * 6) + 1);
      }
      window.addEventListener('load', anchor);
      window.addEventListener('load', theme);
    </script>
  </body>
</html>
