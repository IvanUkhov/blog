<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Out of memory, or gradient accumulation for larger models | Good news, everyone!</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Out of memory, or gradient accumulation for larger models" />
<meta name="author" content="Ivan Ukhov" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="When the model grows large and does not fit on a single device, and there are no more devices to spare, the common mitigation strategy is to reduce the batch size, thereby allowing more space for the model at the expense of the data. However, smaller batches lead to noisier weight updates, which is undesirable. One solution is gradient accumulation where the weights are updated after evaluating the gradients for several batches at a time. In this article, we show how it can be implemented in practice." />
<meta property="og:description" content="When the model grows large and does not fit on a single device, and there are no more devices to spare, the common mitigation strategy is to reduce the batch size, thereby allowing more space for the model at the expense of the data. However, smaller batches lead to noisier weight updates, which is undesirable. One solution is gradient accumulation where the weights are updated after evaluating the gradients for several batches at a time. In this article, we show how it can be implemented in practice." />
<link rel="canonical" href="https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html" />
<meta property="og:url" content="https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html" />
<meta property="og:site_name" content="Good news, everyone!" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-31T07:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Out of memory, or gradient accumulation for larger models" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ivan Ukhov"},"dateModified":"2024-01-31T07:00:00+00:00","datePublished":"2024-01-31T07:00:00+00:00","description":"When the model grows large and does not fit on a single device, and there are no more devices to spare, the common mitigation strategy is to reduce the batch size, thereby allowing more space for the model at the expense of the data. However, smaller batches lead to noisier weight updates, which is undesirable. One solution is gradient accumulation where the weights are updated after evaluating the gradients for several batches at a time. In this article, we show how it can be implemented in practice.","headline":"Out of memory, or gradient accumulation for larger models","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html"},"url":"https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html"}</script>
<!-- End Jekyll SEO tag -->
<meta content="assets/favicon.png" property="og:image">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/assets/favicon.png"><link type="application/atom+xml" rel="alternate" href="https://blog.ivanukhov.com/feed.xml" title="Good news, everyone!" /><meta name="keywords" content="Adam, TensorFlow, distributed systems, gradient, machine learning, optimization"><script async src="https://www.googletagmanager.com/gtag/js?id=G-43DGEP382H"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-43DGEP382H');
  </script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><style>
  .wrapper {
      max-width: -webkit-calc(900px - (30px * 2)) !important;
      max-width: calc(900px - (30px * 2)) !important;
  }
  </style>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Good news, everyone!</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Out of memory, or gradient accumulation for larger models</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-01-31T07:00:00+00:00" itemprop="datePublished">January 31, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>When the model grows large and does not fit on a single device, and there are no
more devices to spare, the common mitigation strategy is to reduce the batch
size, thereby allowing more space for the model at the expense of the data.
However, smaller batches lead to noisier weight updates, which is undesirable.
One solution is gradient accumulation where the weights are updated after
evaluating the gradients for several batches at a time. In this article, we show
how it can be implemented in practice.</p>

<h1 id="solution">Solution</h1>

<p>Long story short:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inherit from any optimizer of choice, such as Adam.
</span><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">):</span>
    <span class="s">"""Optimizer that implements gradient accumulation."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">accumulation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="s">"""Create an instance.

        Arguments:
          accumulation: The number of iterations to accumulate gradients over.
          If it is set to one, no accumulation is performed, and the gradients
          are applied as soon as they are computed. If it is set to a value
          greater than one, the gradients will be accumulated for the specified
          number of iterations and only then applied, starting a new cycle.

        All other arguments are passed to the base optimizer.
        """</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">accumulation</span> <span class="o">=</span> <span class="n">accumulation</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_gradients</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">apply_gradients</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">gradients_variables</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""Apply the gradients according to the accumulation scheme."""</span>
        <span class="c1"># Split off the gradients from the trainable variables.
</span>        <span class="n">gradients</span><span class="p">,</span> <span class="n">variables</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="nb">list</span><span class="p">(</span><span class="n">gradients_variables</span><span class="p">))</span>
        <span class="c1"># Perform the initialization if needed.
</span>        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">init_scope</span><span class="p">():</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="n">variables</span><span class="p">)</span>
        <span class="c1"># Compute a scaling factor that will reset the accumulated gradients at
</span>        <span class="c1"># the beginning of each cycle and do nothing otherwise.
</span>        <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">iterations</span> <span class="o">%</span> <span class="bp">self</span><span class="p">.</span><span class="n">accumulation</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># Add the new gradients to the old ones after scaling.
</span>        <span class="k">for</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">addition</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_gradients</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
            <span class="n">gradient</span><span class="p">.</span><span class="n">assign</span><span class="p">(</span><span class="n">scale</span> <span class="o">*</span> <span class="n">gradient</span> <span class="o">+</span> <span class="n">addition</span><span class="p">)</span>
        <span class="c1"># Apply the gradients to the trainable variables after scaling.
</span>        <span class="n">gradients</span> <span class="o">=</span> <span class="p">[</span><span class="n">gradient</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">accumulation</span> <span class="k">for</span> <span class="n">gradient</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">_gradients</span><span class="p">]</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">().</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">variables</span><span class="p">))</span>

    <span class="o">@</span><span class="n">tf</span><span class="p">.</span><span class="n">function</span>
    <span class="k">def</span> <span class="nf">update_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">variable</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="s">"""Update the trainable variable with the gradient."""</span>
        <span class="c1"># Allow the update to happen only at the end of each cycle.
</span>        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="p">.</span><span class="n">accumulation</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">().</span><span class="n">update_step</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">variable</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variables</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="s">"""Initialize the internal state."""</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">build</span><span class="p">(</span><span class="n">variables</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">_gradients</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># Allocate memory for accumulation.
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">_gradients</span> <span class="o">=</span> <span class="p">[</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">add_variable_from_reference</span><span class="p">(</span>
                    <span class="n">model_variable</span><span class="o">=</span><span class="n">variable</span><span class="p">,</span>
                    <span class="n">variable_name</span><span class="o">=</span><span class="s">"gradient"</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">variables</span>
            <span class="p">]</span>
</code></pre></div></div>

<p>It is important to note that the learning rate is <em>not</em> held constant during
accumulation. However, since it is not expected to change much from one
iteration to another, it is an adequate simplification.</p>

<h1 id="acknowledgments">Acknowledgments</h1>

<p>I would like to thank <a href="https://github.com/andreped">André Pedersen</a>, <a href="https://github.com/roebel">Axel Roebel</a>, and <a href="https://github.com/tno123">Tor-Arne Nordmo</a> for
their help with the implementation.</p>


  </div><div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html';
      this.page.identifier = 'https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html';
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = 'https://good-news-everyone.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript><a class="u-url" href="/2024/01/31/gradient-accumulation.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Good news, everyone!</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ivan Ukhov</li><li><a class="u-email" href="mailto:ivan.ukhov@gmail.com">ivan.ukhov@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/IvanUkhov"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">IvanUkhov</span></a></li><li><a href="https://www.linkedin.com/in/IvanUkhov"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">IvanUkhov</span></a></li><li><a href="https://www.twitter.com/IvanUkhov"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">IvanUkhov</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Solving problems that a software engineer might encounter in practice—or
invent to sharpen their skills in leisure time
</p>
      </div>
    </div>

  </div>

</footer>
<script type="text/javascript">
      function anchor() {
        for (let header of document.querySelectorAll('h1[id], h2[id]')) {
          header.innerHTML += ` <a href="#${header.id}" aria-hidden="true">#</a>`;
        }
      }
      function theme() {
        document.body.className = 'theme-' + (Math.floor(Math.random() * 6) + 1);
      }
      window.addEventListener('load', anchor);
      window.addEventListener('load', theme);
    </script>
  </body>
</html>
