<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Out of memory, or gradient accumulation for larger models | Good news, everyone!</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Out of memory, or gradient accumulation for larger models" />
<meta name="author" content="Ivan Ukhov" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="When the model grows large and does not fit on a single device, and there are no more devices to spare, the common mitigation strategy is to reduce the batch size, thereby allowing more space for the model at the expense of the data. However, smaller batches lead to noisier weight updates, which is undesirable. One solution is gradient accumulation where the weights are updated after evaluating the gradients for several batches at a time. In this article, we show how it can be implemented in practice." />
<meta property="og:description" content="When the model grows large and does not fit on a single device, and there are no more devices to spare, the common mitigation strategy is to reduce the batch size, thereby allowing more space for the model at the expense of the data. However, smaller batches lead to noisier weight updates, which is undesirable. One solution is gradient accumulation where the weights are updated after evaluating the gradients for several batches at a time. In this article, we show how it can be implemented in practice." />
<link rel="canonical" href="https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html" />
<meta property="og:url" content="https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html" />
<meta property="og:site_name" content="Good news, everyone!" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-31T07:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Out of memory, or gradient accumulation for larger models" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ivan Ukhov"},"dateModified":"2024-01-31T07:00:00+00:00","datePublished":"2024-01-31T07:00:00+00:00","description":"When the model grows large and does not fit on a single device, and there are no more devices to spare, the common mitigation strategy is to reduce the batch size, thereby allowing more space for the model at the expense of the data. However, smaller batches lead to noisier weight updates, which is undesirable. One solution is gradient accumulation where the weights are updated after evaluating the gradients for several batches at a time. In this article, we show how it can be implemented in practice.","headline":"Out of memory, or gradient accumulation for larger models","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html"},"url":"https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html"}</script>
<!-- End Jekyll SEO tag -->
<link id="main-stylesheet" rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.ivanukhov.com/feed.xml" title="Good news, everyone!" /><script async src="https://www.googletagmanager.com/gtag/js?id=G-43DGEP382H"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-43DGEP382H');
</script>
<meta content="assets/favicon.png" property="og:image"><meta name="keywords" content="Adam, TensorFlow, distributed systems, gradient, machine learning, optimization"><link rel="stylesheet" href="/assets/main.css">
<link rel="shortcut icon" type="image/png" href="/assets/favicon.png"><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><style>
  .wrapper {
    max-width: -webkit-calc(900px - (30px * 2)) !important;
    max-width: calc(900px - (30px * 2)) !important;
  }
</style>
</head>
<body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Good news, everyone!</a>
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon"></span>
        </label>

        <div class="nav-items">
  <a class="nav-item" href="/about/">About</a>
</div>

      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Out of memory, or gradient accumulation for larger models</h1>
    <div class="post-meta">
      <time class="dt-published" datetime="2024-01-31T07:00:00+00:00" itemprop="datePublished">
        January 31, 2024
      </time>
    </div>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>When the model grows large and does not fit on a single device, and there are no
more devices to spare, the common mitigation strategy is to reduce the batch
size, thereby allowing more space for the model at the expense of the data.
However, smaller batches lead to noisier weight updates, which is undesirable.
One solution is gradient accumulation where the weights are updated after
evaluating the gradients for several batches at a time. In this article, we show
how it can be implemented in practice.</p>

<h1 id="solution">Solution</h1>

<p>Long story short, assuming TensorFlow 2.17:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inherit from any optimizer of choice, such as Adam.
</span><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Optimizer that implements gradient accumulation.</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">accumulation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Create an instance.

        Arguments:
          accumulation: The number of iterations to accumulate gradients over.
          If it is set to one, no accumulation is performed, and the gradients
          are applied as soon as they are computed. If it is set to a value
          greater than one, the gradients will be accumulated for the specified
          number of iterations and only then applied, starting a new cycle.

        All other arguments are passed to the base optimizer.
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">accumulation</span> <span class="o">=</span> <span class="n">accumulation</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_gradients</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">iterations</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Return the number of iterations.</span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="nf">floor_divide</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_iterations</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">accumulation</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">apply_gradients</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">gradients_variables</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Apply the gradients according to the accumulation scheme.</span><span class="sh">"""</span>
        <span class="c1"># Split off the gradients from the trainable variables.
</span>        <span class="n">gradients</span><span class="p">,</span> <span class="n">variables</span> <span class="o">=</span> <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="nf">list</span><span class="p">(</span><span class="n">gradients_variables</span><span class="p">))</span>
        <span class="c1"># Perform the initialization if needed.
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">built</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">init_scope</span><span class="p">():</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">variables</span><span class="p">)</span>
        <span class="n">first</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_iterations</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">accumulation</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">last</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">accumulation</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># Add the new gradients to the old ones with resetting if needed.
</span>        <span class="k">for</span> <span class="nb">sum</span><span class="p">,</span> <span class="n">delta</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_gradients</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">delta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="nb">sum</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="o">~</span><span class="n">first</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="nb">sum</span> <span class="o">+</span> <span class="n">delta</span><span class="p">)</span>
        <span class="c1"># Apply the average accumulated gradients to the trainable variables.
</span>        <span class="n">gradients</span> <span class="o">=</span> <span class="p">[</span><span class="n">gradient</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">accumulation</span> <span class="k">for</span> <span class="n">gradient</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">_gradients</span><span class="p">]</span>
        <span class="k">return</span> <span class="nf">super</span><span class="p">().</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">variables</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">update_step</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">gradient</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">variable</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Update the trainable variable with the gradient.</span><span class="sh">"""</span>
        <span class="n">update_step</span> <span class="o">=</span> <span class="nf">super</span><span class="p">().</span><span class="n">update_step</span>
        <span class="n">last</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">accumulation</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># Allow the update to happen only at the end of each cycle.
</span>        <span class="n">true</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="nf">update_step</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">tf</span><span class="p">.</span><span class="nf">cond</span><span class="p">(</span><span class="n">last</span><span class="p">,</span> <span class="n">true</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="bp">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">variables</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Initialize the internal state.</span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">build</span><span class="p">(</span><span class="n">variables</span><span class="p">)</span>
        <span class="c1"># Allocate memory for accumulation.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">_gradients</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">add_variable_from_reference</span><span class="p">(</span>
                <span class="n">reference_variable</span><span class="o">=</span><span class="n">variable</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">variables</span>
        <span class="p">]</span>
</code></pre></div></div>

<p>It is important to note that the learning rate keeps on changing (if variable)
and the weights keep on decaying (if enabled) during accumulation. Therefore,
one should account for this when configuring the optimizer at hand.</p>

<p>One should also note that TensorFlow does support gradient accumulation as of
version 2.16, which is controlled by the <code class="language-plaintext highlighter-rouge">gradient_accumulation_steps</code> option of
Keras optimizers. However, it does not play well with distributed training
strategies, which will hopefully be rectified in the future.</p>

<h1 id="acknowledgments">Acknowledgments</h1>

<p>I would like to thank <a href="https://github.com/andreped">André Pedersen</a>, <a href="https://github.com/roebel">Axel Roebel</a>, and <a href="https://github.com/tno123">Tor-Arne Nordmo</a> for
their help with the implementation.</p>


  </div>

  <script>
    var disqus_config = function () {
      this.page.url = 'https://blog.ivanukhov.com/2024/01/31/gradient-accumulation.html';
      this.page.identifier = '/2024/01/31/gradient-accumulation';
    };
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://good-news-everyone.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
<a class="u-url" href="/2024/01/31/gradient-accumulation.html" hidden></a>
</article>

      </div>
    </main><link id="fa-stylesheet" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">

<footer class="site-footer h-card">
  <data class="u-url" value="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <ul class="contact-list">
          <li class="p-name">Ivan Ukhov</li>
          <li><a class="u-email" href="mailto:ivan.ukhov@gmail.com">ivan.ukhov@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Solving problems that a software engineer might encounter in practice—or invent to sharpen their skills in leisure time
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
    <a rel="me" href="https://github.com/IvanUkhov" target="_blank" title="GitHub">
      <span class="grey fa-brands fa-github fa-lg"></span>
    </a>
  </li><li>
    <a rel="me" href="https://www.linkedin.com/in/ivanukhov/" target="_blank" title="LinkedIn">
      <span class="grey fa-brands fa-linkedin fa-lg"></span>
    </a>
  </li><li>
    <a rel="me" href="https://x.com/IvanUkhov" target="_blank" title="X">
      <span class="grey fa-brands fa-x-twitter fa-lg"></span>
    </a>
  </li>
  <li>
    <a href="https://blog.ivanukhov.com/feed.xml" target="_blank" title="Subscribe to syndication feed">
      <svg class="svg-icon grey" viewbox="0 0 16 16">
        <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
          11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
          13.806c0-1.21.983-2.195 2.194-2.195zM10.606
          16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
        />
      </svg>
    </a>
  </li>
</ul>
</div>

  </div>

</footer>
<script type="text/javascript">
  function anchor() {
    for (let header of document.querySelectorAll('h1[id], h2[id]')) {
      header.innerHTML += ` <a href="#${header.id}" aria-hidden="true">#</a>`;
    }
  }
  function theme() {
    document.body.className = 'theme-' + (Math.floor(Math.random() * 6) + 1);
  }
  window.addEventListener('load', anchor);
  window.addEventListener('load', theme);
</script>
</body>

</html>
